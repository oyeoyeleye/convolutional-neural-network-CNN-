{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "14a5ded8-7744-46d7-a7a1-e2dfe5cbe947",
    "_uuid": "af5f6b18cd29ecd0cb8050d2b97195885b716148"
   },
   "source": [
    "**Predicting Pathologies In X-Ray Images**  *--work in progress--*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1a01cf7d-4ddd-4ee3-b6f6-6e9e04777d33",
    "_uuid": "d84bc263d4006c32862d766a2517588c74e8ed08"
   },
   "source": [
    "The NIH Clinical Center recently released over 100,000 anonymized chest x-ray images and their corresponding data to the scientific community. The release will allow researchers across the country and around the world to freely access the datasets and increase their ability to teach computers how to detect and diagnose disease. Ultimately, this artificial intelligence mechanism can lead to clinicians making better diagnostic decisions for patients.   \n",
    "\n",
    "https://www.nih.gov/news-events/news-releases/nih-clinical-center-provides-one-largest-publicly-available-chest-x-ray-datasets-scientific-community\n",
    "\n",
    "https://stanfordmlgroup.github.io/projects/chexnet/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "585e543b-d02f-4ff6-b78a-746f917dbe0f",
    "_kg_hide-input": true,
    "_uuid": "d703d77ba8ce1398e6dd328aedd680b934aa1c16"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/alan/miniconda2/envs/tfl/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/alan/miniconda2/envs/tfl/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/alan/miniconda2/envs/tfl/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/alan/miniconda2/envs/tfl/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/alan/miniconda2/envs/tfl/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/alan/miniconda2/envs/tfl/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/alan/miniconda2/envs/tfl/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/alan/miniconda2/envs/tfl/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/alan/miniconda2/envs/tfl/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/alan/miniconda2/envs/tfl/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/alan/miniconda2/envs/tfl/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/alan/miniconda2/envs/tfl/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "os.sys.path\n",
    "from glob import glob\n",
    "import random\n",
    "import matplotlib.pylab as plt\n",
    "import cv2\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "import zlib\n",
    "import itertools\n",
    "import sklearn\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD, RMSprop, Adam, Adagrad, Adadelta\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation,Dense, Dropout, Flatten, Conv2D, MaxPool2D,MaxPooling2D,AveragePooling2D, BatchNormalization\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
    "from keras.models import model_from_json\n",
    "from keras import backend as K\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.applications.mobilenet import MobileNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5411efb9-6888-4cb0-afcd-eb89daa9fdb6",
    "_uuid": "84f6ea7c5c644a344965992cd726b57eeb7f7393"
   },
   "source": [
    "Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "e779528a-cb1b-408a-9ff9-61c5df9e201d",
    "_uuid": "ab4c84025f96f8696fe54bfd9c013f0eeb0166e0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = os.path.abspath(os.path.join('/media/alan/4039-01DB/data_xray/images_001')) \n",
    "SOURCE_IMAGES = os.path.join(PATH, \"images\")\n",
    "images = glob(os.path.join(SOURCE_IMAGES, \"*.png\"))\n",
    "\n",
    "images[0:10]\n",
    "# images = images[0:90000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "9454f462-7785-48ff-bc09-4e290bd36985",
    "_uuid": "8cc29e2914239e690d0db8c4d2ca215765dd54f9",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'/media/alan/4039-01DB/data_xray/Data_Entry_2017.csv' does not exist: b'/media/alan/4039-01DB/data_xray/Data_Entry_2017.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-2f36e38c0dd1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/media/alan/4039-01DB/data_xray/Data_Entry_2017.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/bmnn/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    695\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 697\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/bmnn/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/bmnn/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    888\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 890\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    891\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/bmnn/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1117\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1118\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/bmnn/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1846\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'/media/alan/4039-01DB/data_xray/Data_Entry_2017.csv' does not exist: b'/media/alan/4039-01DB/data_xray/Data_Entry_2017.csv'"
     ]
    }
   ],
   "source": [
    "labels = pd.read_csv('/media/alan/4039-01DB/data_xray/Data_Entry_2017.csv')\n",
    "labels.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "dbcc4926-be51-4256-91d1-bc39eace0a01",
    "_uuid": "3601a7cd6b5178cc9037bbfc55a7ba1145e15a21"
   },
   "source": [
    "Plot a representative image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3fa57fbb-13d1-4bff-9339-7f0bbdce741d",
    "_uuid": "54bc8a9333d23c67a2cb364ff6095da0dea827bc"
   },
   "source": [
    "It should be noted that these images require some significant pre-processing and/or relabeling for best results.  For this exercise we will do only minimal pre-processing of the images.  See the following blogpost for more detail about specific challenges associated with this dataset: https://lukeoakdenrayner.wordpress.com/2017/12/18/the-chestxray14-dataset-problems/. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ae014fcf-d116-425d-ba16-5d8743579d29",
    "_uuid": "6bc77ece46e00762b4323a359aca64f73c04355b"
   },
   "source": [
    "What types of ailments are identified in these annotated X-ray images?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "661da6c5-b296-44de-b4e3-fba40a77b635",
    "_uuid": "41d608661473264d3187e4e60448a05bc598e03f"
   },
   "source": [
    "Convert annotated .png images into labeled numpy arrays.  Discard all images with more than one pathology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "90e123ca-ed1f-476b-a075-a4c7535aa005",
    "_uuid": "a8874a0cca8c84f9098fe62a8173694cd4fe2b36"
   },
   "outputs": [],
   "source": [
    "df=labels\n",
    "data=df.groupby('Finding Labels').count().sort_values('Patient ID',ascending=False)\n",
    "df1=data[['|' in index for index in data.index]].copy()\n",
    "df2=data[['|' not in index for index in data.index]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "765d5e31-0a17-434e-8ba4-4abbc09becca",
    "_uuid": "7e0e1cade7fdaad0677ee3326d35bb25816c3cfc"
   },
   "outputs": [],
   "source": [
    "\n",
    "img_size = 79\n",
    "\n",
    "def proc_images():\n",
    "    \"\"\"\n",
    "    Returns two arrays: \n",
    "        x is an array of resized images\n",
    "        y is an array of labels\n",
    "    \"\"\"\n",
    "    NoFinding = \"No Finding\" #0\n",
    "    Consolidation=\"Consolidation\" #1\n",
    "    Infiltration=\"Infiltration\" #2\n",
    "    Pneumothorax=\"Pneumothorax\" #3\n",
    "    Edema=\"Edema\" # 4\n",
    "    Emphysema=\"Emphysema\" #5\n",
    "    Fibrosis=\"Fibrosis\" #6\n",
    "    Effusion=\"Effusion\" #7\n",
    "    Pneumonia=\"Pneumonia\" #8\n",
    "    Pleural_Thickening=\"Pleural_Thickening\" #9\n",
    "    Cardiomegaly=\"Cardiomegaly\" #10\n",
    "    NoduleMass=\"Nodule\" #11\n",
    "    Hernia=\"Hernia\" #12\n",
    "    Atelectasis=\"Atelectasis\"  #13 \n",
    "   # RareClass = [\"Edema\", \"Emphysema\", \"Fibrosis\", \"Pneumonia\", \"Pleural_Thickening\", \"Cardiomegaly\",\"Hernia\"]\n",
    "    i=0\n",
    "    for img in images:\n",
    "        i+=1\n",
    "        base = os.path.basename(img)\n",
    "        # Read and resize image\n",
    "        finding = labels[\"Finding Labels\"][labels[\"Image Index\"] == base].values[0]\n",
    "        symbol = \"|\"\n",
    "        if i % 1000 == 0:\n",
    "                print(\"Iteration\", i)\n",
    "        if symbol in finding:\n",
    "            images.remove(img)\n",
    "        elif NoFinding in finding:\n",
    "            if (np.random.uniform(0,1,1)>0.75):\n",
    "                images.remove(img)\n",
    "        elif Infiltration in finding:\n",
    "            if (np.random.uniform(0,1,1)>0.75):\n",
    "                images.remove(img)\n",
    "        elif Effusion in finding:\n",
    "            if (np.random.uniform(0,1,1)>0.75):\n",
    "                images.remove(img)\n",
    "        else:\n",
    "            continue\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1000\n",
      "Iteration 2000\n",
      "Iteration 3000\n",
      "Iteration 4000\n",
      "Iteration 5000\n",
      "Iteration 6000\n",
      "Iteration 7000\n",
      "Iteration 8000\n",
      "Iteration 9000\n",
      "Iteration 10000\n",
      "Iteration 11000\n",
      "Iteration 12000\n",
      "Iteration 13000\n",
      "Iteration 14000\n",
      "Iteration 15000\n",
      "Iteration 16000\n",
      "Iteration 17000\n",
      "Iteration 18000\n",
      "Iteration 19000\n",
      "Iteration 20000\n",
      "Iteration 21000\n",
      "Iteration 22000\n",
      "Iteration 23000\n",
      "Iteration 24000\n",
      "Iteration 25000\n",
      "Iteration 26000\n",
      "Iteration 27000\n",
      "Iteration 28000\n",
      "Iteration 29000\n",
      "Iteration 30000\n",
      "Iteration 31000\n",
      "Iteration 32000\n",
      "Iteration 33000\n",
      "Iteration 34000\n",
      "Iteration 35000\n",
      "Iteration 36000\n",
      "Iteration 37000\n",
      "Iteration 38000\n",
      "Iteration 39000\n",
      "Iteration 40000\n",
      "Iteration 41000\n",
      "Iteration 42000\n",
      "Iteration 43000\n",
      "Iteration 44000\n",
      "Iteration 45000\n",
      "Iteration 46000\n",
      "Iteration 47000\n",
      "Iteration 48000\n",
      "Iteration 49000\n",
      "Iteration 50000\n",
      "Iteration 51000\n",
      "Iteration 52000\n",
      "Iteration 53000\n",
      "Iteration 54000\n",
      "Iteration 55000\n",
      "Iteration 56000\n",
      "Iteration 57000\n",
      "Iteration 58000\n",
      "Iteration 59000\n",
      "Iteration 60000\n",
      "Iteration 61000\n",
      "Iteration 62000\n",
      "Iteration 63000\n",
      "Iteration 64000\n",
      "Iteration 65000\n",
      "Iteration 66000\n",
      "Iteration 67000\n",
      "Iteration 68000\n",
      "Iteration 69000\n",
      "Iteration 70000\n",
      "Iteration 71000\n",
      "Iteration 72000\n",
      "Iteration 73000\n",
      "Iteration 74000\n",
      "Iteration 75000\n",
      "Iteration 76000\n",
      "Iteration 77000\n",
      "Iteration 78000\n",
      "Iteration 79000\n",
      "Iteration 80000\n",
      "Iteration 81000\n",
      "Iteration 82000\n",
      "Iteration 83000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "83235"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proc_images()\n",
    "len(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Save and load the X and y from previous times.\n",
    "with open('images_path_singlelabel.pickle', 'wb') as f:\n",
    "    pickle.dump([images], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_cell_guid": "765d5e31-0a17-434e-8ba4-4abbc09becca",
    "_uuid": "7e0e1cade7fdaad0677ee3326d35bb25816c3cfc"
   },
   "outputs": [],
   "source": [
    "\n",
    "img_size = 256\n",
    "\n",
    "def proc_images():\n",
    "    \"\"\"\n",
    "    Returns two arrays: \n",
    "        x is an array of resized images\n",
    "        y is an array of labels\n",
    "    \"\"\"\n",
    "    NoFinding = \"No Finding\" #0\n",
    "    Consolidation=\"Consolidation\" #1\n",
    "    Infiltration=\"Infiltration\" #2\n",
    "    Pneumothorax=\"Pneumothorax\" #3\n",
    "    Edema=\"Edema\" # 4\n",
    "    Emphysema=\"Emphysema\" #5\n",
    "    Fibrosis=\"Fibrosis\" #6\n",
    "    Effusion=\"Effusion\" #7\n",
    "    Pneumonia=\"Pneumonia\" #8\n",
    "    Pleural_Thickening=\"Pleural_Thickening\" #9\n",
    "    Cardiomegaly=\"Cardiomegaly\" #10\n",
    "    NoduleMass=\"Nodule\" #11\n",
    "    Hernia=\"Hernia\" #12\n",
    "    Atelectasis=\"Atelectasis\"  #13 \n",
    "   # RareClass = [\"Edema\", \"Emphysema\", \"Fibrosis\", \"Pneumonia\", \"Pleural_Thickening\", \"Cardiomegaly\",\"Hernia\"]\n",
    "    x = [] # images as arrays\n",
    "    y = [] # labels\n",
    "    WIDTH = img_size\n",
    "    HEIGHT = img_size\n",
    "    a0=0;a1=0;a2=0;a3=0;a4=0;a5=0;a6=0;a7=0;a8=0;a9=0;a10=0;a11=0;a12=0;a13=0;i=0;\n",
    "    num_imgs = 200\n",
    "    for img in images:\n",
    "        base = os.path.basename(img)\n",
    "        # Read and resize image\n",
    "        finding = labels[\"Finding Labels\"][labels[\"Image Index\"] == base].values[0]\n",
    "        symbol = \"|\"\n",
    "        if a0+a1+a2+a3+a4+a5+a6+a7+a8+a9+a10+a11+a12+a13 == (num_imgs-1)*14:\n",
    "            break\n",
    "        if i % 5000 == 0:\n",
    "                print(\"Iteration\", i)\n",
    "        if symbol in finding:\n",
    "            continue\n",
    "        else:\n",
    "            if NoFinding in finding:\n",
    "                finding = 0\n",
    "                if a0 < num_imgs:\n",
    "                    y.append(finding)\n",
    "                    x.append(cv2.resize(cv2.imread(img), (WIDTH,HEIGHT), interpolation=cv2.INTER_CUBIC))\n",
    "                    a0+=1\n",
    "            elif Consolidation in finding:\n",
    "                finding = 1\n",
    "                if a1 < num_imgs:\n",
    "                    y.append(finding)\n",
    "                    x.append(cv2.resize(cv2.imread(img), (WIDTH,HEIGHT), interpolation=cv2.INTER_CUBIC))\n",
    "                    a1+=1\n",
    "            elif Infiltration in finding:\n",
    "                finding = 2\n",
    "                if a2 < num_imgs:\n",
    "                    y.append(finding)\n",
    "                    x.append(cv2.resize(cv2.imread(img), (WIDTH,HEIGHT), interpolation=cv2.INTER_CUBIC))\n",
    "                    a2+=1\n",
    "            elif Pneumothorax in finding:\n",
    "                finding = 3\n",
    "                if a3 < num_imgs:\n",
    "                    y.append(finding)\n",
    "                    x.append(cv2.resize(cv2.imread(img), (WIDTH,HEIGHT), interpolation=cv2.INTER_CUBIC))\n",
    "                    a3+=1\n",
    "            elif Edema in finding:\n",
    "                finding = 4\n",
    "                if a4 < num_imgs:\n",
    "                    y.append(finding)\n",
    "                    x.append(cv2.resize(cv2.imread(img), (WIDTH,HEIGHT), interpolation=cv2.INTER_CUBIC))\n",
    "                    a4+=1\n",
    "            elif Emphysema in finding:\n",
    "                finding = 5\n",
    "                if a5 < num_imgs:\n",
    "                    y.append(finding)\n",
    "                    x.append(cv2.resize(cv2.imread(img), (WIDTH,HEIGHT), interpolation=cv2.INTER_CUBIC))\n",
    "                    a5+=1\n",
    "            elif Fibrosis in finding:\n",
    "                finding = 6\n",
    "                if a6 < num_imgs:\n",
    "                    y.append(finding)\n",
    "                    x.append(cv2.resize(cv2.imread(img), (WIDTH,HEIGHT), interpolation=cv2.INTER_CUBIC))\n",
    "                    a6+=1\n",
    "            elif Effusion in finding:\n",
    "                finding = 7\n",
    "                if a7 < num_imgs:\n",
    "                    y.append(finding)\n",
    "                    x.append(cv2.resize(cv2.imread(img), (WIDTH,HEIGHT), interpolation=cv2.INTER_CUBIC))\n",
    "                    a7+=1\n",
    "            elif Pneumonia in finding:\n",
    "                finding = 8\n",
    "                if a8 < num_imgs:\n",
    "                    y.append(finding)\n",
    "                    x.append(cv2.resize(cv2.imread(img), (WIDTH,HEIGHT), interpolation=cv2.INTER_CUBIC))\n",
    "                    a8+=1\n",
    "            elif Pleural_Thickening in finding:\n",
    "                finding = 9\n",
    "                if a9 < num_imgs:\n",
    "                    y.append(finding)\n",
    "                    x.append(cv2.resize(cv2.imread(img), (WIDTH,HEIGHT), interpolation=cv2.INTER_CUBIC))\n",
    "                    a9+=1\n",
    "            elif Cardiomegaly in finding:\n",
    "                finding = 10\n",
    "                if a10 < num_imgs:\n",
    "                    y.append(finding)\n",
    "                    x.append(cv2.resize(cv2.imread(img), (WIDTH,HEIGHT), interpolation=cv2.INTER_CUBIC))\n",
    "                    a10+=1\n",
    "            elif NoduleMass in finding:\n",
    "                finding = 11\n",
    "                if a11 < num_imgs:\n",
    "                    y.append(finding)\n",
    "                    x.append(cv2.resize(cv2.imread(img), (WIDTH,HEIGHT), interpolation=cv2.INTER_CUBIC))\n",
    "                    a11+=1\n",
    "            elif Hernia in finding:\n",
    "                finding = 12\n",
    "                if a12 < num_imgs:\n",
    "                    y.append(finding)\n",
    "                    x.append(cv2.resize(cv2.imread(img), (WIDTH,HEIGHT), interpolation=cv2.INTER_CUBIC))\n",
    "                    a12+=1\n",
    "            elif Atelectasis in finding:\n",
    "                finding = 13\n",
    "                if a13 < num_imgs:\n",
    "                    y.append(finding)\n",
    "                    x.append(cv2.resize(cv2.imread(img), (WIDTH,HEIGHT), interpolation=cv2.INTER_CUBIC))\n",
    "                    a13+=1\n",
    "            else:\n",
    "                continue\n",
    "        i+=1\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a96ec2f8-819d-4915-9417-3b5c558b3a30",
    "_uuid": "6729fc82857b68767aa2cc9708ce78a8491d906a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Iteration 1000\n",
      "Iteration 2000\n",
      "Iteration 3000\n",
      "Iteration 3000\n",
      "Iteration 4000\n",
      "Iteration 5000\n",
      "Iteration 6000\n",
      "Iteration 7000\n",
      "Iteration 8000\n",
      "Iteration 9000\n",
      "Iteration 10000\n",
      "Iteration 11000\n",
      "Iteration 12000\n",
      "Iteration 12000\n",
      "Iteration 13000\n",
      "Iteration 14000\n",
      "Iteration 15000\n",
      "Iteration 16000\n",
      "Iteration 17000\n"
     ]
    }
   ],
   "source": [
    "X,y = proc_images()\n",
    "df = pd.DataFrame()\n",
    "df[\"images\"]=X\n",
    "df[\"labels\"]=y\n",
    "print(len(df), df.images[0].shape)\n",
    "print(type(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13    300\n",
       "11    300\n",
       "7     300\n",
       "3     300\n",
       "2     300\n",
       "0     300\n",
       "1     245\n",
       "9     208\n",
       "10    187\n",
       "6     148\n",
       "5     143\n",
       "4     109\n",
       "8      50\n",
       "12     15\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1400 (79, 79, 3)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "# Save and load the X and y from previous times.\n",
    "# with open('newSampleXy128x128-diseasesOnly.pickle', 'rb') as f:\n",
    "#     X, y = pickle.load(f)  \n",
    "# with open('newSampleXy79x79-200imgs.pickle', 'rb') as f:\n",
    "#     X, y = pickle.load(f)  \n",
    "# with open('newSampleXy100x100-diseasesOnly.pickle', 'rb') as f:\n",
    "#     X, y = pickle.load(f)\n",
    "# with open('newSampleXy79x79-fulldata.pickle', 'rb') as f:\n",
    "#     X, y = pickle.load(f)\n",
    "# with open('newSampleXy128x128-1000each.pickle', 'rb') as f:\n",
    "#     X, y = pickle.load(f)\n",
    "with open('newSampleXy-79x79-100each.pickle', 'rb') as f:\n",
    "    X, y = pickle.load(f)  \n",
    "\n",
    "df = pd.DataFrame()\n",
    "df[\"images\"]=X\n",
    "df[\"labels\"]=y\n",
    "print(len(df), df.images[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e5bea62c-a1a4-4578-8ea0-bf7619c3cae0",
    "_uuid": "5358d2240be847464db555c481fd78570275f512"
   },
   "source": [
    "Describe new numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "4736747c-6a40-4fbc-94be-ab188a99e0af",
    "_uuid": "72d1c333d3f15537df56c90656e1371d521c030a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               images  labels\n",
      "0   [[[176, 176, 176], [30, 30, 30], [0, 0, 0], [0...      10\n",
      "1   [[[67, 67, 67], [63, 63, 63], [72, 72, 72], [8...       0\n",
      "2   [[[25, 25, 25], [11, 11, 11], [11, 11, 11], [1...      12\n",
      "3   [[[31, 31, 31], [19, 19, 19], [17, 17, 17], [1...      12\n",
      "4   [[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...      12\n",
      "5   [[[77, 77, 77], [68, 68, 68], [68, 68, 68], [6...      12\n",
      "6   [[[7, 7, 7], [7, 7, 7], [7, 7, 7], [7, 7, 7], ...      12\n",
      "7   [[[19, 19, 19], [2, 2, 2], [2, 2, 2], [4, 4, 4...      12\n",
      "8   [[[59, 59, 59], [13, 13, 13], [6, 6, 6], [7, 7...      12\n",
      "9   [[[28, 28, 28], [15, 15, 15], [12, 12, 12], [1...       0\n",
      "10  [[[89, 89, 89], [75, 75, 75], [61, 61, 61], [5...       0\n",
      "11  [[[3, 3, 3], [3, 3, 3], [3, 3, 3], [2, 2, 2], ...       0\n",
      "12  [[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...       0\n",
      "13  [[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...       0\n",
      "14  [[[6, 6, 6], [6, 6, 6], [6, 6, 6], [6, 6, 6], ...       0\n",
      "\n",
      "{0: 'No Finding', 1: 'Consolidation', 2: 'Infiltration', 3: 'Pneumothorax', 4: 'Edema', 5: 'Emphysema', 6: 'Fibrosis', 7: 'Effusion', 8: 'Pneumonia', 9: 'Pleural Thickening', 10: 'Cardiomegaly', 11: 'Nodule Mass', 12: 'Hernia', 13: 'Atelectasis'}\n"
     ]
    }
   ],
   "source": [
    "# dict_characters = {0: 'Consolidation', 1: 'Infiltration', 2: 'Pneumothorax', 3: 'Edema', \n",
    "#                    4: 'Emphysema', 5: 'Fibrosis', 6: 'Effusion', 7: 'Pneumonia', \n",
    "#                    8: 'Pleural Thickening', 9: 'Cardiomegaly', 10: \"Nodule Mass\", 11: 'Hernia', \n",
    "#                    12: \"Atelectasis\"}\n",
    "dict_characters = {0: 'No Finding', 1: 'Consolidation', 2: 'Infiltration', \n",
    "        3: 'Pneumothorax', 4: 'Edema', 5: 'Emphysema', 6: 'Fibrosis', 7: 'Effusion', 8: 'Pneumonia', 9: 'Pleural Thickening', 10: 'Cardiomegaly', 11: \"Nodule Mass\", 12: 'Hernia', 13: \"Atelectasis\"}\n",
    "\n",
    "print(df.head(15))\n",
    "print(\"\")\n",
    "print(dict_characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "de6e66bf-6c6d-4d0d-8622-7ad3d5eca805",
    "_uuid": "855243c5294d51f809735355af165ebac9419730"
   },
   "source": [
    "Describe the distribution of pixel intensities within a representative image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a31ce154-b1af-491a-9b4c-b692dfcf5b71",
    "_uuid": "94f38bd7ea78a5d05ec2fe6f082c19f9b282d366"
   },
   "source": [
    "Normalize the pixel intensities between zero and one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "27aef53a-b2e7-48d3-9bd3-eefef10ac4bf",
    "_uuid": "45576b0f2cae264045648da3a10072746430fa83"
   },
   "outputs": [],
   "source": [
    "# Make the input a Numpy array, and normalize it by max pixel intensity.\n",
    "X=np.array(X)\n",
    "X=X/255.0\n",
    "# import tensorflow as tf\n",
    "# tf.image.per_image_standardization(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b0aed36e-f20c-418e-8775-252529719c87",
    "_uuid": "ebd24913ed1c03798e8fe95df7620f57d961d69f"
   },
   "source": [
    "Describe distribution of class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "2d7fa292-92bb-4f89-9473-97cbc222ac64",
    "_uuid": "f70fd3d49148f917b4cff17117fd3b434248b72a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'No Finding', 1: 'Consolidation', 2: 'Infiltration', 3: 'Pneumothorax', 4: 'Edema', 5: 'Emphysema', 6: 'Fibrosis', 7: 'Effusion', 8: 'Pneumonia', 9: 'Pleural Thickening', 10: 'Cardiomegaly', 11: 'Nodule Mass', 12: 'Hernia', 13: 'Atelectasis'}\n",
      "13    100\n",
      "12    100\n",
      "11    100\n",
      "10    100\n",
      "9     100\n",
      "8     100\n",
      "7     100\n",
      "6     100\n",
      "5     100\n",
      "4     100\n",
      "3     100\n",
      "2     100\n",
      "1     100\n",
      "0     100\n",
      "Name: labels, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lab = df['labels']\n",
    "dist = lab.value_counts()\n",
    "sns.countplot(lab)\n",
    "print(dict_characters)\n",
    "print(dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1b7e7a67-4503-492f-a40b-6023f370a9bc",
    "_uuid": "64f919c9f991a227bb915c1729de3d59e2ea557a"
   },
   "source": [
    "We have imbalanced sample sizes.  This is a problem that needs to be addressed.\n",
    "\n",
    "But for now we can proceed with a preliminary analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "3ffa380e-f815-4bbb-badf-7f976e8c4fee",
    "_uuid": "705a7ea6da6ab0c74b91a69ae6e0dab8567769d1"
   },
   "outputs": [],
   "source": [
    "# Split train and test sets.\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "# Reduce Sample Size for DeBugging\n",
    "# X_train = X_train[0:5000] \n",
    "# Y_train = Y_train[0:5000]\n",
    "# X_test = X_test[0:2000] \n",
    "# Y_test = Y_test[0:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X, y, df #free up some memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "6d5a4e75-d8fb-4897-b6b8-401fb082b72b",
    "_uuid": "18ef548edb3ebee434a5d2c88022d04444b32d6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Shape: (980, 79, 79, 3)\n",
      "Testing Data Shape: (420, 79, 79, 3)\n",
      "Training Data Shape: 980 (79, 79, 3)\n",
      "Testing Data Shape: 420 (79, 79, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Data Shape:\", X_train.shape)\n",
    "print(\"Testing Data Shape:\", X_test.shape)\n",
    "print(\"Training Data Shape:\", len(X_train), X_train[0].shape)\n",
    "print(\"Testing Data Shape:\", len(X_test), X_test[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b086a703-969e-450c-9040-6b4515cc7ec5",
    "_uuid": "73ba314626afbfa2d83f2a91db7a52bb5236b1d1"
   },
   "source": [
    "Now I will try to use a vanilla CNN to predict each ailment based off of the X-Ray image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "a791729e-db89-4fee-9348-35cc35056835",
    "_uuid": "387d7c376f18bc5b14ac7859c6da95dddc9e4fd3"
   },
   "outputs": [],
   "source": [
    "# Encode labels to hot vectors (ex : 2 -> [0,0,1,0,0,0,0,0,0,0])\n",
    "Y_trainHot = to_categorical(Y_train, num_classes = 14)\n",
    "Y_testHot = to_categorical(Y_test, num_classes = 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "705f4edc-65cb-4e72-b574-18e7b0edd474",
    "_uuid": "cf638745b03fb7d335f0b7263d5f86f09249c21a"
   },
   "source": [
    "In order to avoid having a biased model because of skewed class sizes, I will modify the class_weights parameter in order to give more weight to the rare classes.  In this case the class_weights parameter will eventually be passed to the model.fit function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "336f519a-8498-4f31-972b-92821285c592",
    "_uuid": "0916cc3df986ee2018608659f91e53b228a0e868"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.16666667 0.98591549 1.01449275 1.02941176 0.90909091 0.98591549\n",
      " 1.         0.90909091 1.01449275 0.98591549 0.93333333 0.97222222\n",
      " 1.06060606 1.09375   ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import class_weight\n",
    "class_weight = class_weight.compute_class_weight('balanced', np.unique(Y_train), Y_train)\n",
    "# print(class_weight)\n",
    "# class_weight = 14*[1]\n",
    "print(class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "9c3d3492-95e6-47b3-a5fa-5fa45a202c8e",
    "_uuid": "59ba85f24ea7fa7bbf0241807c33397549b10d94"
   },
   "outputs": [],
   "source": [
    "# Helper Functions  Learning Curves and Confusion Matrix\n",
    "\n",
    "from keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "class MetricsCheckpoint(Callback):\n",
    "    \"\"\"Callback that saves metrics after each epoch\"\"\"\n",
    "    def __init__(self, savepath):\n",
    "        super(MetricsCheckpoint, self).__init__()\n",
    "        self.savepath = savepath\n",
    "        self.history = {}\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "        np.save(self.savepath, self.history)\n",
    "\n",
    "\n",
    "def plotKerasLearningCurve(history):\n",
    "    plt.figure(figsize=(10,5))\n",
    "    metrics = np.load('logs.npy')[()]\n",
    "    filt = history.history['accuracy'] # try to add 'loss' to see the loss learning curve\n",
    "    for k in filter(lambda x : np.any([kk in x for kk in filt]), metrics.keys()):\n",
    "        l = np.array(metrics[k])\n",
    "        plt.plot(l, c= 'r' if 'val' not in k else 'b', label='val' if 'val' in k else 'train')\n",
    "        x = np.argmin(l) if 'loss' in k else np.argmax(l)\n",
    "        y = l[x]\n",
    "        plt.scatter(x,y, lw=0, alpha=0.25, s=100, c='r' if 'val' not in k else 'b')\n",
    "        plt.text(x, y, '{} = {:.4f}'.format(x,y), size='15', color= 'r' if 'val' not in k else 'b')   \n",
    "    plt.legend(loc=4)\n",
    "    plt.axis([0, None, None, None]);\n",
    "    plt.grid()\n",
    "    plt.xlabel('Number of epochs')\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize = (5,5))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "def plot_learning_curve(history):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.savefig('./accuracy_curve.png')\n",
    "    #plt.clf()\n",
    "    # summarize history for loss\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.savefig('./loss_curve.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 5075168471269890568\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 398335804167857710\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 16097237190821074549\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 1547304960\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 5078186746162694374\n",
      "physical_device_desc: \"device: 0, name: GeForce 940MX, pci bus id: 0000:01:00.0, compute capability: 5.0\"\n",
      "]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/device:GPU:0'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "import tensorflow as tf\n",
    "tf.test.gpu_device_name()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "config = tf.ConfigProto( device_count = {'GPU': 1 , 'CPU': 1} ) \n",
    "sess = tf.Session(config=config) \n",
    "keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_cell_guid": "0a4269c3-393c-47a2-a34f-27ff64f49fd9",
    "_uuid": "1cd13911f68387a58c55bfe444786619752152fe",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/alan/miniconda2/envs/tfl/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/alan/miniconda2/envs/tfl/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/50\n",
      "607/606 [==============================] - 116s 191ms/step - loss: 2.4780 - accuracy: 0.1936 - val_loss: 11745.0140 - val_accuracy: 0.0906\n",
      "Epoch 2/50\n",
      "607/606 [==============================] - 56s 92ms/step - loss: 2.2304 - accuracy: 0.2010 - val_loss: 42302.0584 - val_accuracy: 0.0906\n",
      "Epoch 3/50\n",
      "607/606 [==============================] - 56s 92ms/step - loss: 2.2276 - accuracy: 0.1942 - val_loss: 43014.8735 - val_accuracy: 0.0906\n",
      "Epoch 4/50\n",
      "607/606 [==============================] - 56s 92ms/step - loss: 2.2446 - accuracy: 0.1944 - val_loss: 43438.2649 - val_accuracy: 0.0906\n",
      "Epoch 5/50\n",
      "607/606 [==============================] - 56s 92ms/step - loss: 2.2261 - accuracy: 0.2076 - val_loss: 44142.9551 - val_accuracy: 0.0906\n",
      "Epoch 6/50\n",
      "607/606 [==============================] - 56s 92ms/step - loss: 2.2395 - accuracy: 0.1942 - val_loss: 63297.2141 - val_accuracy: 0.0906\n",
      "Epoch 7/50\n",
      "607/606 [==============================] - 56s 92ms/step - loss: 2.2249 - accuracy: 0.2066 - val_loss: 61719.0436 - val_accuracy: 0.0906\n",
      "Epoch 8/50\n",
      "286/606 [=============>................] - ETA: 23s - loss: 2.2434 - accuracy: 0.1902"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-ea193dcca7b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunCNNconfusion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_trainHot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_testHot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m13\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0mplot_learning_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-ea193dcca7b1>\u001b[0m in \u001b[0;36mrunCNNconfusion\u001b[0;34m(a, b, c, d, e, f, g, h)\u001b[0m\n\u001b[1;32m     59\u001b[0m         history = model.fit_generator(datagen.flow(a,b, batch_size=10),\n\u001b[1;32m     60\u001b[0m                             \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                       validation_data = [c, d],callbacks = [MetricsCheckpoint('logs')])\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;31m#         history = model.fit(a,b, epochs=epochs, class_weight=e, batch_size = batch_size,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/tfl/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/tfl/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1730\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1732\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/tfl/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    218\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                                             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m                                             reset_metrics=False)\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/tfl/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1512\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1514\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1516\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/tfl/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3292\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m~/miniconda2/envs/tfl/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# from numba import jit, cuda \n",
    "# # function optimized to run on gpu  \n",
    "# @jit(target =\"cuda\") \n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "with tf.device('/device:GPU:0'):\n",
    "\n",
    "    def runCNNconfusion(a,b,c,d,e,f,g,h):\n",
    "        # In -> [[Conv2D->relu]*2 -> MaxPool2D -> Dropout]*2 -> Flatten -> Dense -> Dropout -> Out\n",
    "        batch_size = 10\n",
    "        num_classes = f\n",
    "        epochs = g\n",
    "        #img_rows, img_cols = X_train.shape[1],b.shape[2]\n",
    "        input_shape = (img_size, img_size, 3)\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(filters = 32, kernel_size = (3,3),padding = 'Same', \n",
    "                         activation ='relu', input_shape = input_shape,strides=h))\n",
    "        model.add(Conv2D(filters = 32, kernel_size = (3,3),padding = 'Same', \n",
    "                         activation ='relu'))\n",
    "        model.add(MaxPool2D(pool_size=(2,2)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n",
    "                         activation ='relu'))\n",
    "        model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n",
    "                         activation ='relu'))\n",
    "        model.add(MaxPool2D(pool_size=(2,2)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(filters = 32, kernel_size = (3,3),padding = 'Same', \n",
    "                         activation ='relu'))\n",
    "        model.add(Conv2D(filters = 32, kernel_size = (3,3),padding = 'Same', \n",
    "                         activation ='relu'))\n",
    "        model.add(MaxPool2D(pool_size=(2,2)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(GlobalAveragePooling2D())\n",
    "        model.add(Dense(1024, activation = \"relu\",kernel_regularizer=keras.regularizers.l2(0.0)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Dense(64, activation = \"relu\",kernel_regularizer=keras.regularizers.l2(0.0)))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Dense(num_classes, activation = \"softmax\"))\n",
    "        # Define the optimizer\n",
    "        optimizer = Adam(lr=0.01)\n",
    "        model.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "        datagen = ImageDataGenerator(\n",
    "            featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "            samplewise_center=False,  # set each sample mean to 0\n",
    "            featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "            samplewise_std_normalization=False,  # divide each input by its std\n",
    "            zca_whitening=False,  # apply ZCA whitening\n",
    "            rescale=1. / 255, #rescale\n",
    "            rotation_range=20,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "            width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "            height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "            horizontal_flip=True,  # randomly flip images\n",
    "            vertical_flip=False)  # randomly flip images\n",
    "        datagen.fit(a)\n",
    "        history = model.fit_generator(datagen.flow(a,b, batch_size=10),\n",
    "                            steps_per_epoch=len(a) / 32, epochs=epochs, class_weight = e,  \n",
    "                                      validation_data = [c, d],callbacks = [MetricsCheckpoint('logs')])\n",
    "        model.summary()\n",
    "#         history = model.fit(a,b, epochs=epochs, class_weight=e, batch_size = batch_size,\n",
    "#                             validation_data=(c,d), verbose=1,callbacks = [MetricsCheckpoint('logs')])\n",
    "        score = model.evaluate(c,d, verbose=0) \n",
    "        print('\\nKeras Vanilla CNN - accuracy:', score[1],'\\n')\n",
    "        Y_pred = model.predict(c)\n",
    "        print('\\n', sklearn.metrics.classification_report(np.where(d > 0)[1], np.argmax(Y_pred, axis=1), target_names=list(dict_characters.values())), sep='')    \n",
    "        Y_pred_classes = np.argmax(Y_pred,axis = 1) \n",
    "        Y_true = np.argmax(d,axis = 1) \n",
    "        confusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n",
    "        plot_confusion_matrix(confusion_mtx, classes = list(dict_characters.values()))\n",
    "        plt.show()\n",
    "        return model, history\n",
    "model, history = runCNNconfusion(X_train, Y_trainHot, X_test, Y_testHot,class_weight,13,50,1)\n",
    "plot_learning_curve(history)\n",
    "plt.show()\n",
    "plotKerasLearningCurve(history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model_CNN.h5\")\n",
    "print(\"Saved model to disk\")\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "Train on 8175 samples, validate on 3504 samples\n",
      "Epoch 1/100\n",
      "8175/8175 [==============================] - 61s 7ms/step - loss: 2.3155 - accuracy: 0.2210 - val_loss: 2.4333 - val_accuracy: 0.1821\n",
      "Epoch 2/100\n",
      "8175/8175 [==============================] - 63s 8ms/step - loss: 2.3243 - accuracy: 0.2148 - val_loss: 2.3547 - val_accuracy: 0.2072\n",
      "Epoch 3/100\n",
      "8175/8175 [==============================] - 63s 8ms/step - loss: 2.3216 - accuracy: 0.2144 - val_loss: 2.4211 - val_accuracy: 0.1826\n",
      "Epoch 4/100\n",
      "8175/8175 [==============================] - 63s 8ms/step - loss: 2.3188 - accuracy: 0.2190 - val_loss: 2.3486 - val_accuracy: 0.2069\n",
      "Epoch 5/100\n",
      "8175/8175 [==============================] - 62s 8ms/step - loss: 2.3173 - accuracy: 0.2159 - val_loss: 2.5433 - val_accuracy: 0.1610\n",
      "Epoch 6/100\n",
      "8175/8175 [==============================] - 64s 8ms/step - loss: 2.3183 - accuracy: 0.2143 - val_loss: 2.5088 - val_accuracy: 0.1735\n",
      "Epoch 7/100\n",
      "8175/8175 [==============================] - 64s 8ms/step - loss: 2.3214 - accuracy: 0.2114 - val_loss: 2.3673 - val_accuracy: 0.1906\n",
      "Epoch 8/100\n",
      "8175/8175 [==============================] - 63s 8ms/step - loss: 2.3215 - accuracy: 0.2133 - val_loss: 2.4447 - val_accuracy: 0.1744\n",
      "Epoch 9/100\n",
      "8175/8175 [==============================] - 62s 8ms/step - loss: 2.3154 - accuracy: 0.2209 - val_loss: 2.6415 - val_accuracy: 0.1396\n",
      "Epoch 10/100\n",
      "8175/8175 [==============================] - 64s 8ms/step - loss: 2.3154 - accuracy: 0.2169 - val_loss: 2.3724 - val_accuracy: 0.1929\n",
      "Epoch 11/100\n",
      "8175/8175 [==============================] - 64s 8ms/step - loss: 2.3114 - accuracy: 0.2171 - val_loss: 2.3792 - val_accuracy: 0.1864\n",
      "Epoch 12/100\n",
      "8175/8175 [==============================] - 63s 8ms/step - loss: 2.3137 - accuracy: 0.2141 - val_loss: 2.3764 - val_accuracy: 0.1872\n",
      "Epoch 13/100\n",
      "8175/8175 [==============================] - 62s 8ms/step - loss: 2.3217 - accuracy: 0.2191 - val_loss: 2.4765 - val_accuracy: 0.1670\n",
      "Epoch 14/100\n",
      "8175/8175 [==============================] - 64s 8ms/step - loss: 2.3200 - accuracy: 0.2177 - val_loss: 2.5767 - val_accuracy: 0.1664\n",
      "Epoch 15/100\n",
      "8175/8175 [==============================] - 64s 8ms/step - loss: 2.3146 - accuracy: 0.2164 - val_loss: 2.3318 - val_accuracy: 0.2149\n",
      "Epoch 16/100\n",
      "8175/8175 [==============================] - 63s 8ms/step - loss: 2.3105 - accuracy: 0.2193 - val_loss: 2.3913 - val_accuracy: 0.1901\n",
      "Epoch 17/100\n",
      "8175/8175 [==============================] - 62s 8ms/step - loss: 2.3147 - accuracy: 0.2122 - val_loss: 2.3448 - val_accuracy: 0.2129\n",
      "Epoch 18/100\n",
      "8175/8175 [==============================] - 63s 8ms/step - loss: 2.3149 - accuracy: 0.2199 - val_loss: 2.3843 - val_accuracy: 0.1949\n",
      "Epoch 19/100\n",
      "8175/8175 [==============================] - 64s 8ms/step - loss: 2.3103 - accuracy: 0.2207 - val_loss: 2.3452 - val_accuracy: 0.2160\n",
      "Epoch 20/100\n",
      "4810/8175 [================>.............] - ETA: 23s - loss: 2.3053 - accuracy: 0.2193"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-842756fd53cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mloaded_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'SGD'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m model.fit(X_train,Y_trainHot, epochs=100, class_weight=class_weight, batch_size = 10,\n\u001b[0;32m---> 15\u001b[0;31m                             validation_data=(X_test,Y_testHot), verbose=1,callbacks = [MetricsCheckpoint('logs')])\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;31m# score = loaded_model.evaluate(X, y, verbose=0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/tfl/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m~/miniconda2/envs/tfl/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/tfl/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3292\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m~/miniconda2/envs/tfl/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# later...\n",
    " \n",
    "# load json and create model\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model_CNN.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    " \n",
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss='categorical_crossentropy', optimizer='SGD', metrics=['accuracy'])\n",
    "model.fit(X_train,Y_trainHot, epochs=100, class_weight=class_weight, batch_size = 10,\n",
    "                            validation_data=(X_test,Y_testHot), verbose=1,callbacks = [MetricsCheckpoint('logs')])\n",
    "# score = loaded_model.evaluate(X, y, verbose=0)\n",
    "# print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.client.session.Session at 0x7fbffee0ea90>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "sess\n",
    "# from tensorflow.v1 import ConfigProto\n",
    "# from tensorflow.v1 import InteractiveSession\n",
    "\n",
    "# config = ConfigProto()\n",
    "# config.gpu_options.allow_growth = True\n",
    "# session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 6864416466928320855\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 16188865283982699463\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 14591045856191284591\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 1570504704\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 12430948907066180109\n",
      "physical_device_desc: \"device: 0, name: GeForce 940MX, pci bus id: 0000:01:00.0, compute capability: 5.0\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_cell_guid": "5f8a5979-838d-4f0e-8b0a-e601a8b660e6",
    "_uuid": "9b46de5b9536718d54f39ed593ee68bbcf53da1e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 79, 79, 3)         0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 79, 79, 64)        1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 79, 79, 64)        36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 39, 39, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 39, 39, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 39, 39, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 19, 19, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 19, 19, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 19, 19, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 19, 19, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 9, 9, 256)         0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 9, 9, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 512)               1049088   \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 14)                1806      \n",
      "=================================================================\n",
      "Total params: 15,831,246\n",
      "Trainable params: 1,116,558\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n",
      "Train on 1120 samples, validate on 280 samples\n",
      "Epoch 1/1000\n",
      "1120/1120 [==============================] - 9s 8ms/step - loss: 2.6627 - accuracy: 0.0705 - val_loss: 2.6443 - val_accuracy: 0.0357\n",
      "Epoch 2/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.6279 - accuracy: 0.0938 - val_loss: 2.6189 - val_accuracy: 0.1036\n",
      "Epoch 3/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.6059 - accuracy: 0.0991 - val_loss: 2.6058 - val_accuracy: 0.1500\n",
      "Epoch 4/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.5948 - accuracy: 0.1196 - val_loss: 2.6041 - val_accuracy: 0.1107\n",
      "Epoch 5/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.5853 - accuracy: 0.1018 - val_loss: 2.5912 - val_accuracy: 0.1357\n",
      "Epoch 6/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.5749 - accuracy: 0.1179 - val_loss: 2.5950 - val_accuracy: 0.1464\n",
      "Epoch 7/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.5667 - accuracy: 0.1321 - val_loss: 2.5994 - val_accuracy: 0.1000\n",
      "Epoch 8/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.5569 - accuracy: 0.1321 - val_loss: 2.5890 - val_accuracy: 0.1071\n",
      "Epoch 9/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.5529 - accuracy: 0.1125 - val_loss: 2.5812 - val_accuracy: 0.1429\n",
      "Epoch 10/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.5323 - accuracy: 0.1482 - val_loss: 2.5701 - val_accuracy: 0.1429\n",
      "Epoch 11/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.5345 - accuracy: 0.1437 - val_loss: 2.5613 - val_accuracy: 0.1393\n",
      "Epoch 12/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.5243 - accuracy: 0.1312 - val_loss: 2.5778 - val_accuracy: 0.1107\n",
      "Epoch 13/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.5173 - accuracy: 0.1411 - val_loss: 2.5540 - val_accuracy: 0.1071\n",
      "Epoch 14/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.5139 - accuracy: 0.1509 - val_loss: 2.5494 - val_accuracy: 0.1321\n",
      "Epoch 15/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.5015 - accuracy: 0.1714 - val_loss: 2.5356 - val_accuracy: 0.1571\n",
      "Epoch 16/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.4936 - accuracy: 0.1536 - val_loss: 2.5468 - val_accuracy: 0.1286\n",
      "Epoch 17/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.4863 - accuracy: 0.1723 - val_loss: 2.5560 - val_accuracy: 0.1179\n",
      "Epoch 18/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.4838 - accuracy: 0.1616 - val_loss: 2.5454 - val_accuracy: 0.1214\n",
      "Epoch 19/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.4788 - accuracy: 0.1714 - val_loss: 2.5446 - val_accuracy: 0.1250\n",
      "Epoch 20/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.4705 - accuracy: 0.1705 - val_loss: 2.5478 - val_accuracy: 0.1500\n",
      "Epoch 21/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.4638 - accuracy: 0.1830 - val_loss: 2.5251 - val_accuracy: 0.1536\n",
      "Epoch 22/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.4582 - accuracy: 0.1857 - val_loss: 2.5382 - val_accuracy: 0.1179\n",
      "Epoch 23/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.4543 - accuracy: 0.1804 - val_loss: 2.5152 - val_accuracy: 0.1429\n",
      "Epoch 24/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.4484 - accuracy: 0.1866 - val_loss: 2.5369 - val_accuracy: 0.1500\n",
      "Epoch 25/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.4368 - accuracy: 0.1902 - val_loss: 2.5282 - val_accuracy: 0.1536\n",
      "Epoch 26/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.4392 - accuracy: 0.1759 - val_loss: 2.5230 - val_accuracy: 0.1536\n",
      "Epoch 27/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.4224 - accuracy: 0.1937 - val_loss: 2.5436 - val_accuracy: 0.1393\n",
      "Epoch 28/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.4243 - accuracy: 0.2018 - val_loss: 2.5638 - val_accuracy: 0.1321\n",
      "Epoch 29/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.4222 - accuracy: 0.1813 - val_loss: 2.5532 - val_accuracy: 0.1143\n",
      "Epoch 30/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.4163 - accuracy: 0.1929 - val_loss: 2.5253 - val_accuracy: 0.1429\n",
      "Epoch 31/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.4086 - accuracy: 0.2179 - val_loss: 2.5192 - val_accuracy: 0.1679\n",
      "Epoch 32/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.3992 - accuracy: 0.2054 - val_loss: 2.5183 - val_accuracy: 0.1536\n",
      "Epoch 33/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.4011 - accuracy: 0.1821 - val_loss: 2.5200 - val_accuracy: 0.1286\n",
      "Epoch 34/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.3929 - accuracy: 0.2116 - val_loss: 2.5140 - val_accuracy: 0.1357\n",
      "Epoch 35/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.3878 - accuracy: 0.2036 - val_loss: 2.5182 - val_accuracy: 0.1429\n",
      "Epoch 36/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.3828 - accuracy: 0.2080 - val_loss: 2.4933 - val_accuracy: 0.1821\n",
      "Epoch 37/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.3783 - accuracy: 0.2143 - val_loss: 2.5186 - val_accuracy: 0.1500\n",
      "Epoch 38/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.3755 - accuracy: 0.2143 - val_loss: 2.5157 - val_accuracy: 0.1571\n",
      "Epoch 39/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.3622 - accuracy: 0.2214 - val_loss: 2.4830 - val_accuracy: 0.1607\n",
      "Epoch 40/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.3599 - accuracy: 0.2179 - val_loss: 2.5183 - val_accuracy: 0.1536\n",
      "Epoch 41/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.3494 - accuracy: 0.2196 - val_loss: 2.5249 - val_accuracy: 0.1250\n",
      "Epoch 42/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.3469 - accuracy: 0.2196 - val_loss: 2.5083 - val_accuracy: 0.1786\n",
      "Epoch 43/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.3418 - accuracy: 0.2357 - val_loss: 2.5123 - val_accuracy: 0.1500\n",
      "Epoch 44/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.3378 - accuracy: 0.2152 - val_loss: 2.4975 - val_accuracy: 0.1571\n",
      "Epoch 45/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.3450 - accuracy: 0.2277 - val_loss: 2.5122 - val_accuracy: 0.1643\n",
      "Epoch 46/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.3232 - accuracy: 0.2366 - val_loss: 2.4899 - val_accuracy: 0.1679\n",
      "Epoch 47/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.3256 - accuracy: 0.2321 - val_loss: 2.4774 - val_accuracy: 0.1750\n",
      "Epoch 48/1000\n",
      "1120/1120 [==============================] - 7s 7ms/step - loss: 2.3193 - accuracy: 0.2366 - val_loss: 2.5059 - val_accuracy: 0.1500\n",
      "Epoch 49/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.3103 - accuracy: 0.2554 - val_loss: 2.4962 - val_accuracy: 0.1571\n",
      "Epoch 50/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.3125 - accuracy: 0.2393 - val_loss: 2.4879 - val_accuracy: 0.1571\n",
      "Epoch 51/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.3019 - accuracy: 0.2509 - val_loss: 2.4961 - val_accuracy: 0.1786\n",
      "Epoch 52/1000\n",
      "1120/1120 [==============================] - 7s 7ms/step - loss: 2.3002 - accuracy: 0.2411 - val_loss: 2.4796 - val_accuracy: 0.1821\n",
      "Epoch 53/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.3011 - accuracy: 0.2375 - val_loss: 2.5117 - val_accuracy: 0.1893\n",
      "Epoch 54/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.2883 - accuracy: 0.2482 - val_loss: 2.4799 - val_accuracy: 0.1714\n",
      "Epoch 55/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.2851 - accuracy: 0.2545 - val_loss: 2.4924 - val_accuracy: 0.1571\n",
      "Epoch 56/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.2784 - accuracy: 0.2670 - val_loss: 2.5062 - val_accuracy: 0.1679\n",
      "Epoch 57/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.2693 - accuracy: 0.2554 - val_loss: 2.5749 - val_accuracy: 0.1321\n",
      "Epoch 58/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.2743 - accuracy: 0.2571 - val_loss: 2.5651 - val_accuracy: 0.1214\n",
      "Epoch 59/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.2652 - accuracy: 0.2580 - val_loss: 2.5016 - val_accuracy: 0.1571\n",
      "Epoch 60/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.2588 - accuracy: 0.2598 - val_loss: 2.4734 - val_accuracy: 0.1536\n",
      "Epoch 61/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.2639 - accuracy: 0.2670 - val_loss: 2.4756 - val_accuracy: 0.1714\n",
      "Epoch 62/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.2584 - accuracy: 0.2750 - val_loss: 2.5157 - val_accuracy: 0.1357\n",
      "Epoch 63/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.2447 - accuracy: 0.2545 - val_loss: 2.5354 - val_accuracy: 0.1857\n",
      "Epoch 64/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.2467 - accuracy: 0.2723 - val_loss: 2.5041 - val_accuracy: 0.1679\n",
      "Epoch 65/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.2381 - accuracy: 0.2643 - val_loss: 2.5186 - val_accuracy: 0.1536\n",
      "Epoch 66/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.2353 - accuracy: 0.2812 - val_loss: 2.4883 - val_accuracy: 0.1643\n",
      "Epoch 67/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.2355 - accuracy: 0.2795 - val_loss: 2.4776 - val_accuracy: 0.1536\n",
      "Epoch 68/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.2210 - accuracy: 0.2830 - val_loss: 2.5556 - val_accuracy: 0.1429\n",
      "Epoch 69/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.2213 - accuracy: 0.2679 - val_loss: 2.4729 - val_accuracy: 0.1786\n",
      "Epoch 70/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.2214 - accuracy: 0.2893 - val_loss: 2.4937 - val_accuracy: 0.1750\n",
      "Epoch 71/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.2154 - accuracy: 0.2893 - val_loss: 2.4814 - val_accuracy: 0.1714\n",
      "Epoch 72/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.2058 - accuracy: 0.2723 - val_loss: 2.5005 - val_accuracy: 0.1643\n",
      "Epoch 73/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.2010 - accuracy: 0.2839 - val_loss: 2.5067 - val_accuracy: 0.1929\n",
      "Epoch 74/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.1977 - accuracy: 0.2893 - val_loss: 2.5038 - val_accuracy: 0.1893\n",
      "Epoch 75/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.2011 - accuracy: 0.2777 - val_loss: 2.5343 - val_accuracy: 0.1786\n",
      "Epoch 76/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.1846 - accuracy: 0.2768 - val_loss: 2.4908 - val_accuracy: 0.1929\n",
      "Epoch 77/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.1993 - accuracy: 0.2875 - val_loss: 2.4881 - val_accuracy: 0.1500\n",
      "Epoch 78/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.1773 - accuracy: 0.3036 - val_loss: 2.4949 - val_accuracy: 0.1607\n",
      "Epoch 79/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.1693 - accuracy: 0.3063 - val_loss: 2.5315 - val_accuracy: 0.1607\n",
      "Epoch 80/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.1742 - accuracy: 0.2902 - val_loss: 2.5013 - val_accuracy: 0.1750\n",
      "Epoch 81/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.1669 - accuracy: 0.3009 - val_loss: 2.4809 - val_accuracy: 0.1786\n",
      "Epoch 82/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.1579 - accuracy: 0.2911 - val_loss: 2.4741 - val_accuracy: 0.1821\n",
      "Epoch 83/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.1491 - accuracy: 0.3071 - val_loss: 2.5497 - val_accuracy: 0.1679\n",
      "Epoch 84/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.1556 - accuracy: 0.3036 - val_loss: 2.4707 - val_accuracy: 0.1893\n",
      "Epoch 85/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.1504 - accuracy: 0.3063 - val_loss: 2.4931 - val_accuracy: 0.1750\n",
      "Epoch 86/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.1517 - accuracy: 0.2991 - val_loss: 2.4937 - val_accuracy: 0.1893\n",
      "Epoch 87/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.1353 - accuracy: 0.3125 - val_loss: 2.5623 - val_accuracy: 0.1536\n",
      "Epoch 88/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.1340 - accuracy: 0.3036 - val_loss: 2.5179 - val_accuracy: 0.1929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.1294 - accuracy: 0.3196 - val_loss: 2.4939 - val_accuracy: 0.1929\n",
      "Epoch 90/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.1157 - accuracy: 0.3054 - val_loss: 2.5208 - val_accuracy: 0.1714\n",
      "Epoch 91/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.1262 - accuracy: 0.3223 - val_loss: 2.5011 - val_accuracy: 0.1643\n",
      "Epoch 92/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.1189 - accuracy: 0.3179 - val_loss: 2.5279 - val_accuracy: 0.1714\n",
      "Epoch 93/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.1193 - accuracy: 0.3125 - val_loss: 2.4878 - val_accuracy: 0.1964\n",
      "Epoch 94/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.1062 - accuracy: 0.3259 - val_loss: 2.4758 - val_accuracy: 0.1679\n",
      "Epoch 95/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.1017 - accuracy: 0.3268 - val_loss: 2.4892 - val_accuracy: 0.1679\n",
      "Epoch 96/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.1061 - accuracy: 0.3250 - val_loss: 2.5230 - val_accuracy: 0.1857\n",
      "Epoch 97/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.0952 - accuracy: 0.3313 - val_loss: 2.4997 - val_accuracy: 0.1750\n",
      "Epoch 98/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.0887 - accuracy: 0.3366 - val_loss: 2.4826 - val_accuracy: 0.1750\n",
      "Epoch 99/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.0816 - accuracy: 0.3196 - val_loss: 2.5610 - val_accuracy: 0.1714\n",
      "Epoch 100/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.0885 - accuracy: 0.3286 - val_loss: 2.6217 - val_accuracy: 0.1464\n",
      "Epoch 101/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.0919 - accuracy: 0.3196 - val_loss: 2.4604 - val_accuracy: 0.1929\n",
      "Epoch 102/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.0702 - accuracy: 0.3411 - val_loss: 2.5109 - val_accuracy: 0.1893\n",
      "Epoch 103/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.0673 - accuracy: 0.3330 - val_loss: 2.5037 - val_accuracy: 0.1857\n",
      "Epoch 104/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.0606 - accuracy: 0.3330 - val_loss: 2.5051 - val_accuracy: 0.1964\n",
      "Epoch 105/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.0599 - accuracy: 0.3295 - val_loss: 2.4803 - val_accuracy: 0.1786\n",
      "Epoch 106/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.0566 - accuracy: 0.3339 - val_loss: 2.4619 - val_accuracy: 0.2036\n",
      "Epoch 107/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.0604 - accuracy: 0.3313 - val_loss: 2.4865 - val_accuracy: 0.1893\n",
      "Epoch 108/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.0496 - accuracy: 0.3375 - val_loss: 2.4659 - val_accuracy: 0.2036\n",
      "Epoch 109/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.0598 - accuracy: 0.3446 - val_loss: 2.5198 - val_accuracy: 0.2071\n",
      "Epoch 110/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.0375 - accuracy: 0.3562 - val_loss: 2.5387 - val_accuracy: 0.1679\n",
      "Epoch 111/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.0454 - accuracy: 0.3446 - val_loss: 2.5387 - val_accuracy: 0.1893\n",
      "Epoch 112/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.0416 - accuracy: 0.3491 - val_loss: 2.4846 - val_accuracy: 0.1964\n",
      "Epoch 113/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.0274 - accuracy: 0.3500 - val_loss: 2.4893 - val_accuracy: 0.1893\n",
      "Epoch 114/1000\n",
      "1120/1120 [==============================] - 7s 7ms/step - loss: 2.0305 - accuracy: 0.3455 - val_loss: 2.5029 - val_accuracy: 0.1893\n",
      "Epoch 115/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.0292 - accuracy: 0.3509 - val_loss: 2.4834 - val_accuracy: 0.1643\n",
      "Epoch 116/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.0189 - accuracy: 0.3393 - val_loss: 2.4998 - val_accuracy: 0.1679\n",
      "Epoch 117/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.0022 - accuracy: 0.3616 - val_loss: 2.4798 - val_accuracy: 0.2036\n",
      "Epoch 118/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.0053 - accuracy: 0.3571 - val_loss: 2.5301 - val_accuracy: 0.1786\n",
      "Epoch 119/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.0012 - accuracy: 0.3554 - val_loss: 2.5435 - val_accuracy: 0.1500\n",
      "Epoch 120/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.0013 - accuracy: 0.3580 - val_loss: 2.5415 - val_accuracy: 0.1643\n",
      "Epoch 121/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 2.0026 - accuracy: 0.3652 - val_loss: 2.4800 - val_accuracy: 0.1964\n",
      "Epoch 122/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.9969 - accuracy: 0.3616 - val_loss: 2.5574 - val_accuracy: 0.1893\n",
      "Epoch 123/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.9864 - accuracy: 0.3732 - val_loss: 2.4651 - val_accuracy: 0.1893\n",
      "Epoch 124/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.9865 - accuracy: 0.3750 - val_loss: 2.5442 - val_accuracy: 0.1714\n",
      "Epoch 125/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.9878 - accuracy: 0.3670 - val_loss: 2.4958 - val_accuracy: 0.1857\n",
      "Epoch 126/1000\n",
      "1120/1120 [==============================] - 9s 8ms/step - loss: 1.9836 - accuracy: 0.3795 - val_loss: 2.5375 - val_accuracy: 0.1786\n",
      "Epoch 127/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.9763 - accuracy: 0.3562 - val_loss: 2.4938 - val_accuracy: 0.2071\n",
      "Epoch 128/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.9639 - accuracy: 0.3821 - val_loss: 2.4647 - val_accuracy: 0.1786\n",
      "Epoch 129/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.9495 - accuracy: 0.3830 - val_loss: 2.5169 - val_accuracy: 0.1643\n",
      "Epoch 130/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.9482 - accuracy: 0.3866 - val_loss: 2.4879 - val_accuracy: 0.1893\n",
      "Epoch 131/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.9498 - accuracy: 0.3839 - val_loss: 2.5229 - val_accuracy: 0.1893\n",
      "Epoch 132/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.9637 - accuracy: 0.3795 - val_loss: 2.5222 - val_accuracy: 0.1821\n",
      "Epoch 133/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.9414 - accuracy: 0.3866 - val_loss: 2.5216 - val_accuracy: 0.1893\n",
      "Epoch 134/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.9352 - accuracy: 0.3866 - val_loss: 2.5013 - val_accuracy: 0.2071\n",
      "Epoch 135/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.9430 - accuracy: 0.3741 - val_loss: 2.4887 - val_accuracy: 0.1821\n",
      "Epoch 136/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.9363 - accuracy: 0.3705 - val_loss: 2.5323 - val_accuracy: 0.1714\n",
      "Epoch 137/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.9260 - accuracy: 0.4018 - val_loss: 2.4857 - val_accuracy: 0.1679\n",
      "Epoch 138/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.9292 - accuracy: 0.3875 - val_loss: 2.6090 - val_accuracy: 0.1571\n",
      "Epoch 139/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.9317 - accuracy: 0.3982 - val_loss: 2.5075 - val_accuracy: 0.2107\n",
      "Epoch 140/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.9271 - accuracy: 0.3786 - val_loss: 2.4843 - val_accuracy: 0.1821\n",
      "Epoch 141/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.9161 - accuracy: 0.3911 - val_loss: 2.4955 - val_accuracy: 0.2071\n",
      "Epoch 142/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.9090 - accuracy: 0.3812 - val_loss: 2.5400 - val_accuracy: 0.1893\n",
      "Epoch 143/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.8955 - accuracy: 0.3982 - val_loss: 2.4965 - val_accuracy: 0.1786\n",
      "Epoch 144/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.9053 - accuracy: 0.4027 - val_loss: 2.4831 - val_accuracy: 0.2036\n",
      "Epoch 145/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.8929 - accuracy: 0.4000 - val_loss: 2.4964 - val_accuracy: 0.1964\n",
      "Epoch 146/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.8914 - accuracy: 0.3973 - val_loss: 2.5340 - val_accuracy: 0.1571\n",
      "Epoch 147/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.8887 - accuracy: 0.3991 - val_loss: 2.4719 - val_accuracy: 0.2286\n",
      "Epoch 148/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.8934 - accuracy: 0.3982 - val_loss: 2.4761 - val_accuracy: 0.2143\n",
      "Epoch 149/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.8783 - accuracy: 0.4089 - val_loss: 2.5317 - val_accuracy: 0.1857\n",
      "Epoch 150/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.8579 - accuracy: 0.4250 - val_loss: 2.5037 - val_accuracy: 0.1750\n",
      "Epoch 151/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.8670 - accuracy: 0.4080 - val_loss: 2.4821 - val_accuracy: 0.1857\n",
      "Epoch 152/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.8746 - accuracy: 0.3964 - val_loss: 2.4898 - val_accuracy: 0.2000\n",
      "Epoch 153/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.8672 - accuracy: 0.3982 - val_loss: 2.5094 - val_accuracy: 0.1964\n",
      "Epoch 154/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.8671 - accuracy: 0.4080 - val_loss: 2.6012 - val_accuracy: 0.1786\n",
      "Epoch 155/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.8585 - accuracy: 0.4116 - val_loss: 2.6225 - val_accuracy: 0.1893\n",
      "Epoch 156/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.8852 - accuracy: 0.4116 - val_loss: 2.5110 - val_accuracy: 0.2000\n",
      "Epoch 157/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.8579 - accuracy: 0.4036 - val_loss: 2.5041 - val_accuracy: 0.1750\n",
      "Epoch 158/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.8684 - accuracy: 0.3964 - val_loss: 2.4997 - val_accuracy: 0.1750\n",
      "Epoch 159/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.8418 - accuracy: 0.4304 - val_loss: 2.5397 - val_accuracy: 0.1821\n",
      "Epoch 160/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.8382 - accuracy: 0.4161 - val_loss: 2.5261 - val_accuracy: 0.1821\n",
      "Epoch 161/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.8277 - accuracy: 0.4339 - val_loss: 2.5097 - val_accuracy: 0.1821\n",
      "Epoch 162/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.8266 - accuracy: 0.4098 - val_loss: 2.5115 - val_accuracy: 0.1929\n",
      "Epoch 163/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.8144 - accuracy: 0.4295 - val_loss: 2.5740 - val_accuracy: 0.1429\n",
      "Epoch 164/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.8110 - accuracy: 0.4223 - val_loss: 2.6141 - val_accuracy: 0.1714\n",
      "Epoch 165/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.8095 - accuracy: 0.4375 - val_loss: 2.5877 - val_accuracy: 0.1821\n",
      "Epoch 166/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.8181 - accuracy: 0.4241 - val_loss: 2.5039 - val_accuracy: 0.1714\n",
      "Epoch 167/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.7935 - accuracy: 0.4330 - val_loss: 2.5543 - val_accuracy: 0.1893\n",
      "Epoch 168/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.8241 - accuracy: 0.4286 - val_loss: 2.5706 - val_accuracy: 0.1786\n",
      "Epoch 169/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.8010 - accuracy: 0.4313 - val_loss: 2.5522 - val_accuracy: 0.1786\n",
      "Epoch 170/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.7911 - accuracy: 0.4223 - val_loss: 2.5605 - val_accuracy: 0.1786\n",
      "Epoch 171/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.7845 - accuracy: 0.4357 - val_loss: 2.5649 - val_accuracy: 0.1857\n",
      "Epoch 172/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.7799 - accuracy: 0.4536 - val_loss: 2.4896 - val_accuracy: 0.1821\n",
      "Epoch 173/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.7739 - accuracy: 0.4527 - val_loss: 2.5049 - val_accuracy: 0.1929\n",
      "Epoch 174/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.7733 - accuracy: 0.4455 - val_loss: 2.5310 - val_accuracy: 0.1929\n",
      "Epoch 175/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.7683 - accuracy: 0.4571 - val_loss: 2.5242 - val_accuracy: 0.1786\n",
      "Epoch 176/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.7569 - accuracy: 0.4473 - val_loss: 2.5617 - val_accuracy: 0.1893\n",
      "Epoch 177/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.7709 - accuracy: 0.4375 - val_loss: 2.5576 - val_accuracy: 0.2036\n",
      "Epoch 178/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.7597 - accuracy: 0.4357 - val_loss: 2.5621 - val_accuracy: 0.1643\n",
      "Epoch 179/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.7548 - accuracy: 0.4464 - val_loss: 2.5378 - val_accuracy: 0.2000\n",
      "Epoch 180/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.7450 - accuracy: 0.4411 - val_loss: 2.5961 - val_accuracy: 0.1893\n",
      "Epoch 181/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.7480 - accuracy: 0.4473 - val_loss: 2.5852 - val_accuracy: 0.2286\n",
      "Epoch 182/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.7541 - accuracy: 0.4473 - val_loss: 2.5525 - val_accuracy: 0.1821\n",
      "Epoch 183/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.7412 - accuracy: 0.4670 - val_loss: 2.5323 - val_accuracy: 0.1714\n",
      "Epoch 184/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.7366 - accuracy: 0.4446 - val_loss: 2.5143 - val_accuracy: 0.1679\n",
      "Epoch 185/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.7410 - accuracy: 0.4420 - val_loss: 2.6236 - val_accuracy: 0.1893\n",
      "Epoch 186/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.7317 - accuracy: 0.4429 - val_loss: 2.6013 - val_accuracy: 0.1821\n",
      "Epoch 187/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.7363 - accuracy: 0.4661 - val_loss: 2.5467 - val_accuracy: 0.1679\n",
      "Epoch 188/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.7311 - accuracy: 0.4473 - val_loss: 2.5096 - val_accuracy: 0.1893\n",
      "Epoch 189/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.7247 - accuracy: 0.4437 - val_loss: 2.5366 - val_accuracy: 0.1714\n",
      "Epoch 190/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.7160 - accuracy: 0.4473 - val_loss: 2.6209 - val_accuracy: 0.1786\n",
      "Epoch 191/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.7157 - accuracy: 0.4500 - val_loss: 2.6627 - val_accuracy: 0.1643\n",
      "Epoch 192/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.7263 - accuracy: 0.4402 - val_loss: 2.6254 - val_accuracy: 0.1786\n",
      "Epoch 193/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.7040 - accuracy: 0.4804 - val_loss: 2.5614 - val_accuracy: 0.1714\n",
      "Epoch 194/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.7105 - accuracy: 0.4527 - val_loss: 2.5864 - val_accuracy: 0.1929\n",
      "Epoch 195/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.7232 - accuracy: 0.4536 - val_loss: 2.5846 - val_accuracy: 0.1964\n",
      "Epoch 196/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.6876 - accuracy: 0.4705 - val_loss: 2.7020 - val_accuracy: 0.1607\n",
      "Epoch 197/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.6655 - accuracy: 0.4688 - val_loss: 2.5396 - val_accuracy: 0.2000\n",
      "Epoch 198/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.6898 - accuracy: 0.4589 - val_loss: 2.6011 - val_accuracy: 0.1821\n",
      "Epoch 199/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.6865 - accuracy: 0.4598 - val_loss: 2.6025 - val_accuracy: 0.1786\n",
      "Epoch 200/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.6649 - accuracy: 0.4598 - val_loss: 2.6227 - val_accuracy: 0.1857\n",
      "Epoch 201/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.6784 - accuracy: 0.4598 - val_loss: 2.5926 - val_accuracy: 0.1857\n",
      "Epoch 202/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.6648 - accuracy: 0.4750 - val_loss: 2.5567 - val_accuracy: 0.1821\n",
      "Epoch 203/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.6503 - accuracy: 0.4705 - val_loss: 2.4995 - val_accuracy: 0.1893\n",
      "Epoch 204/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.6227 - accuracy: 0.4955 - val_loss: 2.5809 - val_accuracy: 0.1857\n",
      "Epoch 205/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.6700 - accuracy: 0.4688 - val_loss: 2.5348 - val_accuracy: 0.1857\n",
      "Epoch 206/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.6443 - accuracy: 0.4848 - val_loss: 2.6316 - val_accuracy: 0.1857\n",
      "Epoch 207/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.6447 - accuracy: 0.4893 - val_loss: 2.5985 - val_accuracy: 0.1857\n",
      "Epoch 208/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.6390 - accuracy: 0.4821 - val_loss: 2.5733 - val_accuracy: 0.2071\n",
      "Epoch 209/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.6322 - accuracy: 0.4830 - val_loss: 2.6025 - val_accuracy: 0.1964\n",
      "Epoch 210/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.6213 - accuracy: 0.4938 - val_loss: 2.5691 - val_accuracy: 0.1750\n",
      "Epoch 211/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.6438 - accuracy: 0.4795 - val_loss: 2.6907 - val_accuracy: 0.1429\n",
      "Epoch 212/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.6520 - accuracy: 0.4786 - val_loss: 2.5738 - val_accuracy: 0.2000\n",
      "Epoch 213/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.6424 - accuracy: 0.4768 - val_loss: 2.5627 - val_accuracy: 0.1714\n",
      "Epoch 214/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.6123 - accuracy: 0.5214 - val_loss: 2.7296 - val_accuracy: 0.1429\n",
      "Epoch 215/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.6042 - accuracy: 0.4973 - val_loss: 2.6389 - val_accuracy: 0.1857\n",
      "Epoch 216/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.5882 - accuracy: 0.4964 - val_loss: 2.6244 - val_accuracy: 0.1893\n",
      "Epoch 217/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.6361 - accuracy: 0.4848 - val_loss: 2.5404 - val_accuracy: 0.1893\n",
      "Epoch 218/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.6235 - accuracy: 0.4768 - val_loss: 2.6882 - val_accuracy: 0.1643\n",
      "Epoch 219/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.6158 - accuracy: 0.4938 - val_loss: 2.6096 - val_accuracy: 0.1679\n",
      "Epoch 220/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.6263 - accuracy: 0.4830 - val_loss: 2.7631 - val_accuracy: 0.1607\n",
      "Epoch 221/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.5756 - accuracy: 0.5205 - val_loss: 2.5635 - val_accuracy: 0.1714\n",
      "Epoch 222/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.6224 - accuracy: 0.4920 - val_loss: 2.6466 - val_accuracy: 0.2143\n",
      "Epoch 223/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.5993 - accuracy: 0.4938 - val_loss: 2.6362 - val_accuracy: 0.1679\n",
      "Epoch 224/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.5929 - accuracy: 0.5054 - val_loss: 2.6763 - val_accuracy: 0.1821\n",
      "Epoch 225/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.5787 - accuracy: 0.5063 - val_loss: 2.7125 - val_accuracy: 0.1857\n",
      "Epoch 226/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.5829 - accuracy: 0.5027 - val_loss: 2.7105 - val_accuracy: 0.1821\n",
      "Epoch 227/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.5625 - accuracy: 0.5143 - val_loss: 2.5596 - val_accuracy: 0.1929\n",
      "Epoch 228/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.5808 - accuracy: 0.4929 - val_loss: 2.6118 - val_accuracy: 0.1607\n",
      "Epoch 229/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.5797 - accuracy: 0.5152 - val_loss: 2.5986 - val_accuracy: 0.1821\n",
      "Epoch 230/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.6022 - accuracy: 0.5188 - val_loss: 2.6273 - val_accuracy: 0.2071\n",
      "Epoch 231/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.5302 - accuracy: 0.5277 - val_loss: 2.6457 - val_accuracy: 0.1929\n",
      "Epoch 232/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.5774 - accuracy: 0.4973 - val_loss: 2.7327 - val_accuracy: 0.1571\n",
      "Epoch 233/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.5425 - accuracy: 0.5161 - val_loss: 2.6634 - val_accuracy: 0.1893\n",
      "Epoch 234/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.5463 - accuracy: 0.5089 - val_loss: 2.7682 - val_accuracy: 0.1786\n",
      "Epoch 235/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.5693 - accuracy: 0.5161 - val_loss: 2.6016 - val_accuracy: 0.1857\n",
      "Epoch 236/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.5384 - accuracy: 0.5286 - val_loss: 2.5555 - val_accuracy: 0.2143\n",
      "Epoch 237/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.5182 - accuracy: 0.5259 - val_loss: 2.6097 - val_accuracy: 0.2107\n",
      "Epoch 238/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.5406 - accuracy: 0.5161 - val_loss: 2.6067 - val_accuracy: 0.1893\n",
      "Epoch 239/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.6076 - accuracy: 0.4830 - val_loss: 2.8790 - val_accuracy: 0.1286\n",
      "Epoch 240/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.5496 - accuracy: 0.5000 - val_loss: 2.6848 - val_accuracy: 0.1607\n",
      "Epoch 241/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.5162 - accuracy: 0.5205 - val_loss: 2.7762 - val_accuracy: 0.1679\n",
      "Epoch 242/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.5232 - accuracy: 0.5098 - val_loss: 2.9385 - val_accuracy: 0.1536\n",
      "Epoch 243/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.5206 - accuracy: 0.5089 - val_loss: 2.7102 - val_accuracy: 0.1821\n",
      "Epoch 244/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.5278 - accuracy: 0.5107 - val_loss: 2.7212 - val_accuracy: 0.1750\n",
      "Epoch 245/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.5065 - accuracy: 0.5232 - val_loss: 2.6728 - val_accuracy: 0.1964\n",
      "Epoch 246/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.4958 - accuracy: 0.5402 - val_loss: 2.6171 - val_accuracy: 0.2036\n",
      "Epoch 247/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.5183 - accuracy: 0.5250 - val_loss: 2.8291 - val_accuracy: 0.1536\n",
      "Epoch 248/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.4966 - accuracy: 0.5304 - val_loss: 2.5883 - val_accuracy: 0.1929\n",
      "Epoch 249/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.5012 - accuracy: 0.5268 - val_loss: 2.5492 - val_accuracy: 0.2429\n",
      "Epoch 250/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.4571 - accuracy: 0.5500 - val_loss: 2.5833 - val_accuracy: 0.2000\n",
      "Epoch 251/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.5117 - accuracy: 0.5134 - val_loss: 2.6579 - val_accuracy: 0.1821\n",
      "Epoch 252/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.4850 - accuracy: 0.5250 - val_loss: 2.7282 - val_accuracy: 0.1786\n",
      "Epoch 253/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.5123 - accuracy: 0.5268 - val_loss: 2.6770 - val_accuracy: 0.1821\n",
      "Epoch 254/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.5214 - accuracy: 0.5054 - val_loss: 2.6938 - val_accuracy: 0.1821\n",
      "Epoch 255/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.4590 - accuracy: 0.5384 - val_loss: 2.7935 - val_accuracy: 0.1821\n",
      "Epoch 256/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.4383 - accuracy: 0.5527 - val_loss: 2.7610 - val_accuracy: 0.1786\n",
      "Epoch 257/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.4679 - accuracy: 0.5312 - val_loss: 2.7970 - val_accuracy: 0.1714\n",
      "Epoch 258/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.4785 - accuracy: 0.5286 - val_loss: 2.7633 - val_accuracy: 0.1679\n",
      "Epoch 259/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.4861 - accuracy: 0.5250 - val_loss: 2.7276 - val_accuracy: 0.1893\n",
      "Epoch 260/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.4211 - accuracy: 0.5625 - val_loss: 2.8002 - val_accuracy: 0.1929\n",
      "Epoch 261/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.4605 - accuracy: 0.5429 - val_loss: 2.6401 - val_accuracy: 0.1893\n",
      "Epoch 262/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.4383 - accuracy: 0.5473 - val_loss: 2.6453 - val_accuracy: 0.2107\n",
      "Epoch 263/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.4714 - accuracy: 0.5304 - val_loss: 2.7418 - val_accuracy: 0.1857\n",
      "Epoch 264/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.4137 - accuracy: 0.5652 - val_loss: 2.6128 - val_accuracy: 0.1857\n",
      "Epoch 265/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.4513 - accuracy: 0.5304 - val_loss: 2.9041 - val_accuracy: 0.1643\n",
      "Epoch 266/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.4363 - accuracy: 0.5536 - val_loss: 2.7477 - val_accuracy: 0.1893\n",
      "Epoch 267/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.4174 - accuracy: 0.5518 - val_loss: 2.6897 - val_accuracy: 0.2071\n",
      "Epoch 268/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.4513 - accuracy: 0.5420 - val_loss: 2.7355 - val_accuracy: 0.1786\n",
      "Epoch 269/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.4593 - accuracy: 0.5411 - val_loss: 2.6901 - val_accuracy: 0.1821\n",
      "Epoch 270/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.4205 - accuracy: 0.5437 - val_loss: 2.9175 - val_accuracy: 0.1607\n",
      "Epoch 271/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.4220 - accuracy: 0.5509 - val_loss: 2.8775 - val_accuracy: 0.1536\n",
      "Epoch 272/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.4087 - accuracy: 0.5607 - val_loss: 2.7212 - val_accuracy: 0.2107\n",
      "Epoch 273/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.4032 - accuracy: 0.5464 - val_loss: 2.7114 - val_accuracy: 0.1857\n",
      "Epoch 274/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.4751 - accuracy: 0.5357 - val_loss: 2.7110 - val_accuracy: 0.2000\n",
      "Epoch 275/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.4593 - accuracy: 0.5330 - val_loss: 2.7429 - val_accuracy: 0.1750\n",
      "Epoch 276/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.4231 - accuracy: 0.5437 - val_loss: 2.6897 - val_accuracy: 0.1857\n",
      "Epoch 277/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.3772 - accuracy: 0.5598 - val_loss: 2.6799 - val_accuracy: 0.1857\n",
      "Epoch 278/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.4126 - accuracy: 0.5509 - val_loss: 2.7651 - val_accuracy: 0.1786\n",
      "Epoch 279/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.3729 - accuracy: 0.5705 - val_loss: 2.7209 - val_accuracy: 0.1857\n",
      "Epoch 280/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.3469 - accuracy: 0.5893 - val_loss: 2.6710 - val_accuracy: 0.1964\n",
      "Epoch 281/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.4020 - accuracy: 0.5571 - val_loss: 2.6390 - val_accuracy: 0.2000\n",
      "Epoch 282/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.3661 - accuracy: 0.5643 - val_loss: 2.6903 - val_accuracy: 0.1714\n",
      "Epoch 283/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.3805 - accuracy: 0.5607 - val_loss: 2.6348 - val_accuracy: 0.2036\n",
      "Epoch 284/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.3782 - accuracy: 0.5509 - val_loss: 2.7248 - val_accuracy: 0.2214\n",
      "Epoch 285/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.3641 - accuracy: 0.5705 - val_loss: 2.6552 - val_accuracy: 0.1857\n",
      "Epoch 286/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.3788 - accuracy: 0.5670 - val_loss: 2.7028 - val_accuracy: 0.1786\n",
      "Epoch 287/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.3484 - accuracy: 0.5795 - val_loss: 2.6865 - val_accuracy: 0.2071\n",
      "Epoch 288/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.3312 - accuracy: 0.6018 - val_loss: 2.6732 - val_accuracy: 0.2107\n",
      "Epoch 289/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.4074 - accuracy: 0.5518 - val_loss: 2.8088 - val_accuracy: 0.1893\n",
      "Epoch 290/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.3501 - accuracy: 0.5723 - val_loss: 2.6581 - val_accuracy: 0.1821\n",
      "Epoch 291/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.3752 - accuracy: 0.5661 - val_loss: 2.7294 - val_accuracy: 0.1929\n",
      "Epoch 292/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.3685 - accuracy: 0.5571 - val_loss: 2.9811 - val_accuracy: 0.1893\n",
      "Epoch 293/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.3821 - accuracy: 0.5670 - val_loss: 2.6302 - val_accuracy: 0.2071\n",
      "Epoch 294/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.3216 - accuracy: 0.5902 - val_loss: 2.6612 - val_accuracy: 0.2071\n",
      "Epoch 295/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.2998 - accuracy: 0.5848 - val_loss: 2.8039 - val_accuracy: 0.1786\n",
      "Epoch 296/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.3250 - accuracy: 0.5723 - val_loss: 2.8349 - val_accuracy: 0.2000\n",
      "Epoch 297/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.3408 - accuracy: 0.5643 - val_loss: 2.7804 - val_accuracy: 0.1893\n",
      "Epoch 298/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.3360 - accuracy: 0.6000 - val_loss: 2.7346 - val_accuracy: 0.1643\n",
      "Epoch 299/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.3089 - accuracy: 0.5938 - val_loss: 2.7606 - val_accuracy: 0.1750\n",
      "Epoch 300/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.3282 - accuracy: 0.5848 - val_loss: 2.8144 - val_accuracy: 0.2036\n",
      "Epoch 301/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.3109 - accuracy: 0.5777 - val_loss: 2.6306 - val_accuracy: 0.2000\n",
      "Epoch 302/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.2772 - accuracy: 0.6143 - val_loss: 2.8401 - val_accuracy: 0.2000\n",
      "Epoch 303/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.3216 - accuracy: 0.5866 - val_loss: 2.9107 - val_accuracy: 0.1607\n",
      "Epoch 304/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.2632 - accuracy: 0.6036 - val_loss: 2.6781 - val_accuracy: 0.2000\n",
      "Epoch 305/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.2861 - accuracy: 0.5857 - val_loss: 2.8259 - val_accuracy: 0.1893\n",
      "Epoch 306/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.3343 - accuracy: 0.5821 - val_loss: 2.7578 - val_accuracy: 0.1821\n",
      "Epoch 307/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.2930 - accuracy: 0.6009 - val_loss: 2.7443 - val_accuracy: 0.1821\n",
      "Epoch 308/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.2697 - accuracy: 0.6098 - val_loss: 3.5507 - val_accuracy: 0.1429\n",
      "Epoch 309/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.2810 - accuracy: 0.6089 - val_loss: 2.6704 - val_accuracy: 0.2036\n",
      "Epoch 310/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.3117 - accuracy: 0.5893 - val_loss: 2.8768 - val_accuracy: 0.1679\n",
      "Epoch 311/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.2487 - accuracy: 0.6027 - val_loss: 2.7526 - val_accuracy: 0.1929\n",
      "Epoch 312/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.2586 - accuracy: 0.6080 - val_loss: 2.9100 - val_accuracy: 0.1893\n",
      "Epoch 313/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.2350 - accuracy: 0.6089 - val_loss: 2.8802 - val_accuracy: 0.1679\n",
      "Epoch 314/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.2396 - accuracy: 0.6232 - val_loss: 2.7195 - val_accuracy: 0.2036\n",
      "Epoch 315/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.2585 - accuracy: 0.6152 - val_loss: 2.7451 - val_accuracy: 0.1893\n",
      "Epoch 316/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.2372 - accuracy: 0.6071 - val_loss: 2.6952 - val_accuracy: 0.2036\n",
      "Epoch 317/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.3107 - accuracy: 0.5991 - val_loss: 3.0366 - val_accuracy: 0.1500\n",
      "Epoch 318/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.3096 - accuracy: 0.5973 - val_loss: 2.7275 - val_accuracy: 0.1679\n",
      "Epoch 319/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.2177 - accuracy: 0.6152 - val_loss: 3.0505 - val_accuracy: 0.1714\n",
      "Epoch 320/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.2356 - accuracy: 0.6241 - val_loss: 2.7404 - val_accuracy: 0.1857\n",
      "Epoch 321/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.3096 - accuracy: 0.5804 - val_loss: 2.9205 - val_accuracy: 0.1714\n",
      "Epoch 322/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.2299 - accuracy: 0.6232 - val_loss: 3.0059 - val_accuracy: 0.1964\n",
      "Epoch 323/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.1923 - accuracy: 0.6250 - val_loss: 2.8615 - val_accuracy: 0.2071\n",
      "Epoch 324/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.1859 - accuracy: 0.6455 - val_loss: 2.9153 - val_accuracy: 0.1857\n",
      "Epoch 325/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.2333 - accuracy: 0.6080 - val_loss: 2.7376 - val_accuracy: 0.2071\n",
      "Epoch 326/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.2317 - accuracy: 0.6196 - val_loss: 3.0586 - val_accuracy: 0.1679\n",
      "Epoch 327/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.1683 - accuracy: 0.6482 - val_loss: 2.8637 - val_accuracy: 0.1821\n",
      "Epoch 328/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.1974 - accuracy: 0.6223 - val_loss: 2.9377 - val_accuracy: 0.2000\n",
      "Epoch 329/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.1903 - accuracy: 0.6357 - val_loss: 3.1510 - val_accuracy: 0.1464\n",
      "Epoch 330/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.2563 - accuracy: 0.6045 - val_loss: 2.7326 - val_accuracy: 0.2036\n",
      "Epoch 331/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.2140 - accuracy: 0.6259 - val_loss: 2.7611 - val_accuracy: 0.2036\n",
      "Epoch 332/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.1804 - accuracy: 0.6464 - val_loss: 2.7541 - val_accuracy: 0.2036\n",
      "Epoch 333/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.2257 - accuracy: 0.6080 - val_loss: 2.8518 - val_accuracy: 0.2000\n",
      "Epoch 334/1000\n",
      "1120/1120 [==============================] - 7s 7ms/step - loss: 1.2231 - accuracy: 0.6241 - val_loss: 2.8407 - val_accuracy: 0.1786\n",
      "Epoch 335/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.2057 - accuracy: 0.6214 - val_loss: 3.2785 - val_accuracy: 0.1286\n",
      "Epoch 336/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.2189 - accuracy: 0.6054 - val_loss: 3.0842 - val_accuracy: 0.1750\n",
      "Epoch 337/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.2056 - accuracy: 0.6313 - val_loss: 2.8419 - val_accuracy: 0.2036\n",
      "Epoch 338/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.2199 - accuracy: 0.6187 - val_loss: 3.0366 - val_accuracy: 0.1429\n",
      "Epoch 339/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.1470 - accuracy: 0.6375 - val_loss: 2.9038 - val_accuracy: 0.1929\n",
      "Epoch 340/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.1799 - accuracy: 0.6384 - val_loss: 2.7570 - val_accuracy: 0.2107\n",
      "Epoch 341/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.1513 - accuracy: 0.6313 - val_loss: 2.9313 - val_accuracy: 0.1750\n",
      "Epoch 342/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.2031 - accuracy: 0.6080 - val_loss: 2.8387 - val_accuracy: 0.1714\n",
      "Epoch 343/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.1406 - accuracy: 0.6464 - val_loss: 2.7924 - val_accuracy: 0.1893\n",
      "Epoch 344/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.1905 - accuracy: 0.6232 - val_loss: 3.4191 - val_accuracy: 0.1143\n",
      "Epoch 345/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.1329 - accuracy: 0.6393 - val_loss: 2.9855 - val_accuracy: 0.1821\n",
      "Epoch 346/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.2081 - accuracy: 0.6214 - val_loss: 2.7713 - val_accuracy: 0.1786\n",
      "Epoch 347/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.1390 - accuracy: 0.6384 - val_loss: 2.8144 - val_accuracy: 0.1964\n",
      "Epoch 348/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.1512 - accuracy: 0.6482 - val_loss: 2.8594 - val_accuracy: 0.1857\n",
      "Epoch 349/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.0735 - accuracy: 0.6795 - val_loss: 2.9574 - val_accuracy: 0.1929\n",
      "Epoch 350/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.1484 - accuracy: 0.6205 - val_loss: 2.8520 - val_accuracy: 0.1821\n",
      "Epoch 351/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.1646 - accuracy: 0.6250 - val_loss: 2.8945 - val_accuracy: 0.1714\n",
      "Epoch 352/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.1308 - accuracy: 0.6527 - val_loss: 2.8155 - val_accuracy: 0.2036\n",
      "Epoch 353/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.1183 - accuracy: 0.6580 - val_loss: 3.1203 - val_accuracy: 0.2036\n",
      "Epoch 354/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.1336 - accuracy: 0.6429 - val_loss: 3.1321 - val_accuracy: 0.1857\n",
      "Epoch 355/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.1776 - accuracy: 0.6214 - val_loss: 2.8089 - val_accuracy: 0.1893\n",
      "Epoch 356/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.0306 - accuracy: 0.7063 - val_loss: 3.0545 - val_accuracy: 0.1964\n",
      "Epoch 357/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.0738 - accuracy: 0.6795 - val_loss: 2.8527 - val_accuracy: 0.1929\n",
      "Epoch 358/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.0507 - accuracy: 0.6804 - val_loss: 2.9766 - val_accuracy: 0.2143\n",
      "Epoch 359/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.0732 - accuracy: 0.6687 - val_loss: 2.7907 - val_accuracy: 0.1857\n",
      "Epoch 360/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.1059 - accuracy: 0.6473 - val_loss: 2.8321 - val_accuracy: 0.2321\n",
      "Epoch 361/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.1184 - accuracy: 0.6589 - val_loss: 2.8873 - val_accuracy: 0.1750\n",
      "Epoch 362/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.1018 - accuracy: 0.6670 - val_loss: 2.8302 - val_accuracy: 0.1929\n",
      "Epoch 363/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.1218 - accuracy: 0.6589 - val_loss: 3.0121 - val_accuracy: 0.1750\n",
      "Epoch 364/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.0614 - accuracy: 0.6795 - val_loss: 2.8053 - val_accuracy: 0.2036\n",
      "Epoch 365/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.1101 - accuracy: 0.6482 - val_loss: 2.8189 - val_accuracy: 0.1964\n",
      "Epoch 366/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.0928 - accuracy: 0.6580 - val_loss: 2.8203 - val_accuracy: 0.1929\n",
      "Epoch 367/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.0777 - accuracy: 0.6661 - val_loss: 3.1079 - val_accuracy: 0.1750\n",
      "Epoch 368/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.1180 - accuracy: 0.6500 - val_loss: 2.7968 - val_accuracy: 0.1821\n",
      "Epoch 369/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.0248 - accuracy: 0.6884 - val_loss: 3.5844 - val_accuracy: 0.1286\n",
      "Epoch 370/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.0822 - accuracy: 0.6643 - val_loss: 2.9374 - val_accuracy: 0.1571\n",
      "Epoch 371/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.1170 - accuracy: 0.6536 - val_loss: 2.8653 - val_accuracy: 0.2000\n",
      "Epoch 372/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.0519 - accuracy: 0.6821 - val_loss: 3.0483 - val_accuracy: 0.1536\n",
      "Epoch 373/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.0814 - accuracy: 0.6509 - val_loss: 2.8598 - val_accuracy: 0.1929\n",
      "Epoch 374/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.0680 - accuracy: 0.6625 - val_loss: 2.9374 - val_accuracy: 0.1964\n",
      "Epoch 375/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.0485 - accuracy: 0.6705 - val_loss: 3.0439 - val_accuracy: 0.1786\n",
      "Epoch 376/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.0157 - accuracy: 0.6875 - val_loss: 3.0802 - val_accuracy: 0.1643\n",
      "Epoch 377/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.0654 - accuracy: 0.6696 - val_loss: 2.9460 - val_accuracy: 0.1821\n",
      "Epoch 378/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.0069 - accuracy: 0.6991 - val_loss: 2.8497 - val_accuracy: 0.2107\n",
      "Epoch 379/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.0136 - accuracy: 0.7098 - val_loss: 3.2118 - val_accuracy: 0.1429\n",
      "Epoch 380/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.0576 - accuracy: 0.6661 - val_loss: 2.9946 - val_accuracy: 0.1893\n",
      "Epoch 381/1000\n",
      "1120/1120 [==============================] - 7s 7ms/step - loss: 0.9823 - accuracy: 0.7027 - val_loss: 2.9165 - val_accuracy: 0.1714\n",
      "Epoch 382/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.0137 - accuracy: 0.6768 - val_loss: 2.8598 - val_accuracy: 0.1893\n",
      "Epoch 383/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.0552 - accuracy: 0.6741 - val_loss: 2.9499 - val_accuracy: 0.1929\n",
      "Epoch 384/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.0131 - accuracy: 0.6875 - val_loss: 3.0994 - val_accuracy: 0.1964\n",
      "Epoch 385/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.9563 - accuracy: 0.7259 - val_loss: 2.9141 - val_accuracy: 0.1750\n",
      "Epoch 386/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.0307 - accuracy: 0.6821 - val_loss: 3.0171 - val_accuracy: 0.2071\n",
      "Epoch 387/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.9799 - accuracy: 0.7125 - val_loss: 2.9087 - val_accuracy: 0.2107\n",
      "Epoch 388/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.0128 - accuracy: 0.6759 - val_loss: 3.0540 - val_accuracy: 0.1821\n",
      "Epoch 389/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.0181 - accuracy: 0.6911 - val_loss: 2.9735 - val_accuracy: 0.1679\n",
      "Epoch 390/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.0012 - accuracy: 0.6893 - val_loss: 3.0410 - val_accuracy: 0.1857\n",
      "Epoch 391/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.9961 - accuracy: 0.6946 - val_loss: 3.0261 - val_accuracy: 0.1893\n",
      "Epoch 392/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.0273 - accuracy: 0.6911 - val_loss: 2.8776 - val_accuracy: 0.2036\n",
      "Epoch 393/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.9993 - accuracy: 0.6875 - val_loss: 2.9387 - val_accuracy: 0.1750\n",
      "Epoch 394/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.9846 - accuracy: 0.6911 - val_loss: 2.9059 - val_accuracy: 0.2071\n",
      "Epoch 395/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.0709 - accuracy: 0.6723 - val_loss: 3.2326 - val_accuracy: 0.1750\n",
      "Epoch 396/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.0663 - accuracy: 0.6696 - val_loss: 2.9612 - val_accuracy: 0.2250\n",
      "Epoch 397/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.9761 - accuracy: 0.7134 - val_loss: 2.8367 - val_accuracy: 0.2036\n",
      "Epoch 398/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.0140 - accuracy: 0.6973 - val_loss: 2.9740 - val_accuracy: 0.1929\n",
      "Epoch 399/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.9371 - accuracy: 0.7232 - val_loss: 3.0276 - val_accuracy: 0.1893\n",
      "Epoch 400/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.9975 - accuracy: 0.6920 - val_loss: 2.9939 - val_accuracy: 0.1964\n",
      "Epoch 401/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 1.0528 - accuracy: 0.6723 - val_loss: 3.2675 - val_accuracy: 0.1571\n",
      "Epoch 402/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.9711 - accuracy: 0.7116 - val_loss: 2.9790 - val_accuracy: 0.2107\n",
      "Epoch 403/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.8845 - accuracy: 0.7491 - val_loss: 3.1450 - val_accuracy: 0.1821\n",
      "Epoch 404/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.9550 - accuracy: 0.7205 - val_loss: 3.1975 - val_accuracy: 0.1929\n",
      "Epoch 405/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.9524 - accuracy: 0.7161 - val_loss: 3.0518 - val_accuracy: 0.1714\n",
      "Epoch 406/1000\n",
      "1120/1120 [==============================] - 7s 7ms/step - loss: 1.0616 - accuracy: 0.6857 - val_loss: 2.8431 - val_accuracy: 0.2036\n",
      "Epoch 407/1000\n",
      "1120/1120 [==============================] - 7s 7ms/step - loss: 0.9155 - accuracy: 0.7259 - val_loss: 3.0019 - val_accuracy: 0.1643\n",
      "Epoch 408/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.8642 - accuracy: 0.7509 - val_loss: 2.9914 - val_accuracy: 0.2107\n",
      "Epoch 409/1000\n",
      "1120/1120 [==============================] - 7s 6ms/step - loss: 0.9011 - accuracy: 0.7196 - val_loss: 3.1466 - val_accuracy: 0.1893\n",
      "Epoch 410/1000\n",
      "1120/1120 [==============================] - 7s 6ms/step - loss: 0.9384 - accuracy: 0.7152 - val_loss: 3.0500 - val_accuracy: 0.1679\n",
      "Epoch 411/1000\n",
      "1120/1120 [==============================] - 7s 7ms/step - loss: 0.9554 - accuracy: 0.6866 - val_loss: 2.9984 - val_accuracy: 0.2286\n",
      "Epoch 412/1000\n",
      "1120/1120 [==============================] - 7s 7ms/step - loss: 0.8614 - accuracy: 0.7482 - val_loss: 3.1929 - val_accuracy: 0.1821\n",
      "Epoch 413/1000\n",
      "1120/1120 [==============================] - 7s 7ms/step - loss: 1.0365 - accuracy: 0.6643 - val_loss: 3.2756 - val_accuracy: 0.1857\n",
      "Epoch 414/1000\n",
      "1120/1120 [==============================] - 7s 6ms/step - loss: 0.9475 - accuracy: 0.7152 - val_loss: 2.8792 - val_accuracy: 0.2143\n",
      "Epoch 415/1000\n",
      "1120/1120 [==============================] - 7s 6ms/step - loss: 0.9301 - accuracy: 0.7071 - val_loss: 3.4518 - val_accuracy: 0.1571\n",
      "Epoch 416/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.9469 - accuracy: 0.7080 - val_loss: 3.1302 - val_accuracy: 0.1786\n",
      "Epoch 417/1000\n",
      "1120/1120 [==============================] - 7s 6ms/step - loss: 0.9310 - accuracy: 0.7241 - val_loss: 2.9910 - val_accuracy: 0.1964\n",
      "Epoch 418/1000\n",
      "1120/1120 [==============================] - 7s 6ms/step - loss: 0.8653 - accuracy: 0.7411 - val_loss: 3.2667 - val_accuracy: 0.1929\n",
      "Epoch 419/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.8206 - accuracy: 0.7554 - val_loss: 3.0982 - val_accuracy: 0.1893\n",
      "Epoch 420/1000\n",
      "1120/1120 [==============================] - 7s 7ms/step - loss: 0.9152 - accuracy: 0.7009 - val_loss: 3.0364 - val_accuracy: 0.1929\n",
      "Epoch 421/1000\n",
      "1120/1120 [==============================] - 7s 7ms/step - loss: 0.8520 - accuracy: 0.7545 - val_loss: 3.0704 - val_accuracy: 0.1857\n",
      "Epoch 422/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.8413 - accuracy: 0.7491 - val_loss: 3.3049 - val_accuracy: 0.1714\n",
      "Epoch 423/1000\n",
      "1120/1120 [==============================] - 7s 6ms/step - loss: 0.8360 - accuracy: 0.7616 - val_loss: 3.3107 - val_accuracy: 0.1714\n",
      "Epoch 424/1000\n",
      "1120/1120 [==============================] - 7s 6ms/step - loss: 0.8332 - accuracy: 0.7679 - val_loss: 3.0818 - val_accuracy: 0.1786\n",
      "Epoch 425/1000\n",
      "1120/1120 [==============================] - 7s 6ms/step - loss: 0.8361 - accuracy: 0.7688 - val_loss: 3.1518 - val_accuracy: 0.1750\n",
      "Epoch 426/1000\n",
      "1120/1120 [==============================] - 7s 6ms/step - loss: 0.8884 - accuracy: 0.7375 - val_loss: 3.2497 - val_accuracy: 0.1786\n",
      "Epoch 427/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.8717 - accuracy: 0.7518 - val_loss: 3.0173 - val_accuracy: 0.1929\n",
      "Epoch 428/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.8824 - accuracy: 0.7357 - val_loss: 2.9385 - val_accuracy: 0.2143\n",
      "Epoch 429/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.8227 - accuracy: 0.7652 - val_loss: 3.1889 - val_accuracy: 0.1821\n",
      "Epoch 430/1000\n",
      "1120/1120 [==============================] - 7s 6ms/step - loss: 0.8729 - accuracy: 0.7286 - val_loss: 2.9124 - val_accuracy: 0.2143\n",
      "Epoch 431/1000\n",
      "1120/1120 [==============================] - 7s 6ms/step - loss: 0.7977 - accuracy: 0.7723 - val_loss: 2.9825 - val_accuracy: 0.2214\n",
      "Epoch 432/1000\n",
      "1120/1120 [==============================] - 7s 6ms/step - loss: 0.8324 - accuracy: 0.7625 - val_loss: 3.4822 - val_accuracy: 0.1679\n",
      "Epoch 433/1000\n",
      "1120/1120 [==============================] - 7s 6ms/step - loss: 0.8736 - accuracy: 0.7393 - val_loss: 3.2462 - val_accuracy: 0.1929\n",
      "Epoch 434/1000\n",
      "1120/1120 [==============================] - 7s 6ms/step - loss: 0.8139 - accuracy: 0.7563 - val_loss: 3.3521 - val_accuracy: 0.1536\n",
      "Epoch 435/1000\n",
      "1120/1120 [==============================] - 7s 7ms/step - loss: 0.8332 - accuracy: 0.7696 - val_loss: 3.0652 - val_accuracy: 0.1857\n",
      "Epoch 436/1000\n",
      "1120/1120 [==============================] - 7s 7ms/step - loss: 0.8360 - accuracy: 0.7545 - val_loss: 3.1220 - val_accuracy: 0.1964\n",
      "Epoch 437/1000\n",
      "1120/1120 [==============================] - 7s 7ms/step - loss: 0.7618 - accuracy: 0.7839 - val_loss: 3.2161 - val_accuracy: 0.1750\n",
      "Epoch 438/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.8331 - accuracy: 0.7384 - val_loss: 3.4258 - val_accuracy: 0.1929\n",
      "Epoch 439/1000\n",
      "1120/1120 [==============================] - 7s 7ms/step - loss: 0.7964 - accuracy: 0.7723 - val_loss: 3.5271 - val_accuracy: 0.1857\n",
      "Epoch 440/1000\n",
      "1120/1120 [==============================] - 7s 6ms/step - loss: 0.8085 - accuracy: 0.7634 - val_loss: 3.1934 - val_accuracy: 0.1607\n",
      "Epoch 441/1000\n",
      "1120/1120 [==============================] - 7s 6ms/step - loss: 0.8652 - accuracy: 0.7330 - val_loss: 3.3843 - val_accuracy: 0.1857\n",
      "Epoch 442/1000\n",
      "1120/1120 [==============================] - 7s 6ms/step - loss: 0.8231 - accuracy: 0.7598 - val_loss: 3.4666 - val_accuracy: 0.1750\n",
      "Epoch 443/1000\n",
      "1120/1120 [==============================] - 7s 6ms/step - loss: 0.7995 - accuracy: 0.7670 - val_loss: 3.2170 - val_accuracy: 0.1929\n",
      "Epoch 444/1000\n",
      "1120/1120 [==============================] - 7s 7ms/step - loss: 0.8117 - accuracy: 0.7554 - val_loss: 3.0655 - val_accuracy: 0.1929\n",
      "Epoch 445/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.8173 - accuracy: 0.7500 - val_loss: 3.1277 - val_accuracy: 0.1821\n",
      "Epoch 446/1000\n",
      "1120/1120 [==============================] - 7s 6ms/step - loss: 0.8602 - accuracy: 0.7366 - val_loss: 3.0262 - val_accuracy: 0.2036\n",
      "Epoch 447/1000\n",
      "1120/1120 [==============================] - 7s 6ms/step - loss: 0.8636 - accuracy: 0.7446 - val_loss: 3.1646 - val_accuracy: 0.1714\n",
      "Epoch 448/1000\n",
      "1120/1120 [==============================] - 7s 6ms/step - loss: 0.7229 - accuracy: 0.7866 - val_loss: 3.1752 - val_accuracy: 0.1821\n",
      "Epoch 449/1000\n",
      "1120/1120 [==============================] - 7s 6ms/step - loss: 0.7937 - accuracy: 0.7786 - val_loss: 3.2646 - val_accuracy: 0.1857\n",
      "Epoch 450/1000\n",
      "1120/1120 [==============================] - 7s 6ms/step - loss: 0.8168 - accuracy: 0.7554 - val_loss: 3.0089 - val_accuracy: 0.2000\n",
      "Epoch 451/1000\n",
      "1120/1120 [==============================] - 7s 6ms/step - loss: 0.7923 - accuracy: 0.7679 - val_loss: 3.1321 - val_accuracy: 0.2107\n",
      "Epoch 452/1000\n",
      "1120/1120 [==============================] - 7s 6ms/step - loss: 0.7496 - accuracy: 0.7848 - val_loss: 3.0724 - val_accuracy: 0.2286\n",
      "Epoch 453/1000\n",
      "1120/1120 [==============================] - 7s 6ms/step - loss: 0.7136 - accuracy: 0.8000 - val_loss: 3.1954 - val_accuracy: 0.1893\n",
      "Epoch 454/1000\n",
      "1120/1120 [==============================] - 7s 6ms/step - loss: 0.7971 - accuracy: 0.7714 - val_loss: 3.0688 - val_accuracy: 0.2179\n",
      "Epoch 455/1000\n",
      "1120/1120 [==============================] - 7s 6ms/step - loss: 0.8242 - accuracy: 0.7464 - val_loss: 3.2565 - val_accuracy: 0.1893\n",
      "Epoch 456/1000\n",
      "1120/1120 [==============================] - 7s 6ms/step - loss: 0.7308 - accuracy: 0.7804 - val_loss: 3.3908 - val_accuracy: 0.1786\n",
      "Epoch 457/1000\n",
      "1120/1120 [==============================] - 7s 6ms/step - loss: 0.7149 - accuracy: 0.7982 - val_loss: 3.3071 - val_accuracy: 0.1714\n",
      "Epoch 458/1000\n",
      "1120/1120 [==============================] - 7s 6ms/step - loss: 0.7236 - accuracy: 0.8036 - val_loss: 3.1806 - val_accuracy: 0.2107\n",
      "Epoch 459/1000\n",
      "1120/1120 [==============================] - 7s 6ms/step - loss: 0.7486 - accuracy: 0.7866 - val_loss: 3.1630 - val_accuracy: 0.1643\n",
      "Epoch 460/1000\n",
      "1120/1120 [==============================] - 7s 6ms/step - loss: 0.7806 - accuracy: 0.7598 - val_loss: 3.8753 - val_accuracy: 0.1643\n",
      "Epoch 461/1000\n",
      "1120/1120 [==============================] - 7s 6ms/step - loss: 0.8884 - accuracy: 0.7384 - val_loss: 3.1462 - val_accuracy: 0.1643\n",
      "Epoch 462/1000\n",
      "1120/1120 [==============================] - 7s 6ms/step - loss: 0.7804 - accuracy: 0.7625 - val_loss: 3.4556 - val_accuracy: 0.1643\n",
      "Epoch 463/1000\n",
      "1120/1120 [==============================] - 7s 6ms/step - loss: 0.7440 - accuracy: 0.7937 - val_loss: 3.3265 - val_accuracy: 0.2107\n",
      "Epoch 464/1000\n",
      "1120/1120 [==============================] - 7s 6ms/step - loss: 0.6637 - accuracy: 0.8179 - val_loss: 3.0716 - val_accuracy: 0.1893\n",
      "Epoch 465/1000\n",
      "1120/1120 [==============================] - 7s 6ms/step - loss: 0.7850 - accuracy: 0.7473 - val_loss: 3.1375 - val_accuracy: 0.1786\n",
      "Epoch 466/1000\n",
      "1120/1120 [==============================] - 7s 6ms/step - loss: 0.7635 - accuracy: 0.7777 - val_loss: 3.3184 - val_accuracy: 0.1536\n",
      "Epoch 467/1000\n",
      "1120/1120 [==============================] - 7s 6ms/step - loss: 0.6734 - accuracy: 0.8143 - val_loss: 3.2665 - val_accuracy: 0.2000\n",
      "Epoch 468/1000\n",
      "1120/1120 [==============================] - 7s 6ms/step - loss: 0.7051 - accuracy: 0.8027 - val_loss: 3.2066 - val_accuracy: 0.1750\n",
      "Epoch 469/1000\n",
      "1120/1120 [==============================] - 7s 6ms/step - loss: 0.8068 - accuracy: 0.7679 - val_loss: 3.2252 - val_accuracy: 0.1929\n",
      "Epoch 470/1000\n",
      "1120/1120 [==============================] - 7s 6ms/step - loss: 0.8130 - accuracy: 0.7500 - val_loss: 3.3055 - val_accuracy: 0.1964\n",
      "Epoch 471/1000\n",
      "1120/1120 [==============================] - 7s 6ms/step - loss: 0.7109 - accuracy: 0.7991 - val_loss: 3.3695 - val_accuracy: 0.1964\n",
      "Epoch 472/1000\n",
      "1120/1120 [==============================] - 7s 6ms/step - loss: 0.7624 - accuracy: 0.7688 - val_loss: 3.1167 - val_accuracy: 0.2000\n",
      "Epoch 473/1000\n",
      "1120/1120 [==============================] - 7s 6ms/step - loss: 0.6772 - accuracy: 0.7991 - val_loss: 3.2016 - val_accuracy: 0.2000\n",
      "Epoch 474/1000\n",
      "1120/1120 [==============================] - 7s 6ms/step - loss: 0.7416 - accuracy: 0.7732 - val_loss: 3.1957 - val_accuracy: 0.2143\n",
      "Epoch 475/1000\n",
      "1120/1120 [==============================] - 7s 6ms/step - loss: 0.6551 - accuracy: 0.8259 - val_loss: 3.2801 - val_accuracy: 0.1643\n",
      "Epoch 476/1000\n",
      "1120/1120 [==============================] - 7s 6ms/step - loss: 0.7644 - accuracy: 0.7634 - val_loss: 3.3665 - val_accuracy: 0.2071\n",
      "Epoch 477/1000\n",
      "1120/1120 [==============================] - 7s 6ms/step - loss: 0.6433 - accuracy: 0.8313 - val_loss: 3.1619 - val_accuracy: 0.1786\n",
      "Epoch 478/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.6722 - accuracy: 0.8018 - val_loss: 3.1030 - val_accuracy: 0.2250\n",
      "Epoch 479/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.7362 - accuracy: 0.7723 - val_loss: 3.4169 - val_accuracy: 0.1857\n",
      "Epoch 480/1000\n",
      "1120/1120 [==============================] - 7s 7ms/step - loss: 0.6573 - accuracy: 0.8223 - val_loss: 3.3255 - val_accuracy: 0.2036\n",
      "Epoch 481/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.7696 - accuracy: 0.7812 - val_loss: 3.3820 - val_accuracy: 0.1607\n",
      "Epoch 482/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.6960 - accuracy: 0.7955 - val_loss: 3.3431 - val_accuracy: 0.1893\n",
      "Epoch 483/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.7890 - accuracy: 0.7643 - val_loss: 3.0870 - val_accuracy: 0.2071\n",
      "Epoch 484/1000\n",
      "1120/1120 [==============================] - 9s 8ms/step - loss: 0.7111 - accuracy: 0.7839 - val_loss: 3.2981 - val_accuracy: 0.2000\n",
      "Epoch 485/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.6283 - accuracy: 0.8339 - val_loss: 3.8751 - val_accuracy: 0.1607\n",
      "Epoch 486/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.7163 - accuracy: 0.7902 - val_loss: 3.1624 - val_accuracy: 0.2071\n",
      "Epoch 487/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.6100 - accuracy: 0.8313 - val_loss: 3.2644 - val_accuracy: 0.2071\n",
      "Epoch 488/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.6449 - accuracy: 0.8348 - val_loss: 3.1595 - val_accuracy: 0.2071\n",
      "Epoch 489/1000\n",
      "1120/1120 [==============================] - 7s 6ms/step - loss: 0.6506 - accuracy: 0.8125 - val_loss: 3.1795 - val_accuracy: 0.1929\n",
      "Epoch 490/1000\n",
      "1120/1120 [==============================] - 7s 7ms/step - loss: 0.6775 - accuracy: 0.8232 - val_loss: 3.2440 - val_accuracy: 0.2107\n",
      "Epoch 491/1000\n",
      "1120/1120 [==============================] - 7s 6ms/step - loss: 0.6310 - accuracy: 0.8125 - val_loss: 3.4573 - val_accuracy: 0.2179\n",
      "Epoch 492/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.7473 - accuracy: 0.7723 - val_loss: 3.3958 - val_accuracy: 0.1821\n",
      "Epoch 493/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.6744 - accuracy: 0.8054 - val_loss: 3.4670 - val_accuracy: 0.1607\n",
      "Epoch 494/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.8289 - accuracy: 0.7616 - val_loss: 3.1820 - val_accuracy: 0.2036\n",
      "Epoch 495/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.6940 - accuracy: 0.8161 - val_loss: 3.1750 - val_accuracy: 0.2107\n",
      "Epoch 496/1000\n",
      "1120/1120 [==============================] - 8s 8ms/step - loss: 0.7459 - accuracy: 0.7777 - val_loss: 3.2425 - val_accuracy: 0.2071\n",
      "Epoch 497/1000\n",
      "1120/1120 [==============================] - 9s 8ms/step - loss: 0.5970 - accuracy: 0.8455 - val_loss: 3.7973 - val_accuracy: 0.1714\n",
      "Epoch 498/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.6427 - accuracy: 0.8241 - val_loss: 3.1990 - val_accuracy: 0.1857\n",
      "Epoch 499/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.6105 - accuracy: 0.8384 - val_loss: 3.3920 - val_accuracy: 0.1964\n",
      "Epoch 500/1000\n",
      "1120/1120 [==============================] - 10s 9ms/step - loss: 0.7559 - accuracy: 0.7688 - val_loss: 3.2558 - val_accuracy: 0.2107\n",
      "Epoch 501/1000\n",
      "1120/1120 [==============================] - 10s 9ms/step - loss: 0.8975 - accuracy: 0.7554 - val_loss: 3.1576 - val_accuracy: 0.1893\n",
      "Epoch 502/1000\n",
      "1120/1120 [==============================] - 10s 9ms/step - loss: 0.7198 - accuracy: 0.8170 - val_loss: 3.2887 - val_accuracy: 0.2357\n",
      "Epoch 503/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.5167 - accuracy: 0.8866 - val_loss: 3.3246 - val_accuracy: 0.1857\n",
      "Epoch 504/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.5486 - accuracy: 0.8750 - val_loss: 3.2202 - val_accuracy: 0.2071\n",
      "Epoch 505/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.5906 - accuracy: 0.8366 - val_loss: 3.3113 - val_accuracy: 0.1964\n",
      "Epoch 506/1000\n",
      "1120/1120 [==============================] - 7s 7ms/step - loss: 0.5789 - accuracy: 0.8589 - val_loss: 3.4543 - val_accuracy: 0.1643\n",
      "Epoch 507/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.5493 - accuracy: 0.8554 - val_loss: 3.3280 - val_accuracy: 0.1857\n",
      "Epoch 508/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.5896 - accuracy: 0.8348 - val_loss: 3.1742 - val_accuracy: 0.2036\n",
      "Epoch 509/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.5614 - accuracy: 0.8589 - val_loss: 3.2755 - val_accuracy: 0.1929\n",
      "Epoch 510/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.5922 - accuracy: 0.8482 - val_loss: 3.5196 - val_accuracy: 0.1857\n",
      "Epoch 511/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.5485 - accuracy: 0.8536 - val_loss: 3.2149 - val_accuracy: 0.2143\n",
      "Epoch 512/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.6598 - accuracy: 0.8205 - val_loss: 3.2030 - val_accuracy: 0.2214\n",
      "Epoch 513/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.6873 - accuracy: 0.8018 - val_loss: 3.3773 - val_accuracy: 0.1857\n",
      "Epoch 514/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.6267 - accuracy: 0.8313 - val_loss: 3.1771 - val_accuracy: 0.1929\n",
      "Epoch 515/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.6248 - accuracy: 0.8321 - val_loss: 3.3931 - val_accuracy: 0.1857\n",
      "Epoch 516/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.4963 - accuracy: 0.8875 - val_loss: 3.3622 - val_accuracy: 0.2107\n",
      "Epoch 517/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.6182 - accuracy: 0.8286 - val_loss: 3.5076 - val_accuracy: 0.1857\n",
      "Epoch 518/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.6089 - accuracy: 0.8357 - val_loss: 3.5051 - val_accuracy: 0.2143\n",
      "Epoch 519/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.5331 - accuracy: 0.8500 - val_loss: 3.3562 - val_accuracy: 0.2000\n",
      "Epoch 520/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.6519 - accuracy: 0.8170 - val_loss: 3.3504 - val_accuracy: 0.1893\n",
      "Epoch 521/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.5363 - accuracy: 0.8714 - val_loss: 3.6046 - val_accuracy: 0.1679\n",
      "Epoch 522/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.5965 - accuracy: 0.8268 - val_loss: 3.3100 - val_accuracy: 0.2036\n",
      "Epoch 523/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.5238 - accuracy: 0.8652 - val_loss: 3.5408 - val_accuracy: 0.1857\n",
      "Epoch 524/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.5707 - accuracy: 0.8491 - val_loss: 3.3253 - val_accuracy: 0.2071\n",
      "Epoch 525/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.4997 - accuracy: 0.8750 - val_loss: 3.3255 - val_accuracy: 0.1929\n",
      "Epoch 526/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.5432 - accuracy: 0.8723 - val_loss: 3.3490 - val_accuracy: 0.1821\n",
      "Epoch 527/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.5877 - accuracy: 0.8375 - val_loss: 4.3437 - val_accuracy: 0.1250\n",
      "Epoch 528/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.4911 - accuracy: 0.8884 - val_loss: 3.6264 - val_accuracy: 0.1643\n",
      "Epoch 529/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.5291 - accuracy: 0.8562 - val_loss: 3.3108 - val_accuracy: 0.2143\n",
      "Epoch 530/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.4852 - accuracy: 0.8750 - val_loss: 3.2576 - val_accuracy: 0.1929\n",
      "Epoch 531/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.5894 - accuracy: 0.8161 - val_loss: 4.2950 - val_accuracy: 0.1571\n",
      "Epoch 532/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.5869 - accuracy: 0.8518 - val_loss: 3.6144 - val_accuracy: 0.2000\n",
      "Epoch 533/1000\n",
      "1120/1120 [==============================] - 7s 7ms/step - loss: 0.5545 - accuracy: 0.8411 - val_loss: 3.3631 - val_accuracy: 0.1893\n",
      "Epoch 534/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.5669 - accuracy: 0.8473 - val_loss: 3.2905 - val_accuracy: 0.2071\n",
      "Epoch 535/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.5153 - accuracy: 0.8571 - val_loss: 3.4238 - val_accuracy: 0.2000\n",
      "Epoch 536/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.5137 - accuracy: 0.8589 - val_loss: 3.3334 - val_accuracy: 0.1893\n",
      "Epoch 537/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.4706 - accuracy: 0.8813 - val_loss: 3.3014 - val_accuracy: 0.2179\n",
      "Epoch 538/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.5744 - accuracy: 0.8580 - val_loss: 3.5094 - val_accuracy: 0.1679\n",
      "Epoch 539/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.4607 - accuracy: 0.8830 - val_loss: 3.3074 - val_accuracy: 0.2071\n",
      "Epoch 540/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.4373 - accuracy: 0.9027 - val_loss: 3.4675 - val_accuracy: 0.1821\n",
      "Epoch 541/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.5502 - accuracy: 0.8607 - val_loss: 3.3975 - val_accuracy: 0.2143\n",
      "Epoch 542/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.4960 - accuracy: 0.8723 - val_loss: 3.3364 - val_accuracy: 0.2107\n",
      "Epoch 543/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.6089 - accuracy: 0.8482 - val_loss: 3.3608 - val_accuracy: 0.1893\n",
      "Epoch 544/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.5363 - accuracy: 0.8536 - val_loss: 3.3969 - val_accuracy: 0.1786\n",
      "Epoch 545/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.4216 - accuracy: 0.9107 - val_loss: 3.3812 - val_accuracy: 0.1857\n",
      "Epoch 546/1000\n",
      "1120/1120 [==============================] - 7s 7ms/step - loss: 0.3770 - accuracy: 0.9312 - val_loss: 3.3784 - val_accuracy: 0.2143\n",
      "Epoch 547/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.5724 - accuracy: 0.8402 - val_loss: 3.5523 - val_accuracy: 0.1714\n",
      "Epoch 548/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.5767 - accuracy: 0.8482 - val_loss: 3.3219 - val_accuracy: 0.2071\n",
      "Epoch 549/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.4126 - accuracy: 0.9143 - val_loss: 3.3443 - val_accuracy: 0.2179\n",
      "Epoch 550/1000\n",
      "1120/1120 [==============================] - 9s 8ms/step - loss: 0.4643 - accuracy: 0.8920 - val_loss: 3.3784 - val_accuracy: 0.1857\n",
      "Epoch 551/1000\n",
      "1120/1120 [==============================] - 9s 8ms/step - loss: 0.4361 - accuracy: 0.9009 - val_loss: 3.7629 - val_accuracy: 0.1643\n",
      "Epoch 552/1000\n",
      "1120/1120 [==============================] - 9s 8ms/step - loss: 0.4566 - accuracy: 0.8920 - val_loss: 3.4912 - val_accuracy: 0.2036\n",
      "Epoch 553/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.5285 - accuracy: 0.8723 - val_loss: 3.4756 - val_accuracy: 0.1857\n",
      "Epoch 554/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.5159 - accuracy: 0.8857 - val_loss: 4.0905 - val_accuracy: 0.2250\n",
      "Epoch 555/1000\n",
      "1120/1120 [==============================] - 9s 8ms/step - loss: 0.4806 - accuracy: 0.8813 - val_loss: 3.3799 - val_accuracy: 0.2000\n",
      "Epoch 556/1000\n",
      "1120/1120 [==============================] - 9s 8ms/step - loss: 0.4730 - accuracy: 0.8839 - val_loss: 3.4389 - val_accuracy: 0.2214\n",
      "Epoch 557/1000\n",
      "1120/1120 [==============================] - 9s 8ms/step - loss: 0.3699 - accuracy: 0.9286 - val_loss: 3.6261 - val_accuracy: 0.1857\n",
      "Epoch 558/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.4659 - accuracy: 0.8821 - val_loss: 3.6152 - val_accuracy: 0.2000\n",
      "Epoch 559/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.4667 - accuracy: 0.8687 - val_loss: 3.4593 - val_accuracy: 0.1821\n",
      "Epoch 560/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.4311 - accuracy: 0.8973 - val_loss: 3.4872 - val_accuracy: 0.1786\n",
      "Epoch 561/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.4246 - accuracy: 0.9036 - val_loss: 3.4221 - val_accuracy: 0.2071\n",
      "Epoch 562/1000\n",
      "1120/1120 [==============================] - 9s 8ms/step - loss: 0.4444 - accuracy: 0.8893 - val_loss: 3.3624 - val_accuracy: 0.2071\n",
      "Epoch 563/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.3904 - accuracy: 0.9152 - val_loss: 3.3985 - val_accuracy: 0.1964\n",
      "Epoch 564/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.4912 - accuracy: 0.8750 - val_loss: 3.5076 - val_accuracy: 0.1929\n",
      "Epoch 565/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.4272 - accuracy: 0.9018 - val_loss: 3.5413 - val_accuracy: 0.1786\n",
      "Epoch 566/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.4286 - accuracy: 0.8964 - val_loss: 3.4646 - val_accuracy: 0.2036\n",
      "Epoch 567/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.3397 - accuracy: 0.9357 - val_loss: 3.5034 - val_accuracy: 0.1964\n",
      "Epoch 568/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.3500 - accuracy: 0.9366 - val_loss: 4.0701 - val_accuracy: 0.1500\n",
      "Epoch 569/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.6411 - accuracy: 0.8313 - val_loss: 3.5665 - val_accuracy: 0.2071\n",
      "Epoch 570/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.3958 - accuracy: 0.9143 - val_loss: 3.4327 - val_accuracy: 0.2036\n",
      "Epoch 571/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.3905 - accuracy: 0.9241 - val_loss: 3.6331 - val_accuracy: 0.2107\n",
      "Epoch 572/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.4063 - accuracy: 0.9214 - val_loss: 3.4949 - val_accuracy: 0.2143\n",
      "Epoch 573/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.5015 - accuracy: 0.8571 - val_loss: 3.5465 - val_accuracy: 0.2071\n",
      "Epoch 574/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.4019 - accuracy: 0.9161 - val_loss: 3.6025 - val_accuracy: 0.2107\n",
      "Epoch 575/1000\n",
      "1120/1120 [==============================] - 9s 8ms/step - loss: 0.3231 - accuracy: 0.9429 - val_loss: 3.4224 - val_accuracy: 0.1893\n",
      "Epoch 576/1000\n",
      "1120/1120 [==============================] - 9s 8ms/step - loss: 0.3231 - accuracy: 0.9464 - val_loss: 3.5368 - val_accuracy: 0.2143\n",
      "Epoch 577/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.3698 - accuracy: 0.9223 - val_loss: 3.3803 - val_accuracy: 0.2214\n",
      "Epoch 578/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.3471 - accuracy: 0.9304 - val_loss: 3.6505 - val_accuracy: 0.1893\n",
      "Epoch 579/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.3358 - accuracy: 0.9348 - val_loss: 3.5921 - val_accuracy: 0.2071\n",
      "Epoch 580/1000\n",
      "1120/1120 [==============================] - 9s 8ms/step - loss: 0.3640 - accuracy: 0.9187 - val_loss: 3.4926 - val_accuracy: 0.1964\n",
      "Epoch 581/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.3365 - accuracy: 0.9366 - val_loss: 3.5166 - val_accuracy: 0.2036\n",
      "Epoch 582/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.3591 - accuracy: 0.9286 - val_loss: 3.5334 - val_accuracy: 0.2107\n",
      "Epoch 583/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.3344 - accuracy: 0.9375 - val_loss: 3.4892 - val_accuracy: 0.2143\n",
      "Epoch 584/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.3043 - accuracy: 0.9509 - val_loss: 3.5850 - val_accuracy: 0.2143\n",
      "Epoch 585/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.3504 - accuracy: 0.9295 - val_loss: 3.5383 - val_accuracy: 0.1929\n",
      "Epoch 586/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.4168 - accuracy: 0.9196 - val_loss: 3.4634 - val_accuracy: 0.2071\n",
      "Epoch 587/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.4923 - accuracy: 0.8687 - val_loss: 3.4625 - val_accuracy: 0.2250\n",
      "Epoch 588/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.3229 - accuracy: 0.9491 - val_loss: 3.5346 - val_accuracy: 0.1714\n",
      "Epoch 589/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.3463 - accuracy: 0.9321 - val_loss: 3.4350 - val_accuracy: 0.1929\n",
      "Epoch 590/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.3396 - accuracy: 0.9321 - val_loss: 3.6386 - val_accuracy: 0.1893\n",
      "Epoch 591/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.3174 - accuracy: 0.9393 - val_loss: 3.4972 - val_accuracy: 0.2000\n",
      "Epoch 592/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.2818 - accuracy: 0.9607 - val_loss: 3.5246 - val_accuracy: 0.2036\n",
      "Epoch 593/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.4007 - accuracy: 0.9027 - val_loss: 3.7230 - val_accuracy: 0.1786\n",
      "Epoch 594/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.3353 - accuracy: 0.9295 - val_loss: 3.9266 - val_accuracy: 0.1714\n",
      "Epoch 595/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.3847 - accuracy: 0.9134 - val_loss: 3.5188 - val_accuracy: 0.1929\n",
      "Epoch 596/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.3539 - accuracy: 0.9250 - val_loss: 3.5439 - val_accuracy: 0.2000\n",
      "Epoch 597/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.3725 - accuracy: 0.9152 - val_loss: 3.7794 - val_accuracy: 0.1643\n",
      "Epoch 598/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.3007 - accuracy: 0.9509 - val_loss: 3.5464 - val_accuracy: 0.2036\n",
      "Epoch 599/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.3009 - accuracy: 0.9455 - val_loss: 3.8680 - val_accuracy: 0.2071\n",
      "Epoch 600/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.3811 - accuracy: 0.9125 - val_loss: 3.5156 - val_accuracy: 0.2000\n",
      "Epoch 601/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.3356 - accuracy: 0.9348 - val_loss: 3.5783 - val_accuracy: 0.2143\n",
      "Epoch 602/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.2593 - accuracy: 0.9652 - val_loss: 3.7544 - val_accuracy: 0.1786\n",
      "Epoch 603/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.3557 - accuracy: 0.9277 - val_loss: 3.5500 - val_accuracy: 0.2000\n",
      "Epoch 604/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.3649 - accuracy: 0.9179 - val_loss: 3.7381 - val_accuracy: 0.1786\n",
      "Epoch 605/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.3140 - accuracy: 0.9366 - val_loss: 3.5087 - val_accuracy: 0.2179\n",
      "Epoch 606/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.2806 - accuracy: 0.9554 - val_loss: 3.5774 - val_accuracy: 0.1893\n",
      "Epoch 607/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.5961 - accuracy: 0.8616 - val_loss: 3.5856 - val_accuracy: 0.1893\n",
      "Epoch 608/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.2469 - accuracy: 0.9777 - val_loss: 3.5161 - val_accuracy: 0.2000\n",
      "Epoch 609/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.2532 - accuracy: 0.9643 - val_loss: 3.5303 - val_accuracy: 0.1929\n",
      "Epoch 610/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.3354 - accuracy: 0.9339 - val_loss: 3.4748 - val_accuracy: 0.2179\n",
      "Epoch 611/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.2890 - accuracy: 0.9446 - val_loss: 3.8937 - val_accuracy: 0.1964\n",
      "Epoch 612/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.2535 - accuracy: 0.9625 - val_loss: 3.6718 - val_accuracy: 0.2071\n",
      "Epoch 613/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.4934 - accuracy: 0.8991 - val_loss: 3.4990 - val_accuracy: 0.1857\n",
      "Epoch 614/1000\n",
      "1120/1120 [==============================] - 7s 7ms/step - loss: 0.2625 - accuracy: 0.9625 - val_loss: 3.6752 - val_accuracy: 0.1929\n",
      "Epoch 615/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.2743 - accuracy: 0.9554 - val_loss: 3.5453 - val_accuracy: 0.1857\n",
      "Epoch 616/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.2361 - accuracy: 0.9732 - val_loss: 3.6521 - val_accuracy: 0.2036\n",
      "Epoch 617/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.2420 - accuracy: 0.9652 - val_loss: 3.6093 - val_accuracy: 0.1821\n",
      "Epoch 618/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.2785 - accuracy: 0.9580 - val_loss: 3.7713 - val_accuracy: 0.1857\n",
      "Epoch 619/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.2450 - accuracy: 0.9679 - val_loss: 3.7188 - val_accuracy: 0.1786\n",
      "Epoch 620/1000\n",
      "1120/1120 [==============================] - 7s 7ms/step - loss: 0.3048 - accuracy: 0.9500 - val_loss: 3.8558 - val_accuracy: 0.1786\n",
      "Epoch 621/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.2195 - accuracy: 0.9768 - val_loss: 3.6858 - val_accuracy: 0.1964\n",
      "Epoch 622/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.2727 - accuracy: 0.9536 - val_loss: 4.0944 - val_accuracy: 0.1679\n",
      "Epoch 623/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.2405 - accuracy: 0.9634 - val_loss: 3.5802 - val_accuracy: 0.2036\n",
      "Epoch 624/1000\n",
      "1120/1120 [==============================] - 7s 7ms/step - loss: 0.2160 - accuracy: 0.9750 - val_loss: 3.6744 - val_accuracy: 0.2000\n",
      "Epoch 625/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.2411 - accuracy: 0.9643 - val_loss: 4.0353 - val_accuracy: 0.1786\n",
      "Epoch 626/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.2925 - accuracy: 0.9455 - val_loss: 3.6622 - val_accuracy: 0.1893\n",
      "Epoch 627/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.2052 - accuracy: 0.9786 - val_loss: 3.7280 - val_accuracy: 0.1893\n",
      "Epoch 628/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.1928 - accuracy: 0.9857 - val_loss: 3.6638 - val_accuracy: 0.2000\n",
      "Epoch 629/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.2153 - accuracy: 0.9768 - val_loss: 4.0106 - val_accuracy: 0.1821\n",
      "Epoch 630/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.2125 - accuracy: 0.9777 - val_loss: 3.6768 - val_accuracy: 0.1929\n",
      "Epoch 631/1000\n",
      "1120/1120 [==============================] - 7s 7ms/step - loss: 0.2506 - accuracy: 0.9598 - val_loss: 3.6679 - val_accuracy: 0.2036\n",
      "Epoch 632/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.2491 - accuracy: 0.9625 - val_loss: 3.6831 - val_accuracy: 0.1893\n",
      "Epoch 633/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.2037 - accuracy: 0.9786 - val_loss: 3.7801 - val_accuracy: 0.2036\n",
      "Epoch 634/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.1920 - accuracy: 0.9821 - val_loss: 3.6442 - val_accuracy: 0.2036\n",
      "Epoch 635/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.2745 - accuracy: 0.9411 - val_loss: 3.6018 - val_accuracy: 0.2071\n",
      "Epoch 636/1000\n",
      "1120/1120 [==============================] - 7s 7ms/step - loss: 0.2164 - accuracy: 0.9768 - val_loss: 3.7101 - val_accuracy: 0.2036\n",
      "Epoch 637/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.1941 - accuracy: 0.9875 - val_loss: 3.7233 - val_accuracy: 0.2000\n",
      "Epoch 638/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.2078 - accuracy: 0.9786 - val_loss: 3.7269 - val_accuracy: 0.1964\n",
      "Epoch 639/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.3341 - accuracy: 0.9232 - val_loss: 3.7738 - val_accuracy: 0.1964\n",
      "Epoch 640/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.2385 - accuracy: 0.9634 - val_loss: 3.6484 - val_accuracy: 0.2036\n",
      "Epoch 641/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.1927 - accuracy: 0.9857 - val_loss: 3.9499 - val_accuracy: 0.1821\n",
      "Epoch 642/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.1932 - accuracy: 0.9875 - val_loss: 3.7564 - val_accuracy: 0.2000\n",
      "Epoch 643/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.1838 - accuracy: 0.9839 - val_loss: 3.6459 - val_accuracy: 0.2071\n",
      "Epoch 644/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.2058 - accuracy: 0.9732 - val_loss: 3.6332 - val_accuracy: 0.2179\n",
      "Epoch 645/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.1736 - accuracy: 0.9911 - val_loss: 3.6879 - val_accuracy: 0.2071\n",
      "Epoch 646/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.1788 - accuracy: 0.9857 - val_loss: 3.7753 - val_accuracy: 0.2000\n",
      "Epoch 647/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.1867 - accuracy: 0.9830 - val_loss: 3.6867 - val_accuracy: 0.1893\n",
      "Epoch 648/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.1979 - accuracy: 0.9750 - val_loss: 4.4454 - val_accuracy: 0.1607\n",
      "Epoch 649/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.2269 - accuracy: 0.9616 - val_loss: 3.8707 - val_accuracy: 0.1893\n",
      "Epoch 650/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.1874 - accuracy: 0.9866 - val_loss: 3.7335 - val_accuracy: 0.2071\n",
      "Epoch 651/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.2113 - accuracy: 0.9768 - val_loss: 3.6802 - val_accuracy: 0.1964\n",
      "Epoch 652/1000\n",
      "1120/1120 [==============================] - 7s 7ms/step - loss: 0.1806 - accuracy: 0.9777 - val_loss: 3.7605 - val_accuracy: 0.2179\n",
      "Epoch 653/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.1532 - accuracy: 0.9911 - val_loss: 4.4405 - val_accuracy: 0.1393\n",
      "Epoch 654/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.2041 - accuracy: 0.9688 - val_loss: 4.0700 - val_accuracy: 0.1679\n",
      "Epoch 655/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.1808 - accuracy: 0.9830 - val_loss: 3.8343 - val_accuracy: 0.2000\n",
      "Epoch 656/1000\n",
      "1120/1120 [==============================] - 7s 7ms/step - loss: 0.1564 - accuracy: 0.9937 - val_loss: 3.8285 - val_accuracy: 0.2000\n",
      "Epoch 657/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.1777 - accuracy: 0.9795 - val_loss: 3.7207 - val_accuracy: 0.1964\n",
      "Epoch 658/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.1975 - accuracy: 0.9786 - val_loss: 4.1792 - val_accuracy: 0.1786\n",
      "Epoch 659/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.1745 - accuracy: 0.9830 - val_loss: 3.8365 - val_accuracy: 0.2179\n",
      "Epoch 660/1000\n",
      "1120/1120 [==============================] - 7s 7ms/step - loss: 0.1583 - accuracy: 0.9946 - val_loss: 3.7416 - val_accuracy: 0.2071\n",
      "Epoch 661/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.1679 - accuracy: 0.9839 - val_loss: 3.7949 - val_accuracy: 0.1964\n",
      "Epoch 662/1000\n",
      "1120/1120 [==============================] - 7s 7ms/step - loss: 0.1568 - accuracy: 0.9911 - val_loss: 3.8305 - val_accuracy: 0.1964\n",
      "Epoch 663/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.1452 - accuracy: 0.9937 - val_loss: 3.7927 - val_accuracy: 0.1893\n",
      "Epoch 664/1000\n",
      "1120/1120 [==============================] - 7s 7ms/step - loss: 0.1472 - accuracy: 0.9946 - val_loss: 3.8168 - val_accuracy: 0.2071\n",
      "Epoch 665/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.3066 - accuracy: 0.9420 - val_loss: 3.7858 - val_accuracy: 0.2107\n",
      "Epoch 666/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.1609 - accuracy: 0.9884 - val_loss: 3.8051 - val_accuracy: 0.2036\n",
      "Epoch 667/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.1478 - accuracy: 0.9893 - val_loss: 3.8246 - val_accuracy: 0.2071\n",
      "Epoch 668/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.1512 - accuracy: 0.9946 - val_loss: 3.9262 - val_accuracy: 0.1786\n",
      "Epoch 669/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.1891 - accuracy: 0.9714 - val_loss: 4.3088 - val_accuracy: 0.1750\n",
      "Epoch 670/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.2549 - accuracy: 0.9527 - val_loss: 3.7807 - val_accuracy: 0.2000\n",
      "Epoch 671/1000\n",
      "1120/1120 [==============================] - 7s 7ms/step - loss: 0.1375 - accuracy: 0.9955 - val_loss: 3.8447 - val_accuracy: 0.2321\n",
      "Epoch 672/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.1348 - accuracy: 0.9964 - val_loss: 3.9743 - val_accuracy: 0.2036\n",
      "Epoch 673/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.1404 - accuracy: 0.9964 - val_loss: 3.8070 - val_accuracy: 0.2000\n",
      "Epoch 674/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.1420 - accuracy: 0.9955 - val_loss: 3.8313 - val_accuracy: 0.2036\n",
      "Epoch 675/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.1308 - accuracy: 0.9973 - val_loss: 3.8506 - val_accuracy: 0.2036\n",
      "Epoch 676/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.1427 - accuracy: 0.9911 - val_loss: 3.8840 - val_accuracy: 0.1929\n",
      "Epoch 677/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.1482 - accuracy: 0.9866 - val_loss: 3.9105 - val_accuracy: 0.2036\n",
      "Epoch 678/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.1487 - accuracy: 0.9875 - val_loss: 3.9956 - val_accuracy: 0.1821\n",
      "Epoch 679/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.1394 - accuracy: 0.9955 - val_loss: 3.8228 - val_accuracy: 0.1964\n",
      "Epoch 680/1000\n",
      "1120/1120 [==============================] - 7s 7ms/step - loss: 0.1702 - accuracy: 0.9848 - val_loss: 3.8213 - val_accuracy: 0.2036\n",
      "Epoch 681/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.1230 - accuracy: 0.9991 - val_loss: 3.8041 - val_accuracy: 0.2107\n",
      "Epoch 682/1000\n",
      "1120/1120 [==============================] - 7s 7ms/step - loss: 0.1576 - accuracy: 0.9812 - val_loss: 4.2820 - val_accuracy: 0.2179\n",
      "Epoch 683/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.1591 - accuracy: 0.9857 - val_loss: 3.9430 - val_accuracy: 0.2036\n",
      "Epoch 684/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.1279 - accuracy: 0.9964 - val_loss: 3.9249 - val_accuracy: 0.2000\n",
      "Epoch 685/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.1217 - accuracy: 0.9982 - val_loss: 3.8793 - val_accuracy: 0.1929\n",
      "Epoch 686/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.1316 - accuracy: 0.9946 - val_loss: 3.8652 - val_accuracy: 0.2000\n",
      "Epoch 687/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.1377 - accuracy: 0.9937 - val_loss: 4.0175 - val_accuracy: 0.1964\n",
      "Epoch 688/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.1367 - accuracy: 0.9920 - val_loss: 3.8413 - val_accuracy: 0.2071\n",
      "Epoch 689/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.1413 - accuracy: 0.9893 - val_loss: 4.0652 - val_accuracy: 0.2000\n",
      "Epoch 690/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.1400 - accuracy: 0.9911 - val_loss: 3.8776 - val_accuracy: 0.1893\n",
      "Epoch 691/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.1231 - accuracy: 0.9982 - val_loss: 3.8603 - val_accuracy: 0.2143\n",
      "Epoch 692/1000\n",
      "1120/1120 [==============================] - 7s 7ms/step - loss: 0.1185 - accuracy: 0.9982 - val_loss: 3.9179 - val_accuracy: 0.2071\n",
      "Epoch 693/1000\n",
      "1120/1120 [==============================] - 7s 7ms/step - loss: 0.1281 - accuracy: 0.9937 - val_loss: 3.9466 - val_accuracy: 0.1964\n",
      "Epoch 694/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.1132 - accuracy: 0.9964 - val_loss: 3.8541 - val_accuracy: 0.2107\n",
      "Epoch 695/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.1419 - accuracy: 0.9893 - val_loss: 3.8686 - val_accuracy: 0.1964\n",
      "Epoch 696/1000\n",
      "1120/1120 [==============================] - 7s 7ms/step - loss: 0.1468 - accuracy: 0.9839 - val_loss: 3.8803 - val_accuracy: 0.1929\n",
      "Epoch 697/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.1127 - accuracy: 1.0000 - val_loss: 3.8330 - val_accuracy: 0.1964\n",
      "Epoch 698/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.1169 - accuracy: 1.0000 - val_loss: 3.8499 - val_accuracy: 0.2036\n",
      "Epoch 699/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.1221 - accuracy: 0.9982 - val_loss: 3.8701 - val_accuracy: 0.2071\n",
      "Epoch 700/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.1096 - accuracy: 0.9982 - val_loss: 3.8413 - val_accuracy: 0.2000\n",
      "Epoch 701/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.1103 - accuracy: 0.9991 - val_loss: 3.8522 - val_accuracy: 0.2071\n",
      "Epoch 702/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.1068 - accuracy: 0.9982 - val_loss: 3.9740 - val_accuracy: 0.2000\n",
      "Epoch 703/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.1068 - accuracy: 0.9991 - val_loss: 3.9075 - val_accuracy: 0.2179\n",
      "Epoch 704/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.1012 - accuracy: 1.0000 - val_loss: 3.9053 - val_accuracy: 0.2107\n",
      "Epoch 705/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.1010 - accuracy: 1.0000 - val_loss: 3.9525 - val_accuracy: 0.2143\n",
      "Epoch 706/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.1048 - accuracy: 0.9982 - val_loss: 3.8803 - val_accuracy: 0.2036\n",
      "Epoch 707/1000\n",
      "1120/1120 [==============================] - 7s 7ms/step - loss: 0.1148 - accuracy: 0.9937 - val_loss: 3.9276 - val_accuracy: 0.2036\n",
      "Epoch 708/1000\n",
      "1120/1120 [==============================] - 7s 7ms/step - loss: 0.1114 - accuracy: 0.9964 - val_loss: 3.9138 - val_accuracy: 0.2107\n",
      "Epoch 709/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.1121 - accuracy: 0.9973 - val_loss: 3.9185 - val_accuracy: 0.2250\n",
      "Epoch 710/1000\n",
      "1120/1120 [==============================] - 7s 7ms/step - loss: 0.1141 - accuracy: 0.9973 - val_loss: 3.9389 - val_accuracy: 0.2179\n",
      "Epoch 711/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.1011 - accuracy: 1.0000 - val_loss: 3.8920 - val_accuracy: 0.2179\n",
      "Epoch 712/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.1017 - accuracy: 0.9991 - val_loss: 3.8623 - val_accuracy: 0.2071\n",
      "Epoch 713/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.1056 - accuracy: 0.9955 - val_loss: 3.9200 - val_accuracy: 0.2071\n",
      "Epoch 714/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.1091 - accuracy: 0.9973 - val_loss: 4.0473 - val_accuracy: 0.2071\n",
      "Epoch 715/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.1113 - accuracy: 0.9964 - val_loss: 3.9277 - val_accuracy: 0.2107\n",
      "Epoch 716/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0957 - accuracy: 0.9991 - val_loss: 3.9419 - val_accuracy: 0.2071\n",
      "Epoch 717/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.1014 - accuracy: 0.9991 - val_loss: 3.9181 - val_accuracy: 0.2036\n",
      "Epoch 718/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0940 - accuracy: 0.9991 - val_loss: 3.9749 - val_accuracy: 0.1929\n",
      "Epoch 719/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.1165 - accuracy: 0.9929 - val_loss: 3.9217 - val_accuracy: 0.2214\n",
      "Epoch 720/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.1049 - accuracy: 0.9955 - val_loss: 3.9137 - val_accuracy: 0.2071\n",
      "Epoch 721/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0942 - accuracy: 0.9991 - val_loss: 3.9240 - val_accuracy: 0.2214\n",
      "Epoch 722/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0990 - accuracy: 0.9991 - val_loss: 3.9395 - val_accuracy: 0.1964\n",
      "Epoch 723/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0925 - accuracy: 0.9991 - val_loss: 3.9725 - val_accuracy: 0.2000\n",
      "Epoch 724/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0949 - accuracy: 0.9991 - val_loss: 3.9805 - val_accuracy: 0.2071\n",
      "Epoch 725/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0955 - accuracy: 0.9991 - val_loss: 4.0712 - val_accuracy: 0.2071\n",
      "Epoch 726/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0913 - accuracy: 1.0000 - val_loss: 4.0031 - val_accuracy: 0.2036\n",
      "Epoch 727/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0895 - accuracy: 1.0000 - val_loss: 3.9603 - val_accuracy: 0.2179\n",
      "Epoch 728/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0893 - accuracy: 0.9991 - val_loss: 3.9238 - val_accuracy: 0.2071\n",
      "Epoch 729/1000\n",
      "1120/1120 [==============================] - 9s 8ms/step - loss: 0.0892 - accuracy: 1.0000 - val_loss: 4.0379 - val_accuracy: 0.2071\n",
      "Epoch 730/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0945 - accuracy: 0.9991 - val_loss: 4.1415 - val_accuracy: 0.2071\n",
      "Epoch 731/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0908 - accuracy: 0.9982 - val_loss: 4.0038 - val_accuracy: 0.2179\n",
      "Epoch 732/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.1026 - accuracy: 0.9946 - val_loss: 3.9914 - val_accuracy: 0.2000\n",
      "Epoch 733/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0964 - accuracy: 0.9955 - val_loss: 4.0413 - val_accuracy: 0.2143\n",
      "Epoch 734/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0866 - accuracy: 1.0000 - val_loss: 3.9739 - val_accuracy: 0.2036\n",
      "Epoch 735/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0812 - accuracy: 1.0000 - val_loss: 4.0232 - val_accuracy: 0.1929\n",
      "Epoch 736/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0863 - accuracy: 0.9991 - val_loss: 4.0260 - val_accuracy: 0.2107\n",
      "Epoch 737/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0840 - accuracy: 1.0000 - val_loss: 3.9650 - val_accuracy: 0.2036\n",
      "Epoch 738/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0835 - accuracy: 1.0000 - val_loss: 4.0955 - val_accuracy: 0.2143\n",
      "Epoch 739/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0814 - accuracy: 1.0000 - val_loss: 4.1373 - val_accuracy: 0.1929\n",
      "Epoch 740/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0830 - accuracy: 1.0000 - val_loss: 3.9874 - val_accuracy: 0.2036\n",
      "Epoch 741/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0849 - accuracy: 0.9991 - val_loss: 4.0152 - val_accuracy: 0.2107\n",
      "Epoch 742/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0765 - accuracy: 1.0000 - val_loss: 3.9731 - val_accuracy: 0.2143\n",
      "Epoch 743/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0789 - accuracy: 1.0000 - val_loss: 4.0696 - val_accuracy: 0.2107\n",
      "Epoch 744/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0754 - accuracy: 1.0000 - val_loss: 4.0108 - val_accuracy: 0.2107\n",
      "Epoch 745/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0807 - accuracy: 1.0000 - val_loss: 4.0131 - val_accuracy: 0.2179\n",
      "Epoch 746/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0796 - accuracy: 1.0000 - val_loss: 4.0228 - val_accuracy: 0.2107\n",
      "Epoch 747/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0797 - accuracy: 1.0000 - val_loss: 4.0638 - val_accuracy: 0.2143\n",
      "Epoch 748/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0804 - accuracy: 1.0000 - val_loss: 4.0076 - val_accuracy: 0.2214\n",
      "Epoch 749/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0753 - accuracy: 1.0000 - val_loss: 4.0404 - val_accuracy: 0.1964\n",
      "Epoch 750/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0752 - accuracy: 1.0000 - val_loss: 4.0369 - val_accuracy: 0.2179\n",
      "Epoch 751/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0757 - accuracy: 1.0000 - val_loss: 4.0606 - val_accuracy: 0.2071\n",
      "Epoch 752/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0754 - accuracy: 1.0000 - val_loss: 4.1107 - val_accuracy: 0.2000\n",
      "Epoch 753/1000\n",
      "1120/1120 [==============================] - 7s 7ms/step - loss: 0.0754 - accuracy: 1.0000 - val_loss: 4.0366 - val_accuracy: 0.2107\n",
      "Epoch 754/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0773 - accuracy: 1.0000 - val_loss: 4.0112 - val_accuracy: 0.2036\n",
      "Epoch 755/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0733 - accuracy: 1.0000 - val_loss: 4.0152 - val_accuracy: 0.2000\n",
      "Epoch 756/1000\n",
      "1120/1120 [==============================] - 7s 7ms/step - loss: 0.0695 - accuracy: 1.0000 - val_loss: 4.0156 - val_accuracy: 0.2107\n",
      "Epoch 757/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0724 - accuracy: 1.0000 - val_loss: 4.0451 - val_accuracy: 0.2107\n",
      "Epoch 758/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0742 - accuracy: 1.0000 - val_loss: 4.0885 - val_accuracy: 0.2071\n",
      "Epoch 759/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0715 - accuracy: 1.0000 - val_loss: 4.1112 - val_accuracy: 0.2107\n",
      "Epoch 760/1000\n",
      "1120/1120 [==============================] - 7s 7ms/step - loss: 0.0725 - accuracy: 1.0000 - val_loss: 4.0466 - val_accuracy: 0.2036\n",
      "Epoch 761/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0715 - accuracy: 1.0000 - val_loss: 4.0673 - val_accuracy: 0.2143\n",
      "Epoch 762/1000\n",
      "1120/1120 [==============================] - 7s 7ms/step - loss: 0.0733 - accuracy: 1.0000 - val_loss: 4.0272 - val_accuracy: 0.2143\n",
      "Epoch 763/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0717 - accuracy: 1.0000 - val_loss: 4.0727 - val_accuracy: 0.2036\n",
      "Epoch 764/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0714 - accuracy: 1.0000 - val_loss: 4.0320 - val_accuracy: 0.2036\n",
      "Epoch 765/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0700 - accuracy: 1.0000 - val_loss: 4.0538 - val_accuracy: 0.2036\n",
      "Epoch 766/1000\n",
      "1120/1120 [==============================] - 7s 7ms/step - loss: 0.0737 - accuracy: 1.0000 - val_loss: 4.0755 - val_accuracy: 0.2000\n",
      "Epoch 767/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0673 - accuracy: 1.0000 - val_loss: 4.1034 - val_accuracy: 0.1964\n",
      "Epoch 768/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0678 - accuracy: 1.0000 - val_loss: 4.1248 - val_accuracy: 0.2036\n",
      "Epoch 769/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0676 - accuracy: 1.0000 - val_loss: 4.1122 - val_accuracy: 0.2214\n",
      "Epoch 770/1000\n",
      "1120/1120 [==============================] - 7s 7ms/step - loss: 0.0688 - accuracy: 1.0000 - val_loss: 4.0846 - val_accuracy: 0.2107\n",
      "Epoch 771/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0689 - accuracy: 0.9991 - val_loss: 4.1522 - val_accuracy: 0.2036\n",
      "Epoch 772/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0685 - accuracy: 1.0000 - val_loss: 4.1021 - val_accuracy: 0.2107\n",
      "Epoch 773/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0671 - accuracy: 0.9991 - val_loss: 4.4229 - val_accuracy: 0.1714\n",
      "Epoch 774/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0813 - accuracy: 0.9955 - val_loss: 4.1016 - val_accuracy: 0.2107\n",
      "Epoch 775/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0627 - accuracy: 1.0000 - val_loss: 4.1040 - val_accuracy: 0.2000\n",
      "Epoch 776/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0625 - accuracy: 1.0000 - val_loss: 4.1514 - val_accuracy: 0.2071\n",
      "Epoch 777/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0652 - accuracy: 0.9991 - val_loss: 4.1076 - val_accuracy: 0.2107\n",
      "Epoch 778/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0639 - accuracy: 1.0000 - val_loss: 4.1530 - val_accuracy: 0.2250\n",
      "Epoch 779/1000\n",
      "1120/1120 [==============================] - 9s 8ms/step - loss: 0.0638 - accuracy: 1.0000 - val_loss: 4.1578 - val_accuracy: 0.2250\n",
      "Epoch 780/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0624 - accuracy: 1.0000 - val_loss: 4.1105 - val_accuracy: 0.2000\n",
      "Epoch 781/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0629 - accuracy: 1.0000 - val_loss: 4.1071 - val_accuracy: 0.2036\n",
      "Epoch 782/1000\n",
      "1120/1120 [==============================] - 8s 8ms/step - loss: 0.0647 - accuracy: 0.9982 - val_loss: 4.1306 - val_accuracy: 0.2143\n",
      "Epoch 783/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0627 - accuracy: 1.0000 - val_loss: 4.1887 - val_accuracy: 0.2071\n",
      "Epoch 784/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0606 - accuracy: 1.0000 - val_loss: 4.1692 - val_accuracy: 0.2071\n",
      "Epoch 785/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0604 - accuracy: 1.0000 - val_loss: 4.1760 - val_accuracy: 0.2107\n",
      "Epoch 786/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0604 - accuracy: 1.0000 - val_loss: 4.1279 - val_accuracy: 0.2107\n",
      "Epoch 787/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0612 - accuracy: 1.0000 - val_loss: 4.1185 - val_accuracy: 0.2036\n",
      "Epoch 788/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0614 - accuracy: 1.0000 - val_loss: 4.1252 - val_accuracy: 0.2250\n",
      "Epoch 789/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0613 - accuracy: 1.0000 - val_loss: 4.1693 - val_accuracy: 0.2143\n",
      "Epoch 790/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0624 - accuracy: 1.0000 - val_loss: 4.1262 - val_accuracy: 0.2036\n",
      "Epoch 791/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0586 - accuracy: 1.0000 - val_loss: 4.1326 - val_accuracy: 0.2107\n",
      "Epoch 792/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0584 - accuracy: 1.0000 - val_loss: 4.1490 - val_accuracy: 0.2071\n",
      "Epoch 793/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0591 - accuracy: 1.0000 - val_loss: 4.1393 - val_accuracy: 0.2179\n",
      "Epoch 794/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0611 - accuracy: 1.0000 - val_loss: 4.1798 - val_accuracy: 0.2107\n",
      "Epoch 795/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0579 - accuracy: 1.0000 - val_loss: 4.1612 - val_accuracy: 0.2071\n",
      "Epoch 796/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0592 - accuracy: 1.0000 - val_loss: 4.1281 - val_accuracy: 0.2000\n",
      "Epoch 797/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0570 - accuracy: 1.0000 - val_loss: 4.1684 - val_accuracy: 0.2107\n",
      "Epoch 798/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0575 - accuracy: 1.0000 - val_loss: 4.1662 - val_accuracy: 0.2143\n",
      "Epoch 799/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0553 - accuracy: 1.0000 - val_loss: 4.1362 - val_accuracy: 0.2000\n",
      "Epoch 800/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0571 - accuracy: 1.0000 - val_loss: 4.2548 - val_accuracy: 0.2000\n",
      "Epoch 801/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0575 - accuracy: 1.0000 - val_loss: 4.1932 - val_accuracy: 0.2000\n",
      "Epoch 802/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0588 - accuracy: 1.0000 - val_loss: 4.1513 - val_accuracy: 0.2107\n",
      "Epoch 803/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0559 - accuracy: 1.0000 - val_loss: 4.1610 - val_accuracy: 0.2214\n",
      "Epoch 804/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0550 - accuracy: 1.0000 - val_loss: 4.1555 - val_accuracy: 0.2107\n",
      "Epoch 805/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0557 - accuracy: 1.0000 - val_loss: 4.2103 - val_accuracy: 0.2107\n",
      "Epoch 806/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0548 - accuracy: 1.0000 - val_loss: 4.1954 - val_accuracy: 0.2036\n",
      "Epoch 807/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0557 - accuracy: 1.0000 - val_loss: 4.2078 - val_accuracy: 0.2107\n",
      "Epoch 808/1000\n",
      "1120/1120 [==============================] - 7s 7ms/step - loss: 0.0549 - accuracy: 1.0000 - val_loss: 4.1788 - val_accuracy: 0.2107\n",
      "Epoch 809/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0565 - accuracy: 1.0000 - val_loss: 4.1992 - val_accuracy: 0.2000\n",
      "Epoch 810/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0534 - accuracy: 1.0000 - val_loss: 4.1903 - val_accuracy: 0.2250\n",
      "Epoch 811/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0533 - accuracy: 1.0000 - val_loss: 4.1853 - val_accuracy: 0.2143\n",
      "Epoch 812/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0552 - accuracy: 1.0000 - val_loss: 4.1668 - val_accuracy: 0.2179\n",
      "Epoch 813/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0519 - accuracy: 1.0000 - val_loss: 4.2056 - val_accuracy: 0.2036\n",
      "Epoch 814/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0517 - accuracy: 1.0000 - val_loss: 4.1948 - val_accuracy: 0.2179\n",
      "Epoch 815/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0536 - accuracy: 1.0000 - val_loss: 4.2216 - val_accuracy: 0.2036\n",
      "Epoch 816/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0530 - accuracy: 1.0000 - val_loss: 4.1807 - val_accuracy: 0.2357\n",
      "Epoch 817/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0510 - accuracy: 1.0000 - val_loss: 4.2034 - val_accuracy: 0.2143\n",
      "Epoch 818/1000\n",
      "1120/1120 [==============================] - 7s 7ms/step - loss: 0.0508 - accuracy: 1.0000 - val_loss: 4.1998 - val_accuracy: 0.2214\n",
      "Epoch 819/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0517 - accuracy: 1.0000 - val_loss: 4.1918 - val_accuracy: 0.2071\n",
      "Epoch 820/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0512 - accuracy: 1.0000 - val_loss: 4.2299 - val_accuracy: 0.2107\n",
      "Epoch 821/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0516 - accuracy: 1.0000 - val_loss: 4.2142 - val_accuracy: 0.2071\n",
      "Epoch 822/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0505 - accuracy: 1.0000 - val_loss: 4.1908 - val_accuracy: 0.1964\n",
      "Epoch 823/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0515 - accuracy: 1.0000 - val_loss: 4.2170 - val_accuracy: 0.2250\n",
      "Epoch 824/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0511 - accuracy: 1.0000 - val_loss: 4.2034 - val_accuracy: 0.2107\n",
      "Epoch 825/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0501 - accuracy: 1.0000 - val_loss: 4.1986 - val_accuracy: 0.2000\n",
      "Epoch 826/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0527 - accuracy: 1.0000 - val_loss: 4.2236 - val_accuracy: 0.2036\n",
      "Epoch 827/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0486 - accuracy: 1.0000 - val_loss: 4.2589 - val_accuracy: 0.2000\n",
      "Epoch 828/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0506 - accuracy: 1.0000 - val_loss: 4.2216 - val_accuracy: 0.2036\n",
      "Epoch 829/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0492 - accuracy: 1.0000 - val_loss: 4.2307 - val_accuracy: 0.2179\n",
      "Epoch 830/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0485 - accuracy: 1.0000 - val_loss: 4.2745 - val_accuracy: 0.2143\n",
      "Epoch 831/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0494 - accuracy: 1.0000 - val_loss: 4.3098 - val_accuracy: 0.2000\n",
      "Epoch 832/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0488 - accuracy: 1.0000 - val_loss: 4.2641 - val_accuracy: 0.2214\n",
      "Epoch 833/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0481 - accuracy: 1.0000 - val_loss: 4.2367 - val_accuracy: 0.2143\n",
      "Epoch 834/1000\n",
      "1120/1120 [==============================] - 7s 7ms/step - loss: 0.0479 - accuracy: 1.0000 - val_loss: 4.2434 - val_accuracy: 0.2071\n",
      "Epoch 835/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0470 - accuracy: 1.0000 - val_loss: 4.2537 - val_accuracy: 0.2179\n",
      "Epoch 836/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0473 - accuracy: 1.0000 - val_loss: 4.2492 - val_accuracy: 0.2107\n",
      "Epoch 837/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0477 - accuracy: 1.0000 - val_loss: 4.2264 - val_accuracy: 0.2107\n",
      "Epoch 838/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0475 - accuracy: 1.0000 - val_loss: 4.2443 - val_accuracy: 0.2179\n",
      "Epoch 839/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0466 - accuracy: 1.0000 - val_loss: 4.2420 - val_accuracy: 0.2036\n",
      "Epoch 840/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0475 - accuracy: 1.0000 - val_loss: 4.2460 - val_accuracy: 0.2107\n",
      "Epoch 841/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0465 - accuracy: 1.0000 - val_loss: 4.2778 - val_accuracy: 0.2107\n",
      "Epoch 842/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0470 - accuracy: 1.0000 - val_loss: 4.2953 - val_accuracy: 0.2214\n",
      "Epoch 843/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0458 - accuracy: 1.0000 - val_loss: 4.2728 - val_accuracy: 0.2143\n",
      "Epoch 844/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0454 - accuracy: 1.0000 - val_loss: 4.2540 - val_accuracy: 0.2107\n",
      "Epoch 845/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0451 - accuracy: 1.0000 - val_loss: 4.2530 - val_accuracy: 0.1964\n",
      "Epoch 846/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0459 - accuracy: 1.0000 - val_loss: 4.2510 - val_accuracy: 0.2357\n",
      "Epoch 847/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0452 - accuracy: 1.0000 - val_loss: 4.2656 - val_accuracy: 0.2000\n",
      "Epoch 848/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0447 - accuracy: 1.0000 - val_loss: 4.2610 - val_accuracy: 0.2286\n",
      "Epoch 849/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0445 - accuracy: 1.0000 - val_loss: 4.2860 - val_accuracy: 0.2179\n",
      "Epoch 850/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0441 - accuracy: 1.0000 - val_loss: 4.3081 - val_accuracy: 0.2036\n",
      "Epoch 851/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0443 - accuracy: 1.0000 - val_loss: 4.2680 - val_accuracy: 0.2143\n",
      "Epoch 852/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0429 - accuracy: 1.0000 - val_loss: 4.3127 - val_accuracy: 0.1929\n",
      "Epoch 853/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0442 - accuracy: 1.0000 - val_loss: 4.3250 - val_accuracy: 0.2107\n",
      "Epoch 854/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0445 - accuracy: 1.0000 - val_loss: 4.3262 - val_accuracy: 0.2214\n",
      "Epoch 855/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0437 - accuracy: 1.0000 - val_loss: 4.3192 - val_accuracy: 0.2036\n",
      "Epoch 856/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0429 - accuracy: 1.0000 - val_loss: 4.2739 - val_accuracy: 0.2214\n",
      "Epoch 857/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0433 - accuracy: 1.0000 - val_loss: 4.3136 - val_accuracy: 0.2107\n",
      "Epoch 858/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0424 - accuracy: 1.0000 - val_loss: 4.2892 - val_accuracy: 0.2179\n",
      "Epoch 859/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0423 - accuracy: 1.0000 - val_loss: 4.3260 - val_accuracy: 0.2143\n",
      "Epoch 860/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0420 - accuracy: 1.0000 - val_loss: 4.2699 - val_accuracy: 0.2071\n",
      "Epoch 861/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0413 - accuracy: 1.0000 - val_loss: 4.3154 - val_accuracy: 0.2179\n",
      "Epoch 862/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0420 - accuracy: 1.0000 - val_loss: 4.3133 - val_accuracy: 0.2143\n",
      "Epoch 863/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0430 - accuracy: 1.0000 - val_loss: 4.3438 - val_accuracy: 0.2143\n",
      "Epoch 864/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0422 - accuracy: 1.0000 - val_loss: 4.3267 - val_accuracy: 0.2000\n",
      "Epoch 865/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0412 - accuracy: 1.0000 - val_loss: 4.2871 - val_accuracy: 0.2143\n",
      "Epoch 866/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0414 - accuracy: 1.0000 - val_loss: 4.3118 - val_accuracy: 0.2286\n",
      "Epoch 867/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0412 - accuracy: 1.0000 - val_loss: 4.3284 - val_accuracy: 0.2179\n",
      "Epoch 868/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0407 - accuracy: 1.0000 - val_loss: 4.2897 - val_accuracy: 0.2036\n",
      "Epoch 869/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0411 - accuracy: 1.0000 - val_loss: 4.3322 - val_accuracy: 0.2000\n",
      "Epoch 870/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0403 - accuracy: 1.0000 - val_loss: 4.3394 - val_accuracy: 0.2107\n",
      "Epoch 871/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0405 - accuracy: 1.0000 - val_loss: 4.3218 - val_accuracy: 0.2143\n",
      "Epoch 872/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0411 - accuracy: 1.0000 - val_loss: 4.3175 - val_accuracy: 0.2250\n",
      "Epoch 873/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0403 - accuracy: 1.0000 - val_loss: 4.3438 - val_accuracy: 0.2143\n",
      "Epoch 874/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0399 - accuracy: 1.0000 - val_loss: 4.3735 - val_accuracy: 0.1964\n",
      "Epoch 875/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0396 - accuracy: 1.0000 - val_loss: 4.3151 - val_accuracy: 0.2179\n",
      "Epoch 876/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0397 - accuracy: 1.0000 - val_loss: 4.3264 - val_accuracy: 0.2250\n",
      "Epoch 877/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0389 - accuracy: 1.0000 - val_loss: 4.3412 - val_accuracy: 0.2179\n",
      "Epoch 878/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0384 - accuracy: 1.0000 - val_loss: 4.3429 - val_accuracy: 0.2000\n",
      "Epoch 879/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0395 - accuracy: 1.0000 - val_loss: 4.3340 - val_accuracy: 0.2179\n",
      "Epoch 880/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0393 - accuracy: 1.0000 - val_loss: 4.3922 - val_accuracy: 0.2357\n",
      "Epoch 881/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0388 - accuracy: 1.0000 - val_loss: 4.3449 - val_accuracy: 0.2107\n",
      "Epoch 882/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0389 - accuracy: 1.0000 - val_loss: 4.3590 - val_accuracy: 0.2107\n",
      "Epoch 883/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0383 - accuracy: 1.0000 - val_loss: 4.3790 - val_accuracy: 0.2107\n",
      "Epoch 884/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0389 - accuracy: 1.0000 - val_loss: 4.3429 - val_accuracy: 0.2107\n",
      "Epoch 885/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0388 - accuracy: 1.0000 - val_loss: 4.3379 - val_accuracy: 0.2179\n",
      "Epoch 886/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0373 - accuracy: 1.0000 - val_loss: 4.3959 - val_accuracy: 0.2250\n",
      "Epoch 887/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0381 - accuracy: 1.0000 - val_loss: 4.3304 - val_accuracy: 0.2107\n",
      "Epoch 888/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0374 - accuracy: 1.0000 - val_loss: 4.3545 - val_accuracy: 0.2036\n",
      "Epoch 889/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0377 - accuracy: 1.0000 - val_loss: 4.3439 - val_accuracy: 0.2143\n",
      "Epoch 890/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0372 - accuracy: 1.0000 - val_loss: 4.3797 - val_accuracy: 0.2321\n",
      "Epoch 891/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0376 - accuracy: 1.0000 - val_loss: 4.3818 - val_accuracy: 0.2214\n",
      "Epoch 892/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0374 - accuracy: 1.0000 - val_loss: 4.3795 - val_accuracy: 0.2143\n",
      "Epoch 893/1000\n",
      "1120/1120 [==============================] - 7s 6ms/step - loss: 0.0374 - accuracy: 1.0000 - val_loss: 4.3798 - val_accuracy: 0.2143\n",
      "Epoch 894/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0371 - accuracy: 1.0000 - val_loss: 4.3547 - val_accuracy: 0.2214\n",
      "Epoch 895/1000\n",
      "1120/1120 [==============================] - 8s 8ms/step - loss: 0.0356 - accuracy: 1.0000 - val_loss: 4.3690 - val_accuracy: 0.2071\n",
      "Epoch 896/1000\n",
      "1120/1120 [==============================] - 9s 8ms/step - loss: 0.0371 - accuracy: 1.0000 - val_loss: 4.3777 - val_accuracy: 0.2107\n",
      "Epoch 897/1000\n",
      "1120/1120 [==============================] - 9s 8ms/step - loss: 0.0365 - accuracy: 1.0000 - val_loss: 4.3901 - val_accuracy: 0.2071\n",
      "Epoch 898/1000\n",
      "1120/1120 [==============================] - 10s 9ms/step - loss: 0.0356 - accuracy: 1.0000 - val_loss: 4.3843 - val_accuracy: 0.2071\n",
      "Epoch 899/1000\n",
      "1120/1120 [==============================] - 10s 9ms/step - loss: 0.0360 - accuracy: 1.0000 - val_loss: 4.3451 - val_accuracy: 0.2143\n",
      "Epoch 900/1000\n",
      "1120/1120 [==============================] - 9s 8ms/step - loss: 0.0355 - accuracy: 1.0000 - val_loss: 4.3943 - val_accuracy: 0.2143\n",
      "Epoch 901/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0360 - accuracy: 1.0000 - val_loss: 4.4184 - val_accuracy: 0.2214\n",
      "Epoch 902/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0358 - accuracy: 1.0000 - val_loss: 4.3982 - val_accuracy: 0.2214\n",
      "Epoch 903/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0361 - accuracy: 1.0000 - val_loss: 4.4054 - val_accuracy: 0.2071\n",
      "Epoch 904/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0358 - accuracy: 1.0000 - val_loss: 4.4255 - val_accuracy: 0.2071\n",
      "Epoch 905/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0356 - accuracy: 1.0000 - val_loss: 4.3881 - val_accuracy: 0.2143\n",
      "Epoch 906/1000\n",
      "1120/1120 [==============================] - 9s 8ms/step - loss: 0.0352 - accuracy: 1.0000 - val_loss: 4.3968 - val_accuracy: 0.2107\n",
      "Epoch 907/1000\n",
      "1120/1120 [==============================] - 9s 8ms/step - loss: 0.0359 - accuracy: 1.0000 - val_loss: 4.3831 - val_accuracy: 0.1964\n",
      "Epoch 908/1000\n",
      "1120/1120 [==============================] - 7s 7ms/step - loss: 0.0356 - accuracy: 1.0000 - val_loss: 4.3673 - val_accuracy: 0.2214\n",
      "Epoch 909/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0349 - accuracy: 1.0000 - val_loss: 4.4086 - val_accuracy: 0.2214\n",
      "Epoch 910/1000\n",
      "1120/1120 [==============================] - 9s 8ms/step - loss: 0.0345 - accuracy: 1.0000 - val_loss: 4.3890 - val_accuracy: 0.2071\n",
      "Epoch 911/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0345 - accuracy: 1.0000 - val_loss: 4.4241 - val_accuracy: 0.2143\n",
      "Epoch 912/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0344 - accuracy: 1.0000 - val_loss: 4.4053 - val_accuracy: 0.2107\n",
      "Epoch 913/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0345 - accuracy: 1.0000 - val_loss: 4.4049 - val_accuracy: 0.2214\n",
      "Epoch 914/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0344 - accuracy: 1.0000 - val_loss: 4.4227 - val_accuracy: 0.2250\n",
      "Epoch 915/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0341 - accuracy: 1.0000 - val_loss: 4.4034 - val_accuracy: 0.2250\n",
      "Epoch 916/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0339 - accuracy: 1.0000 - val_loss: 4.4191 - val_accuracy: 0.2250\n",
      "Epoch 917/1000\n",
      "1120/1120 [==============================] - 7s 7ms/step - loss: 0.0341 - accuracy: 1.0000 - val_loss: 4.4049 - val_accuracy: 0.2107\n",
      "Epoch 918/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0340 - accuracy: 1.0000 - val_loss: 4.3919 - val_accuracy: 0.2250\n",
      "Epoch 919/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0336 - accuracy: 1.0000 - val_loss: 4.4208 - val_accuracy: 0.2107\n",
      "Epoch 920/1000\n",
      "1120/1120 [==============================] - 7s 7ms/step - loss: 0.0332 - accuracy: 1.0000 - val_loss: 4.4350 - val_accuracy: 0.2036\n",
      "Epoch 921/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0332 - accuracy: 1.0000 - val_loss: 4.4248 - val_accuracy: 0.2286\n",
      "Epoch 922/1000\n",
      "1120/1120 [==============================] - 7s 7ms/step - loss: 0.0330 - accuracy: 1.0000 - val_loss: 4.4412 - val_accuracy: 0.2107\n",
      "Epoch 923/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0334 - accuracy: 1.0000 - val_loss: 4.4206 - val_accuracy: 0.2143\n",
      "Epoch 924/1000\n",
      "1120/1120 [==============================] - 7s 7ms/step - loss: 0.0334 - accuracy: 1.0000 - val_loss: 4.4744 - val_accuracy: 0.2107\n",
      "Epoch 925/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0330 - accuracy: 1.0000 - val_loss: 4.4191 - val_accuracy: 0.2143\n",
      "Epoch 926/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0326 - accuracy: 1.0000 - val_loss: 4.4189 - val_accuracy: 0.2179\n",
      "Epoch 927/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0324 - accuracy: 1.0000 - val_loss: 4.4134 - val_accuracy: 0.2107\n",
      "Epoch 928/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0327 - accuracy: 1.0000 - val_loss: 4.4450 - val_accuracy: 0.2179\n",
      "Epoch 929/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0322 - accuracy: 1.0000 - val_loss: 4.4362 - val_accuracy: 0.2143\n",
      "Epoch 930/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0326 - accuracy: 1.0000 - val_loss: 4.4696 - val_accuracy: 0.2250\n",
      "Epoch 931/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0326 - accuracy: 1.0000 - val_loss: 4.4113 - val_accuracy: 0.1964\n",
      "Epoch 932/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0322 - accuracy: 1.0000 - val_loss: 4.4307 - val_accuracy: 0.2250\n",
      "Epoch 933/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0319 - accuracy: 1.0000 - val_loss: 4.4473 - val_accuracy: 0.2071\n",
      "Epoch 934/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0319 - accuracy: 1.0000 - val_loss: 4.4274 - val_accuracy: 0.2214\n",
      "Epoch 935/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0316 - accuracy: 1.0000 - val_loss: 4.4462 - val_accuracy: 0.2107\n",
      "Epoch 936/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0320 - accuracy: 1.0000 - val_loss: 4.4419 - val_accuracy: 0.2286\n",
      "Epoch 937/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0315 - accuracy: 1.0000 - val_loss: 4.5042 - val_accuracy: 0.2071\n",
      "Epoch 938/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0316 - accuracy: 1.0000 - val_loss: 4.4691 - val_accuracy: 0.2214\n",
      "Epoch 939/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0315 - accuracy: 1.0000 - val_loss: 4.4430 - val_accuracy: 0.2071\n",
      "Epoch 940/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0312 - accuracy: 1.0000 - val_loss: 4.4546 - val_accuracy: 0.2179\n",
      "Epoch 941/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0313 - accuracy: 1.0000 - val_loss: 4.5028 - val_accuracy: 0.2179\n",
      "Epoch 942/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0309 - accuracy: 1.0000 - val_loss: 4.5124 - val_accuracy: 0.2071\n",
      "Epoch 943/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0307 - accuracy: 1.0000 - val_loss: 4.4308 - val_accuracy: 0.2143\n",
      "Epoch 944/1000\n",
      "1120/1120 [==============================] - 9s 8ms/step - loss: 0.0307 - accuracy: 1.0000 - val_loss: 4.4736 - val_accuracy: 0.2179\n",
      "Epoch 945/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0310 - accuracy: 1.0000 - val_loss: 4.4566 - val_accuracy: 0.2071\n",
      "Epoch 946/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0309 - accuracy: 1.0000 - val_loss: 4.4715 - val_accuracy: 0.2179\n",
      "Epoch 947/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0302 - accuracy: 1.0000 - val_loss: 4.4709 - val_accuracy: 0.2107\n",
      "Epoch 948/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0308 - accuracy: 1.0000 - val_loss: 4.4726 - val_accuracy: 0.2107\n",
      "Epoch 949/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0302 - accuracy: 1.0000 - val_loss: 4.4894 - val_accuracy: 0.2214\n",
      "Epoch 950/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0300 - accuracy: 1.0000 - val_loss: 4.4786 - val_accuracy: 0.2286\n",
      "Epoch 951/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0301 - accuracy: 1.0000 - val_loss: 4.4705 - val_accuracy: 0.2107\n",
      "Epoch 952/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0301 - accuracy: 1.0000 - val_loss: 4.4854 - val_accuracy: 0.2071\n",
      "Epoch 953/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0300 - accuracy: 1.0000 - val_loss: 4.4740 - val_accuracy: 0.2107\n",
      "Epoch 954/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0299 - accuracy: 1.0000 - val_loss: 4.4747 - val_accuracy: 0.2143\n",
      "Epoch 955/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0296 - accuracy: 1.0000 - val_loss: 4.4697 - val_accuracy: 0.2214\n",
      "Epoch 956/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0298 - accuracy: 1.0000 - val_loss: 4.4878 - val_accuracy: 0.2107\n",
      "Epoch 957/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0295 - accuracy: 1.0000 - val_loss: 4.4744 - val_accuracy: 0.2179\n",
      "Epoch 958/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0292 - accuracy: 1.0000 - val_loss: 4.4887 - val_accuracy: 0.2000\n",
      "Epoch 959/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0292 - accuracy: 1.0000 - val_loss: 4.4776 - val_accuracy: 0.2357\n",
      "Epoch 960/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0292 - accuracy: 1.0000 - val_loss: 4.4918 - val_accuracy: 0.2107\n",
      "Epoch 961/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0291 - accuracy: 1.0000 - val_loss: 4.5026 - val_accuracy: 0.2071\n",
      "Epoch 962/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0296 - accuracy: 1.0000 - val_loss: 4.4951 - val_accuracy: 0.2214\n",
      "Epoch 963/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0288 - accuracy: 1.0000 - val_loss: 4.5032 - val_accuracy: 0.2214\n",
      "Epoch 964/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0289 - accuracy: 1.0000 - val_loss: 4.5121 - val_accuracy: 0.2071\n",
      "Epoch 965/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0286 - accuracy: 1.0000 - val_loss: 4.4900 - val_accuracy: 0.1964\n",
      "Epoch 966/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0286 - accuracy: 1.0000 - val_loss: 4.5101 - val_accuracy: 0.2286\n",
      "Epoch 967/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0285 - accuracy: 1.0000 - val_loss: 4.4896 - val_accuracy: 0.2107\n",
      "Epoch 968/1000\n",
      "1120/1120 [==============================] - 8s 8ms/step - loss: 0.0284 - accuracy: 1.0000 - val_loss: 4.4812 - val_accuracy: 0.2214\n",
      "Epoch 969/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0284 - accuracy: 1.0000 - val_loss: 4.5219 - val_accuracy: 0.2000\n",
      "Epoch 970/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0284 - accuracy: 1.0000 - val_loss: 4.4942 - val_accuracy: 0.2250\n",
      "Epoch 971/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0281 - accuracy: 1.0000 - val_loss: 4.4923 - val_accuracy: 0.2250\n",
      "Epoch 972/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0280 - accuracy: 1.0000 - val_loss: 4.5111 - val_accuracy: 0.2107\n",
      "Epoch 973/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0279 - accuracy: 1.0000 - val_loss: 4.5092 - val_accuracy: 0.2214\n",
      "Epoch 974/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0280 - accuracy: 1.0000 - val_loss: 4.5116 - val_accuracy: 0.2286\n",
      "Epoch 975/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0275 - accuracy: 1.0000 - val_loss: 4.5216 - val_accuracy: 0.2214\n",
      "Epoch 976/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0278 - accuracy: 1.0000 - val_loss: 4.4970 - val_accuracy: 0.2321\n",
      "Epoch 977/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0274 - accuracy: 1.0000 - val_loss: 4.4986 - val_accuracy: 0.2214\n",
      "Epoch 978/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0278 - accuracy: 1.0000 - val_loss: 4.5187 - val_accuracy: 0.2321\n",
      "Epoch 979/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0274 - accuracy: 1.0000 - val_loss: 4.5008 - val_accuracy: 0.2250\n",
      "Epoch 980/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0274 - accuracy: 1.0000 - val_loss: 4.5159 - val_accuracy: 0.2143\n",
      "Epoch 981/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0277 - accuracy: 1.0000 - val_loss: 4.5197 - val_accuracy: 0.2179\n",
      "Epoch 982/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0272 - accuracy: 1.0000 - val_loss: 4.5185 - val_accuracy: 0.2214\n",
      "Epoch 983/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0274 - accuracy: 1.0000 - val_loss: 4.5307 - val_accuracy: 0.2071\n",
      "Epoch 984/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0267 - accuracy: 1.0000 - val_loss: 4.5193 - val_accuracy: 0.2107\n",
      "Epoch 985/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0271 - accuracy: 1.0000 - val_loss: 4.5228 - val_accuracy: 0.2036\n",
      "Epoch 986/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0270 - accuracy: 1.0000 - val_loss: 4.5289 - val_accuracy: 0.2250\n",
      "Epoch 987/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0265 - accuracy: 1.0000 - val_loss: 4.5270 - val_accuracy: 0.2286\n",
      "Epoch 988/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0265 - accuracy: 1.0000 - val_loss: 4.5235 - val_accuracy: 0.2250\n",
      "Epoch 989/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0269 - accuracy: 1.0000 - val_loss: 4.5251 - val_accuracy: 0.2107\n",
      "Epoch 990/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0264 - accuracy: 1.0000 - val_loss: 4.5338 - val_accuracy: 0.2071\n",
      "Epoch 991/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0262 - accuracy: 1.0000 - val_loss: 4.5205 - val_accuracy: 0.2286\n",
      "Epoch 992/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0264 - accuracy: 1.0000 - val_loss: 4.5272 - val_accuracy: 0.2214\n",
      "Epoch 993/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0266 - accuracy: 1.0000 - val_loss: 4.5374 - val_accuracy: 0.2214\n",
      "Epoch 994/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0260 - accuracy: 1.0000 - val_loss: 4.5294 - val_accuracy: 0.2214\n",
      "Epoch 995/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0265 - accuracy: 1.0000 - val_loss: 4.5541 - val_accuracy: 0.2071\n",
      "Epoch 996/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0263 - accuracy: 1.0000 - val_loss: 4.5421 - val_accuracy: 0.2214\n",
      "Epoch 997/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0258 - accuracy: 1.0000 - val_loss: 4.5398 - val_accuracy: 0.2214\n",
      "Epoch 998/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0257 - accuracy: 1.0000 - val_loss: 4.5452 - val_accuracy: 0.2036\n",
      "Epoch 999/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0254 - accuracy: 1.0000 - val_loss: 4.5521 - val_accuracy: 0.2107\n",
      "Epoch 1000/1000\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.0259 - accuracy: 1.0000 - val_loss: 4.5759 - val_accuracy: 0.2143\n",
      "\n",
      "Keras VGG16 #2 - accuracy: 0.2142857164144516 \n",
      "\n",
      "\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "        No Finding       0.21      0.67      0.32         9\n",
      "     Consolidation       0.25      0.20      0.22        25\n",
      "      Infiltration       0.20      0.23      0.21        22\n",
      "      Pneumothorax       0.21      0.14      0.17        22\n",
      "             Edema       0.32      0.41      0.36        17\n",
      "         Emphysema       0.32      0.32      0.32        19\n",
      "          Fibrosis       0.24      0.22      0.23        27\n",
      "          Effusion       0.22      0.19      0.21        21\n",
      "         Pneumonia       0.06      0.04      0.05        24\n",
      "Pleural Thickening       0.12      0.14      0.13        22\n",
      "      Cardiomegaly       0.18      0.12      0.14        17\n",
      "       Nodule Mass       0.24      0.28      0.26        18\n",
      "            Hernia       0.28      0.25      0.26        20\n",
      "       Atelectasis       0.12      0.12      0.12        17\n",
      "\n",
      "          accuracy                           0.21       280\n",
      "         macro avg       0.21      0.24      0.21       280\n",
      "      weighted avg       0.21      0.21      0.21       280\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Object arrays cannot be loaded when allow_pickle=False",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-554b75babec2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvgg16network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_trainHot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_testHot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0mplot_learning_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0mplotKerasLearningCurve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-204fdc86a5f9>\u001b[0m in \u001b[0;36mplotKerasLearningCurve\u001b[0;34m(history)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplotKerasLearningCurve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'logs.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mfilt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# try to add 'loss' to see the loss learning curve\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfilt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/tfl/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    451\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m                 return format.read_array(fid, allow_pickle=allow_pickle,\n\u001b[0;32m--> 453\u001b[0;31m                                          pickle_kwargs=pickle_kwargs)\n\u001b[0m\u001b[1;32m    454\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m             \u001b[0;31m# Try a pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/tfl/lib/python3.6/site-packages/numpy/lib/format.py\u001b[0m in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m         \u001b[0;31m# The array contained Python objects. We need to unpickle the data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             raise ValueError(\"Object arrays cannot be loaded when \"\n\u001b[0m\u001b[1;32m    723\u001b[0m                              \"allow_pickle=False\")\n\u001b[1;32m    724\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpickle_kwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Object arrays cannot be loaded when allow_pickle=False"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "weight_path = './keras-pretrained-models/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "map_characters=dict_characters\n",
    "\n",
    "with tf.device('/device:GPU:0'):\n",
    "\n",
    "    def vgg16network(a,b,c,d,e,f,g):\n",
    "        num_class = f\n",
    "        epochs = g\n",
    "        base_model = VGG16(weights='imagenet',\n",
    "            include_top=False, input_shape=(img_size, img_size, 3))\n",
    "        # Add a new top layer\n",
    "        x = base_model.output\n",
    "        x = Flatten()(x)\n",
    "#         x = BatchNormalization()(x)\n",
    "#         x = Dropout(0.25)(x)\n",
    "        x = Dense(512, activation = \"relu\",kernel_regularizer=keras.regularizers.l2(0.0)\n",
    "                 ,bias_regularizer=keras.regularizers.l2(0.0))(x)\n",
    "#         x = BatchNormalization()(x)\n",
    "#         x = Dropout(0.5)(x)\n",
    "        x = Dense(128, activation = \"relu\",kernel_regularizer=keras.regularizers.l2(0.)\n",
    "                 ,bias_regularizer=keras.regularizers.l2(0.))(x)\n",
    "#         x = BatchNormalization()(x)\n",
    "#         x = Dropout(0.5)(x)\n",
    "        predictions = Dense(num_class, activation='softmax')(x)\n",
    "        # This is the model we will train\n",
    "        model = Model(inputs=base_model.input, outputs=predictions)\n",
    "        # First: train only the top layers (which were randomly initialized)\n",
    "        for layer in base_model.layers:\n",
    "            layer.trainable = False\n",
    "        model.compile(loss='categorical_crossentropy', \n",
    "                      optimizer=keras.optimizers.SGD(lr=0.005), metrics=['accuracy'])\n",
    "        callbacks_list = [keras.callbacks.EarlyStopping(monitor='val_acc', patience=3, verbose=1)]\n",
    "        model.summary()\n",
    "        history = model.fit(a,b, epochs=epochs, class_weight=e, \n",
    "                            validation_data=(c,d), verbose=1,callbacks = [MetricsCheckpoint('logs')])\n",
    "        score = model.evaluate(c,d, verbose=0)\n",
    "        print('\\nKeras VGG16 #2 - accuracy:', score[1], '\\n')\n",
    "        y_pred = model.predict(c)\n",
    "        print('\\n', sklearn.metrics.classification_report(np.where(d > 0)[1], np.argmax(y_pred, axis=1), target_names=list(map_characters.values())), sep='') \n",
    "        Y_pred_classes = np.argmax(y_pred,axis = 1) \n",
    "        Y_true = np.argmax(d,axis = 1) \n",
    "        confusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n",
    "        plot_confusion_matrix(confusion_mtx, classes = list(map_characters.values()))\n",
    "        plt.show()\n",
    "        return model, history\n",
    "model, history = vgg16network(X_train, Y_trainHot, X_test, Y_testHot,class_weight,14,1000)\n",
    "plot_learning_curve(history)\n",
    "plotKerasLearningCurve(history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_cell_guid": "5f8a5979-838d-4f0e-8b0a-e601a8b660e6",
    "_uuid": "9b46de5b9536718d54f39ed593ee68bbcf53da1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 79, 79, 3)         0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 79, 79, 64)        1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 79, 79, 64)        36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 39, 39, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 39, 39, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 39, 39, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 19, 19, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 19, 19, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 19, 19, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 19, 19, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 9, 9, 256)         0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 9, 9, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                131072    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 14)                910       \n",
      "=================================================================\n",
      "Total params: 14,846,926\n",
      "Trainable params: 132,110\n",
      "Non-trainable params: 14,714,816\n",
      "_________________________________________________________________\n",
      "Train on 980 samples, validate on 420 samples\n",
      "Epoch 1/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 6.0189 - accuracy: 0.0735 - val_loss: 4.4853 - val_accuracy: 0.0833\n",
      "Epoch 2/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 5.0650 - accuracy: 0.0806 - val_loss: 4.4705 - val_accuracy: 0.0833\n",
      "Epoch 3/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 5.0331 - accuracy: 0.0663 - val_loss: 4.4548 - val_accuracy: 0.0810\n",
      "Epoch 4/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 4.9709 - accuracy: 0.0816 - val_loss: 4.4418 - val_accuracy: 0.0762\n",
      "Epoch 5/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 4.9630 - accuracy: 0.0643 - val_loss: 4.4313 - val_accuracy: 0.0786\n",
      "Epoch 6/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 4.9151 - accuracy: 0.0735 - val_loss: 4.4205 - val_accuracy: 0.0786\n",
      "Epoch 7/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 4.8503 - accuracy: 0.0837 - val_loss: 4.4102 - val_accuracy: 0.0833\n",
      "Epoch 8/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 4.8580 - accuracy: 0.0745 - val_loss: 4.4067 - val_accuracy: 0.0810\n",
      "Epoch 9/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 4.7856 - accuracy: 0.0929 - val_loss: 4.3996 - val_accuracy: 0.0810\n",
      "Epoch 10/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 4.7377 - accuracy: 0.0980 - val_loss: 4.4009 - val_accuracy: 0.0905\n",
      "Epoch 11/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 4.7310 - accuracy: 0.0888 - val_loss: 4.3966 - val_accuracy: 0.0929\n",
      "Epoch 12/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 4.7272 - accuracy: 0.1000 - val_loss: 4.3953 - val_accuracy: 0.0905\n",
      "Epoch 13/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 4.6942 - accuracy: 0.1000 - val_loss: 4.3934 - val_accuracy: 0.1000\n",
      "Epoch 14/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 4.6552 - accuracy: 0.1041 - val_loss: 4.3902 - val_accuracy: 0.0952\n",
      "Epoch 15/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 4.6338 - accuracy: 0.0990 - val_loss: 4.3849 - val_accuracy: 0.1071\n",
      "Epoch 16/1000\n",
      "980/980 [==============================] - 8s 9ms/step - loss: 4.5105 - accuracy: 0.1194 - val_loss: 4.3855 - val_accuracy: 0.1024\n",
      "Epoch 17/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 4.5536 - accuracy: 0.1092 - val_loss: 4.3817 - val_accuracy: 0.1071\n",
      "Epoch 18/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 4.5256 - accuracy: 0.1041 - val_loss: 4.3776 - val_accuracy: 0.1024\n",
      "Epoch 19/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 4.5480 - accuracy: 0.1133 - val_loss: 4.3695 - val_accuracy: 0.1095\n",
      "Epoch 20/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 4.4938 - accuracy: 0.1194 - val_loss: 4.3595 - val_accuracy: 0.1167\n",
      "Epoch 21/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 4.5096 - accuracy: 0.1082 - val_loss: 4.3565 - val_accuracy: 0.1119\n",
      "Epoch 22/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 4.4893 - accuracy: 0.1071 - val_loss: 4.3474 - val_accuracy: 0.1167\n",
      "Epoch 23/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 4.4059 - accuracy: 0.1133 - val_loss: 4.3403 - val_accuracy: 0.1190\n",
      "Epoch 24/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 4.4045 - accuracy: 0.1316 - val_loss: 4.3315 - val_accuracy: 0.1190\n",
      "Epoch 25/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 4.3892 - accuracy: 0.1337 - val_loss: 4.3238 - val_accuracy: 0.1167\n",
      "Epoch 26/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 4.3754 - accuracy: 0.1173 - val_loss: 4.3166 - val_accuracy: 0.1167\n",
      "Epoch 27/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 4.3359 - accuracy: 0.1265 - val_loss: 4.3058 - val_accuracy: 0.1214\n",
      "Epoch 28/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 4.4033 - accuracy: 0.1235 - val_loss: 4.2999 - val_accuracy: 0.1310\n",
      "Epoch 29/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 4.3373 - accuracy: 0.1439 - val_loss: 4.2892 - val_accuracy: 0.1143\n",
      "Epoch 30/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 4.3240 - accuracy: 0.1306 - val_loss: 4.2800 - val_accuracy: 0.1167\n",
      "Epoch 31/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 4.3437 - accuracy: 0.1204 - val_loss: 4.2783 - val_accuracy: 0.1286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 4.3209 - accuracy: 0.1255 - val_loss: 4.2696 - val_accuracy: 0.1214\n",
      "Epoch 33/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 4.2294 - accuracy: 0.1592 - val_loss: 4.2615 - val_accuracy: 0.1310\n",
      "Epoch 34/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 4.1951 - accuracy: 0.1602 - val_loss: 4.2565 - val_accuracy: 0.1262\n",
      "Epoch 35/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 4.2246 - accuracy: 0.1459 - val_loss: 4.2433 - val_accuracy: 0.1286\n",
      "Epoch 36/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 4.1982 - accuracy: 0.1408 - val_loss: 4.2380 - val_accuracy: 0.1238\n",
      "Epoch 37/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 4.1777 - accuracy: 0.1735 - val_loss: 4.2306 - val_accuracy: 0.1262\n",
      "Epoch 38/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 4.2306 - accuracy: 0.1449 - val_loss: 4.2265 - val_accuracy: 0.1333\n",
      "Epoch 39/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 4.1330 - accuracy: 0.1561 - val_loss: 4.2209 - val_accuracy: 0.1286\n",
      "Epoch 40/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 4.1227 - accuracy: 0.1898 - val_loss: 4.2155 - val_accuracy: 0.1357\n",
      "Epoch 41/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 4.1366 - accuracy: 0.1673 - val_loss: 4.2072 - val_accuracy: 0.1333\n",
      "Epoch 42/1000\n",
      "980/980 [==============================] - 8s 9ms/step - loss: 4.1300 - accuracy: 0.1663 - val_loss: 4.2072 - val_accuracy: 0.1357\n",
      "Epoch 43/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 4.1247 - accuracy: 0.2010 - val_loss: 4.1955 - val_accuracy: 0.1381\n",
      "Epoch 44/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 4.0968 - accuracy: 0.1786 - val_loss: 4.1866 - val_accuracy: 0.1357\n",
      "Epoch 45/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 4.0821 - accuracy: 0.1592 - val_loss: 4.1827 - val_accuracy: 0.1381\n",
      "Epoch 46/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 4.0557 - accuracy: 0.1714 - val_loss: 4.1834 - val_accuracy: 0.1452\n",
      "Epoch 47/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 4.0479 - accuracy: 0.1827 - val_loss: 4.1750 - val_accuracy: 0.1452\n",
      "Epoch 48/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 4.0101 - accuracy: 0.1837 - val_loss: 4.1676 - val_accuracy: 0.1429\n",
      "Epoch 49/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 4.0220 - accuracy: 0.2020 - val_loss: 4.1574 - val_accuracy: 0.1429\n",
      "Epoch 50/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 4.0424 - accuracy: 0.1582 - val_loss: 4.1616 - val_accuracy: 0.1429\n",
      "Epoch 51/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.9926 - accuracy: 0.1867 - val_loss: 4.1542 - val_accuracy: 0.1429\n",
      "Epoch 52/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.9708 - accuracy: 0.1837 - val_loss: 4.1492 - val_accuracy: 0.1333\n",
      "Epoch 53/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.9397 - accuracy: 0.1959 - val_loss: 4.1500 - val_accuracy: 0.1238\n",
      "Epoch 54/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 4.0228 - accuracy: 0.1837 - val_loss: 4.1446 - val_accuracy: 0.1333\n",
      "Epoch 55/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.9436 - accuracy: 0.1867 - val_loss: 4.1348 - val_accuracy: 0.1357\n",
      "Epoch 56/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.9297 - accuracy: 0.1898 - val_loss: 4.1310 - val_accuracy: 0.1286\n",
      "Epoch 57/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.9453 - accuracy: 0.2000 - val_loss: 4.1269 - val_accuracy: 0.1262\n",
      "Epoch 58/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.8908 - accuracy: 0.2051 - val_loss: 4.1223 - val_accuracy: 0.1310\n",
      "Epoch 59/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.8656 - accuracy: 0.2143 - val_loss: 4.1188 - val_accuracy: 0.1333\n",
      "Epoch 60/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.8934 - accuracy: 0.2214 - val_loss: 4.1115 - val_accuracy: 0.1238\n",
      "Epoch 61/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.8329 - accuracy: 0.2286 - val_loss: 4.1112 - val_accuracy: 0.1262\n",
      "Epoch 62/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.8874 - accuracy: 0.2153 - val_loss: 4.1104 - val_accuracy: 0.1310\n",
      "Epoch 63/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.8271 - accuracy: 0.2388 - val_loss: 4.1032 - val_accuracy: 0.1238\n",
      "Epoch 64/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.8679 - accuracy: 0.2327 - val_loss: 4.1014 - val_accuracy: 0.1214\n",
      "Epoch 65/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.8371 - accuracy: 0.2296 - val_loss: 4.0984 - val_accuracy: 0.1167\n",
      "Epoch 66/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.8142 - accuracy: 0.2480 - val_loss: 4.0938 - val_accuracy: 0.1238\n",
      "Epoch 67/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.8487 - accuracy: 0.2184 - val_loss: 4.0936 - val_accuracy: 0.1333\n",
      "Epoch 68/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.7931 - accuracy: 0.2408 - val_loss: 4.0898 - val_accuracy: 0.1214\n",
      "Epoch 69/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.7847 - accuracy: 0.2276 - val_loss: 4.0813 - val_accuracy: 0.1119\n",
      "Epoch 70/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.7357 - accuracy: 0.2612 - val_loss: 4.0785 - val_accuracy: 0.1071\n",
      "Epoch 71/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.7690 - accuracy: 0.2469 - val_loss: 4.0798 - val_accuracy: 0.1167\n",
      "Epoch 72/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.7364 - accuracy: 0.2541 - val_loss: 4.0749 - val_accuracy: 0.1167\n",
      "Epoch 73/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.7670 - accuracy: 0.2316 - val_loss: 4.0722 - val_accuracy: 0.1143\n",
      "Epoch 74/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.7047 - accuracy: 0.2582 - val_loss: 4.0637 - val_accuracy: 0.1190\n",
      "Epoch 75/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.7052 - accuracy: 0.2500 - val_loss: 4.0641 - val_accuracy: 0.1143\n",
      "Epoch 76/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.7335 - accuracy: 0.2622 - val_loss: 4.0623 - val_accuracy: 0.1190\n",
      "Epoch 77/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.7191 - accuracy: 0.2490 - val_loss: 4.0517 - val_accuracy: 0.1238\n",
      "Epoch 78/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.7180 - accuracy: 0.2459 - val_loss: 4.0505 - val_accuracy: 0.1286\n",
      "Epoch 79/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.7150 - accuracy: 0.2429 - val_loss: 4.0485 - val_accuracy: 0.1357\n",
      "Epoch 80/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.6502 - accuracy: 0.2500 - val_loss: 4.0480 - val_accuracy: 0.1262\n",
      "Epoch 81/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.6731 - accuracy: 0.2612 - val_loss: 4.0416 - val_accuracy: 0.1214\n",
      "Epoch 82/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.7165 - accuracy: 0.2418 - val_loss: 4.0395 - val_accuracy: 0.1214\n",
      "Epoch 83/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.6010 - accuracy: 0.2867 - val_loss: 4.0348 - val_accuracy: 0.1357\n",
      "Epoch 84/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.6251 - accuracy: 0.2980 - val_loss: 4.0312 - val_accuracy: 0.1119\n",
      "Epoch 85/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.6189 - accuracy: 0.2714 - val_loss: 4.0318 - val_accuracy: 0.1310\n",
      "Epoch 86/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.6472 - accuracy: 0.2796 - val_loss: 4.0255 - val_accuracy: 0.1190\n",
      "Epoch 87/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.6011 - accuracy: 0.2939 - val_loss: 4.0291 - val_accuracy: 0.1310\n",
      "Epoch 88/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.6077 - accuracy: 0.2786 - val_loss: 4.0253 - val_accuracy: 0.1357\n",
      "Epoch 89/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.6396 - accuracy: 0.2684 - val_loss: 4.0182 - val_accuracy: 0.1333\n",
      "Epoch 90/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.6105 - accuracy: 0.2755 - val_loss: 4.0103 - val_accuracy: 0.1333\n",
      "Epoch 91/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.5488 - accuracy: 0.3031 - val_loss: 4.0149 - val_accuracy: 0.1381\n",
      "Epoch 92/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.6012 - accuracy: 0.2929 - val_loss: 4.0117 - val_accuracy: 0.1429\n",
      "Epoch 93/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.5532 - accuracy: 0.2878 - val_loss: 4.0087 - val_accuracy: 0.1357\n",
      "Epoch 94/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.5607 - accuracy: 0.2980 - val_loss: 4.0058 - val_accuracy: 0.1452\n",
      "Epoch 95/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.5654 - accuracy: 0.2806 - val_loss: 4.0097 - val_accuracy: 0.1262\n",
      "Epoch 96/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.5431 - accuracy: 0.3051 - val_loss: 4.0006 - val_accuracy: 0.1476\n",
      "Epoch 97/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.5173 - accuracy: 0.3082 - val_loss: 4.0064 - val_accuracy: 0.1381\n",
      "Epoch 98/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.5427 - accuracy: 0.3163 - val_loss: 3.9982 - val_accuracy: 0.1429\n",
      "Epoch 99/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.5271 - accuracy: 0.3071 - val_loss: 3.9981 - val_accuracy: 0.1405\n",
      "Epoch 100/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.5059 - accuracy: 0.3102 - val_loss: 3.9941 - val_accuracy: 0.1405\n",
      "Epoch 101/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.5390 - accuracy: 0.2847 - val_loss: 3.9958 - val_accuracy: 0.1452\n",
      "Epoch 102/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.5012 - accuracy: 0.3102 - val_loss: 3.9902 - val_accuracy: 0.1452\n",
      "Epoch 103/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.4794 - accuracy: 0.3010 - val_loss: 3.9879 - val_accuracy: 0.1452\n",
      "Epoch 104/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.5174 - accuracy: 0.3235 - val_loss: 3.9819 - val_accuracy: 0.1548\n",
      "Epoch 105/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.4804 - accuracy: 0.3061 - val_loss: 3.9812 - val_accuracy: 0.1500\n",
      "Epoch 106/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.4935 - accuracy: 0.2918 - val_loss: 3.9817 - val_accuracy: 0.1381\n",
      "Epoch 107/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.4845 - accuracy: 0.3102 - val_loss: 3.9829 - val_accuracy: 0.1429\n",
      "Epoch 108/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.4405 - accuracy: 0.3327 - val_loss: 3.9800 - val_accuracy: 0.1476\n",
      "Epoch 109/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.4405 - accuracy: 0.3316 - val_loss: 3.9787 - val_accuracy: 0.1548\n",
      "Epoch 110/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.4301 - accuracy: 0.3214 - val_loss: 3.9732 - val_accuracy: 0.1548\n",
      "Epoch 111/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.4109 - accuracy: 0.3316 - val_loss: 3.9695 - val_accuracy: 0.1500\n",
      "Epoch 112/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.4163 - accuracy: 0.3337 - val_loss: 3.9700 - val_accuracy: 0.1548\n",
      "Epoch 113/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.4483 - accuracy: 0.3194 - val_loss: 3.9645 - val_accuracy: 0.1452\n",
      "Epoch 114/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.3812 - accuracy: 0.3439 - val_loss: 3.9604 - val_accuracy: 0.1595\n",
      "Epoch 115/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.4106 - accuracy: 0.3265 - val_loss: 3.9616 - val_accuracy: 0.1595\n",
      "Epoch 116/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.3871 - accuracy: 0.3265 - val_loss: 3.9625 - val_accuracy: 0.1524\n",
      "Epoch 117/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.4118 - accuracy: 0.3214 - val_loss: 3.9565 - val_accuracy: 0.1476\n",
      "Epoch 118/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.4118 - accuracy: 0.3327 - val_loss: 3.9555 - val_accuracy: 0.1500\n",
      "Epoch 119/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.3460 - accuracy: 0.3602 - val_loss: 3.9559 - val_accuracy: 0.1476\n",
      "Epoch 120/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.3930 - accuracy: 0.3439 - val_loss: 3.9508 - val_accuracy: 0.1524\n",
      "Epoch 121/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.3555 - accuracy: 0.3571 - val_loss: 3.9424 - val_accuracy: 0.1571\n",
      "Epoch 122/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.3792 - accuracy: 0.3296 - val_loss: 3.9439 - val_accuracy: 0.1619\n",
      "Epoch 123/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.3781 - accuracy: 0.3429 - val_loss: 3.9439 - val_accuracy: 0.1548\n",
      "Epoch 124/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.3718 - accuracy: 0.3429 - val_loss: 3.9464 - val_accuracy: 0.1643\n",
      "Epoch 125/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.3453 - accuracy: 0.3367 - val_loss: 3.9442 - val_accuracy: 0.1595\n",
      "Epoch 126/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.3826 - accuracy: 0.3265 - val_loss: 3.9418 - val_accuracy: 0.1571\n",
      "Epoch 127/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.3313 - accuracy: 0.3582 - val_loss: 3.9452 - val_accuracy: 0.1476\n",
      "Epoch 128/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.3563 - accuracy: 0.3520 - val_loss: 3.9410 - val_accuracy: 0.1548\n",
      "Epoch 129/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.2674 - accuracy: 0.3714 - val_loss: 3.9387 - val_accuracy: 0.1643\n",
      "Epoch 130/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.2802 - accuracy: 0.3735 - val_loss: 3.9365 - val_accuracy: 0.1500\n",
      "Epoch 131/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.3454 - accuracy: 0.3561 - val_loss: 3.9376 - val_accuracy: 0.1571\n",
      "Epoch 132/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.3233 - accuracy: 0.3531 - val_loss: 3.9334 - val_accuracy: 0.1643\n",
      "Epoch 133/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.3228 - accuracy: 0.3551 - val_loss: 3.9359 - val_accuracy: 0.1571\n",
      "Epoch 134/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.3291 - accuracy: 0.3398 - val_loss: 3.9331 - val_accuracy: 0.1571\n",
      "Epoch 135/1000\n",
      "980/980 [==============================] - 9s 9ms/step - loss: 3.2240 - accuracy: 0.3990 - val_loss: 3.9259 - val_accuracy: 0.1667\n",
      "Epoch 136/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.2314 - accuracy: 0.3796 - val_loss: 3.9258 - val_accuracy: 0.1619\n",
      "Epoch 137/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.2476 - accuracy: 0.3796 - val_loss: 3.9212 - val_accuracy: 0.1667\n",
      "Epoch 138/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.2832 - accuracy: 0.3745 - val_loss: 3.9201 - val_accuracy: 0.1643\n",
      "Epoch 139/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.2405 - accuracy: 0.3571 - val_loss: 3.9207 - val_accuracy: 0.1619\n",
      "Epoch 140/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.2830 - accuracy: 0.3592 - val_loss: 3.9186 - val_accuracy: 0.1690\n",
      "Epoch 141/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.2824 - accuracy: 0.3663 - val_loss: 3.9112 - val_accuracy: 0.1667\n",
      "Epoch 142/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.2723 - accuracy: 0.3571 - val_loss: 3.9190 - val_accuracy: 0.1476\n",
      "Epoch 143/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.2066 - accuracy: 0.3878 - val_loss: 3.9114 - val_accuracy: 0.1595\n",
      "Epoch 144/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "980/980 [==============================] - 8s 8ms/step - loss: 3.2510 - accuracy: 0.3704 - val_loss: 3.9113 - val_accuracy: 0.1500\n",
      "Epoch 145/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.2571 - accuracy: 0.3653 - val_loss: 3.9103 - val_accuracy: 0.1714\n",
      "Epoch 146/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.1921 - accuracy: 0.3969 - val_loss: 3.9057 - val_accuracy: 0.1643\n",
      "Epoch 147/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.1977 - accuracy: 0.3857 - val_loss: 3.9021 - val_accuracy: 0.1643\n",
      "Epoch 148/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.1833 - accuracy: 0.4143 - val_loss: 3.9032 - val_accuracy: 0.1714\n",
      "Epoch 149/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.1912 - accuracy: 0.3786 - val_loss: 3.9075 - val_accuracy: 0.1571\n",
      "Epoch 150/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.1763 - accuracy: 0.4082 - val_loss: 3.9027 - val_accuracy: 0.1643\n",
      "Epoch 151/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.2023 - accuracy: 0.3908 - val_loss: 3.8997 - val_accuracy: 0.1548\n",
      "Epoch 152/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.1742 - accuracy: 0.3888 - val_loss: 3.8974 - val_accuracy: 0.1619\n",
      "Epoch 153/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.1346 - accuracy: 0.4031 - val_loss: 3.9062 - val_accuracy: 0.1524\n",
      "Epoch 154/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.1649 - accuracy: 0.3949 - val_loss: 3.9034 - val_accuracy: 0.1571\n",
      "Epoch 155/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.1821 - accuracy: 0.3929 - val_loss: 3.8983 - val_accuracy: 0.1667\n",
      "Epoch 156/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.1358 - accuracy: 0.4255 - val_loss: 3.8914 - val_accuracy: 0.1690\n",
      "Epoch 157/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.1471 - accuracy: 0.3969 - val_loss: 3.8988 - val_accuracy: 0.1619\n",
      "Epoch 158/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.1383 - accuracy: 0.4173 - val_loss: 3.8971 - val_accuracy: 0.1643\n",
      "Epoch 159/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.1176 - accuracy: 0.4265 - val_loss: 3.8931 - val_accuracy: 0.1619\n",
      "Epoch 160/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.1161 - accuracy: 0.3939 - val_loss: 3.8979 - val_accuracy: 0.1667\n",
      "Epoch 161/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.0863 - accuracy: 0.4276 - val_loss: 3.8949 - val_accuracy: 0.1619\n",
      "Epoch 162/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.0986 - accuracy: 0.4224 - val_loss: 3.8941 - val_accuracy: 0.1667\n",
      "Epoch 163/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.1302 - accuracy: 0.4224 - val_loss: 3.8884 - val_accuracy: 0.1643\n",
      "Epoch 164/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.0763 - accuracy: 0.4306 - val_loss: 3.8854 - val_accuracy: 0.1810\n",
      "Epoch 165/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.1037 - accuracy: 0.4235 - val_loss: 3.8896 - val_accuracy: 0.1667\n",
      "Epoch 166/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.1024 - accuracy: 0.4265 - val_loss: 3.8874 - val_accuracy: 0.1762\n",
      "Epoch 167/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.1237 - accuracy: 0.3959 - val_loss: 3.8774 - val_accuracy: 0.1690\n",
      "Epoch 168/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.1060 - accuracy: 0.4153 - val_loss: 3.8763 - val_accuracy: 0.1762\n",
      "Epoch 169/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.0843 - accuracy: 0.4173 - val_loss: 3.8803 - val_accuracy: 0.1667\n",
      "Epoch 170/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.0673 - accuracy: 0.4306 - val_loss: 3.8764 - val_accuracy: 0.1667\n",
      "Epoch 171/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.0647 - accuracy: 0.4327 - val_loss: 3.8817 - val_accuracy: 0.1667\n",
      "Epoch 172/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.0625 - accuracy: 0.4357 - val_loss: 3.8770 - val_accuracy: 0.1714\n",
      "Epoch 173/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.0723 - accuracy: 0.4204 - val_loss: 3.8759 - val_accuracy: 0.1619\n",
      "Epoch 174/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.0716 - accuracy: 0.4306 - val_loss: 3.8735 - val_accuracy: 0.1690\n",
      "Epoch 175/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.0159 - accuracy: 0.4643 - val_loss: 3.8717 - val_accuracy: 0.1738\n",
      "Epoch 176/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.0416 - accuracy: 0.4418 - val_loss: 3.8696 - val_accuracy: 0.1643\n",
      "Epoch 177/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.0755 - accuracy: 0.4286 - val_loss: 3.8670 - val_accuracy: 0.1714\n",
      "Epoch 178/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.0473 - accuracy: 0.4316 - val_loss: 3.8673 - val_accuracy: 0.1738\n",
      "Epoch 179/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.0288 - accuracy: 0.4439 - val_loss: 3.8708 - val_accuracy: 0.1667\n",
      "Epoch 180/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.0164 - accuracy: 0.4480 - val_loss: 3.8678 - val_accuracy: 0.1595\n",
      "Epoch 181/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.0396 - accuracy: 0.4398 - val_loss: 3.8660 - val_accuracy: 0.1595\n",
      "Epoch 182/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.0215 - accuracy: 0.4541 - val_loss: 3.8694 - val_accuracy: 0.1690\n",
      "Epoch 183/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.0189 - accuracy: 0.4337 - val_loss: 3.8630 - val_accuracy: 0.1690\n",
      "Epoch 184/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.9698 - accuracy: 0.4786 - val_loss: 3.8635 - val_accuracy: 0.1619\n",
      "Epoch 185/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.9941 - accuracy: 0.4653 - val_loss: 3.8585 - val_accuracy: 0.1762\n",
      "Epoch 186/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.9836 - accuracy: 0.4653 - val_loss: 3.8639 - val_accuracy: 0.1714\n",
      "Epoch 187/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.0213 - accuracy: 0.4582 - val_loss: 3.8605 - val_accuracy: 0.1690\n",
      "Epoch 188/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.0045 - accuracy: 0.4347 - val_loss: 3.8599 - val_accuracy: 0.1595\n",
      "Epoch 189/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.0031 - accuracy: 0.4582 - val_loss: 3.8490 - val_accuracy: 0.1786\n",
      "Epoch 190/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.9780 - accuracy: 0.4561 - val_loss: 3.8592 - val_accuracy: 0.1738\n",
      "Epoch 191/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 3.0117 - accuracy: 0.4347 - val_loss: 3.8511 - val_accuracy: 0.1690\n",
      "Epoch 192/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.9414 - accuracy: 0.4755 - val_loss: 3.8510 - val_accuracy: 0.1786\n",
      "Epoch 193/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.9738 - accuracy: 0.4612 - val_loss: 3.8545 - val_accuracy: 0.1714\n",
      "Epoch 194/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.9401 - accuracy: 0.4755 - val_loss: 3.8556 - val_accuracy: 0.1667\n",
      "Epoch 195/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.9821 - accuracy: 0.4439 - val_loss: 3.8432 - val_accuracy: 0.1786\n",
      "Epoch 196/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.9480 - accuracy: 0.4663 - val_loss: 3.8458 - val_accuracy: 0.1738\n",
      "Epoch 197/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.9450 - accuracy: 0.4592 - val_loss: 3.8443 - val_accuracy: 0.1667\n",
      "Epoch 198/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.9276 - accuracy: 0.4704 - val_loss: 3.8404 - val_accuracy: 0.1786\n",
      "Epoch 199/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.9297 - accuracy: 0.4898 - val_loss: 3.8418 - val_accuracy: 0.1714\n",
      "Epoch 200/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.9256 - accuracy: 0.4776 - val_loss: 3.8401 - val_accuracy: 0.1762\n",
      "Epoch 201/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.9467 - accuracy: 0.4367 - val_loss: 3.8387 - val_accuracy: 0.1762\n",
      "Epoch 202/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.9116 - accuracy: 0.4878 - val_loss: 3.8349 - val_accuracy: 0.1810\n",
      "Epoch 203/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.9294 - accuracy: 0.4735 - val_loss: 3.8308 - val_accuracy: 0.1786\n",
      "Epoch 204/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.9091 - accuracy: 0.4765 - val_loss: 3.8339 - val_accuracy: 0.1810\n",
      "Epoch 205/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.9147 - accuracy: 0.4653 - val_loss: 3.8366 - val_accuracy: 0.1833\n",
      "Epoch 206/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.9008 - accuracy: 0.4765 - val_loss: 3.8368 - val_accuracy: 0.1786\n",
      "Epoch 207/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.8553 - accuracy: 0.4827 - val_loss: 3.8346 - val_accuracy: 0.1667\n",
      "Epoch 208/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.8854 - accuracy: 0.4969 - val_loss: 3.8376 - val_accuracy: 0.1738\n",
      "Epoch 209/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.8907 - accuracy: 0.4867 - val_loss: 3.8331 - val_accuracy: 0.1738\n",
      "Epoch 210/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.8739 - accuracy: 0.4939 - val_loss: 3.8339 - val_accuracy: 0.1714\n",
      "Epoch 211/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.8667 - accuracy: 0.4980 - val_loss: 3.8349 - val_accuracy: 0.1786\n",
      "Epoch 212/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.8638 - accuracy: 0.5031 - val_loss: 3.8325 - val_accuracy: 0.1714\n",
      "Epoch 213/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.8474 - accuracy: 0.4888 - val_loss: 3.8233 - val_accuracy: 0.1833\n",
      "Epoch 214/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.8732 - accuracy: 0.4949 - val_loss: 3.8271 - val_accuracy: 0.1786\n",
      "Epoch 215/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.8431 - accuracy: 0.5204 - val_loss: 3.8219 - val_accuracy: 0.1738\n",
      "Epoch 216/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.8558 - accuracy: 0.4857 - val_loss: 3.8216 - val_accuracy: 0.1786\n",
      "Epoch 217/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.8117 - accuracy: 0.5092 - val_loss: 3.8209 - val_accuracy: 0.1833\n",
      "Epoch 218/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.8245 - accuracy: 0.5163 - val_loss: 3.8217 - val_accuracy: 0.1833\n",
      "Epoch 219/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.8360 - accuracy: 0.5102 - val_loss: 3.8159 - val_accuracy: 0.1738\n",
      "Epoch 220/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.8023 - accuracy: 0.5143 - val_loss: 3.8181 - val_accuracy: 0.1833\n",
      "Epoch 221/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.8119 - accuracy: 0.5245 - val_loss: 3.8183 - val_accuracy: 0.1690\n",
      "Epoch 222/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.8512 - accuracy: 0.5031 - val_loss: 3.8211 - val_accuracy: 0.1762\n",
      "Epoch 223/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.7914 - accuracy: 0.5265 - val_loss: 3.8167 - val_accuracy: 0.1762\n",
      "Epoch 224/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.7941 - accuracy: 0.5276 - val_loss: 3.8219 - val_accuracy: 0.1905\n",
      "Epoch 225/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.7792 - accuracy: 0.5388 - val_loss: 3.8216 - val_accuracy: 0.1881\n",
      "Epoch 226/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.8004 - accuracy: 0.5163 - val_loss: 3.8110 - val_accuracy: 0.1833\n",
      "Epoch 227/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.8009 - accuracy: 0.5163 - val_loss: 3.8200 - val_accuracy: 0.1833\n",
      "Epoch 228/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.7569 - accuracy: 0.5173 - val_loss: 3.8180 - val_accuracy: 0.1810\n",
      "Epoch 229/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.7879 - accuracy: 0.5306 - val_loss: 3.8184 - val_accuracy: 0.1786\n",
      "Epoch 230/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.7694 - accuracy: 0.5398 - val_loss: 3.8192 - val_accuracy: 0.1738\n",
      "Epoch 231/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.7720 - accuracy: 0.5296 - val_loss: 3.8188 - val_accuracy: 0.1857\n",
      "Epoch 232/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.7870 - accuracy: 0.5163 - val_loss: 3.8218 - val_accuracy: 0.1762\n",
      "Epoch 233/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.7503 - accuracy: 0.5490 - val_loss: 3.8160 - val_accuracy: 0.1833\n",
      "Epoch 234/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.7581 - accuracy: 0.5276 - val_loss: 3.8153 - val_accuracy: 0.1786\n",
      "Epoch 235/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.7432 - accuracy: 0.5347 - val_loss: 3.8139 - val_accuracy: 0.1881\n",
      "Epoch 236/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.7433 - accuracy: 0.5357 - val_loss: 3.8255 - val_accuracy: 0.1786\n",
      "Epoch 237/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.7324 - accuracy: 0.5469 - val_loss: 3.8105 - val_accuracy: 0.1762\n",
      "Epoch 238/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.7742 - accuracy: 0.5296 - val_loss: 3.8152 - val_accuracy: 0.1905\n",
      "Epoch 239/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.7402 - accuracy: 0.5173 - val_loss: 3.8071 - val_accuracy: 0.1857\n",
      "Epoch 240/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.7264 - accuracy: 0.5541 - val_loss: 3.8070 - val_accuracy: 0.1929\n",
      "Epoch 241/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.7298 - accuracy: 0.5480 - val_loss: 3.8085 - val_accuracy: 0.1786\n",
      "Epoch 242/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.7349 - accuracy: 0.5357 - val_loss: 3.8106 - val_accuracy: 0.1786\n",
      "Epoch 243/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.7194 - accuracy: 0.5418 - val_loss: 3.8049 - val_accuracy: 0.1905\n",
      "Epoch 244/1000\n",
      "980/980 [==============================] - 9s 9ms/step - loss: 2.7517 - accuracy: 0.5480 - val_loss: 3.8062 - val_accuracy: 0.1810\n",
      "Epoch 245/1000\n",
      "980/980 [==============================] - 10s 10ms/step - loss: 2.7029 - accuracy: 0.5408 - val_loss: 3.8038 - val_accuracy: 0.1786\n",
      "Epoch 246/1000\n",
      "980/980 [==============================] - 8s 9ms/step - loss: 2.6800 - accuracy: 0.5694 - val_loss: 3.8064 - val_accuracy: 0.1810\n",
      "Epoch 247/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.7104 - accuracy: 0.5265 - val_loss: 3.8054 - val_accuracy: 0.1810\n",
      "Epoch 248/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.7320 - accuracy: 0.5245 - val_loss: 3.8011 - val_accuracy: 0.1833\n",
      "Epoch 249/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.6830 - accuracy: 0.5653 - val_loss: 3.8003 - val_accuracy: 0.1905\n",
      "Epoch 250/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.7049 - accuracy: 0.5622 - val_loss: 3.8026 - val_accuracy: 0.1738\n",
      "Epoch 251/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.6735 - accuracy: 0.5755 - val_loss: 3.8041 - val_accuracy: 0.1833\n",
      "Epoch 252/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.6968 - accuracy: 0.5561 - val_loss: 3.8000 - val_accuracy: 0.1810\n",
      "Epoch 253/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.6693 - accuracy: 0.5541 - val_loss: 3.8032 - val_accuracy: 0.1762\n",
      "Epoch 254/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.6566 - accuracy: 0.5684 - val_loss: 3.8049 - val_accuracy: 0.1738\n",
      "Epoch 255/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.6768 - accuracy: 0.5459 - val_loss: 3.8007 - val_accuracy: 0.1762\n",
      "Epoch 256/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "980/980 [==============================] - 8s 8ms/step - loss: 2.7102 - accuracy: 0.5327 - val_loss: 3.7992 - val_accuracy: 0.1738\n",
      "Epoch 257/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.7163 - accuracy: 0.5245 - val_loss: 3.7980 - val_accuracy: 0.1810\n",
      "Epoch 258/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.6259 - accuracy: 0.5816 - val_loss: 3.8053 - val_accuracy: 0.1810\n",
      "Epoch 259/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.6723 - accuracy: 0.5490 - val_loss: 3.8001 - val_accuracy: 0.1786\n",
      "Epoch 260/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.6157 - accuracy: 0.5888 - val_loss: 3.7950 - val_accuracy: 0.1810\n",
      "Epoch 261/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.6488 - accuracy: 0.5786 - val_loss: 3.7977 - val_accuracy: 0.1762\n",
      "Epoch 262/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.6746 - accuracy: 0.5633 - val_loss: 3.7980 - val_accuracy: 0.1738\n",
      "Epoch 263/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.6478 - accuracy: 0.5694 - val_loss: 3.7972 - val_accuracy: 0.1857\n",
      "Epoch 264/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.6209 - accuracy: 0.5776 - val_loss: 3.7968 - val_accuracy: 0.1810\n",
      "Epoch 265/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.6080 - accuracy: 0.5888 - val_loss: 3.7925 - val_accuracy: 0.1810\n",
      "Epoch 266/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.6320 - accuracy: 0.5724 - val_loss: 3.7909 - val_accuracy: 0.1810\n",
      "Epoch 267/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.6398 - accuracy: 0.5806 - val_loss: 3.7906 - val_accuracy: 0.1810\n",
      "Epoch 268/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.6178 - accuracy: 0.5969 - val_loss: 3.7930 - val_accuracy: 0.1738\n",
      "Epoch 269/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.6030 - accuracy: 0.5776 - val_loss: 3.7944 - val_accuracy: 0.1738\n",
      "Epoch 270/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.5805 - accuracy: 0.6133 - val_loss: 3.7853 - val_accuracy: 0.1810\n",
      "Epoch 271/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.6375 - accuracy: 0.5673 - val_loss: 3.7917 - val_accuracy: 0.1714\n",
      "Epoch 272/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.6292 - accuracy: 0.5653 - val_loss: 3.7929 - val_accuracy: 0.1833\n",
      "Epoch 273/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.5634 - accuracy: 0.5980 - val_loss: 3.7872 - val_accuracy: 0.1810\n",
      "Epoch 274/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.5815 - accuracy: 0.6041 - val_loss: 3.7864 - val_accuracy: 0.1786\n",
      "Epoch 275/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.6073 - accuracy: 0.5939 - val_loss: 3.7878 - val_accuracy: 0.1810\n",
      "Epoch 276/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.5873 - accuracy: 0.5816 - val_loss: 3.7831 - val_accuracy: 0.1714\n",
      "Epoch 277/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.5739 - accuracy: 0.5908 - val_loss: 3.7834 - val_accuracy: 0.1810\n",
      "Epoch 278/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.5846 - accuracy: 0.5888 - val_loss: 3.7833 - val_accuracy: 0.1857\n",
      "Epoch 279/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.5887 - accuracy: 0.5867 - val_loss: 3.7778 - val_accuracy: 0.1786\n",
      "Epoch 280/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.5255 - accuracy: 0.6122 - val_loss: 3.7840 - val_accuracy: 0.1714\n",
      "Epoch 281/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.5887 - accuracy: 0.5939 - val_loss: 3.7770 - val_accuracy: 0.1690\n",
      "Epoch 282/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.5621 - accuracy: 0.6020 - val_loss: 3.7807 - val_accuracy: 0.1810\n",
      "Epoch 283/1000\n",
      "980/980 [==============================] - 8s 9ms/step - loss: 2.5524 - accuracy: 0.5878 - val_loss: 3.7770 - val_accuracy: 0.1786\n",
      "Epoch 284/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.5844 - accuracy: 0.6000 - val_loss: 3.7802 - val_accuracy: 0.1857\n",
      "Epoch 285/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.5559 - accuracy: 0.6051 - val_loss: 3.7798 - val_accuracy: 0.1929\n",
      "Epoch 286/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.5524 - accuracy: 0.5888 - val_loss: 3.7764 - val_accuracy: 0.1810\n",
      "Epoch 287/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.5685 - accuracy: 0.5796 - val_loss: 3.7790 - val_accuracy: 0.1833\n",
      "Epoch 288/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.5346 - accuracy: 0.6061 - val_loss: 3.7794 - val_accuracy: 0.1738\n",
      "Epoch 289/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.5708 - accuracy: 0.5857 - val_loss: 3.7790 - val_accuracy: 0.1786\n",
      "Epoch 290/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.5061 - accuracy: 0.6163 - val_loss: 3.7742 - val_accuracy: 0.1833\n",
      "Epoch 291/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.5114 - accuracy: 0.6051 - val_loss: 3.7714 - val_accuracy: 0.1810\n",
      "Epoch 292/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.5247 - accuracy: 0.5949 - val_loss: 3.7749 - val_accuracy: 0.1857\n",
      "Epoch 293/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.5220 - accuracy: 0.6235 - val_loss: 3.7724 - val_accuracy: 0.1857\n",
      "Epoch 294/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.5254 - accuracy: 0.6061 - val_loss: 3.7674 - val_accuracy: 0.1929\n",
      "Epoch 295/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.5342 - accuracy: 0.6020 - val_loss: 3.7719 - val_accuracy: 0.1857\n",
      "Epoch 296/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.5264 - accuracy: 0.5980 - val_loss: 3.7757 - val_accuracy: 0.1738\n",
      "Epoch 297/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.5330 - accuracy: 0.5959 - val_loss: 3.7702 - val_accuracy: 0.1857\n",
      "Epoch 298/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.5263 - accuracy: 0.5939 - val_loss: 3.7684 - val_accuracy: 0.1786\n",
      "Epoch 299/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.5047 - accuracy: 0.6031 - val_loss: 3.7700 - val_accuracy: 0.1833\n",
      "Epoch 300/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.4800 - accuracy: 0.6327 - val_loss: 3.7731 - val_accuracy: 0.1810\n",
      "Epoch 301/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.5147 - accuracy: 0.6031 - val_loss: 3.7714 - val_accuracy: 0.1881\n",
      "Epoch 302/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.5156 - accuracy: 0.6010 - val_loss: 3.7727 - val_accuracy: 0.1810\n",
      "Epoch 303/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.5205 - accuracy: 0.6082 - val_loss: 3.7698 - val_accuracy: 0.1833\n",
      "Epoch 304/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.4820 - accuracy: 0.6327 - val_loss: 3.7682 - val_accuracy: 0.1881\n",
      "Epoch 305/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.4960 - accuracy: 0.6082 - val_loss: 3.7750 - val_accuracy: 0.1810\n",
      "Epoch 306/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.4736 - accuracy: 0.6286 - val_loss: 3.7649 - val_accuracy: 0.1857\n",
      "Epoch 307/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.4743 - accuracy: 0.6224 - val_loss: 3.7680 - val_accuracy: 0.1881\n",
      "Epoch 308/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.5158 - accuracy: 0.5980 - val_loss: 3.7669 - val_accuracy: 0.1786\n",
      "Epoch 309/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.4397 - accuracy: 0.6337 - val_loss: 3.7626 - val_accuracy: 0.1881\n",
      "Epoch 310/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.4761 - accuracy: 0.6347 - val_loss: 3.7670 - val_accuracy: 0.1738\n",
      "Epoch 311/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.4509 - accuracy: 0.6469 - val_loss: 3.7606 - val_accuracy: 0.1786\n",
      "Epoch 312/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.4690 - accuracy: 0.6265 - val_loss: 3.7633 - val_accuracy: 0.1762\n",
      "Epoch 313/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.4428 - accuracy: 0.6367 - val_loss: 3.7679 - val_accuracy: 0.1738\n",
      "Epoch 314/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.4380 - accuracy: 0.6296 - val_loss: 3.7667 - val_accuracy: 0.1714\n",
      "Epoch 315/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.4424 - accuracy: 0.6347 - val_loss: 3.7633 - val_accuracy: 0.1786\n",
      "Epoch 316/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.4503 - accuracy: 0.6224 - val_loss: 3.7616 - val_accuracy: 0.1881\n",
      "Epoch 317/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.4070 - accuracy: 0.6633 - val_loss: 3.7626 - val_accuracy: 0.1786\n",
      "Epoch 318/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.4418 - accuracy: 0.6204 - val_loss: 3.7606 - val_accuracy: 0.1690\n",
      "Epoch 319/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.4818 - accuracy: 0.6214 - val_loss: 3.7618 - val_accuracy: 0.1667\n",
      "Epoch 320/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.4194 - accuracy: 0.6398 - val_loss: 3.7538 - val_accuracy: 0.1762\n",
      "Epoch 321/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.4396 - accuracy: 0.6439 - val_loss: 3.7538 - val_accuracy: 0.1881\n",
      "Epoch 322/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.4326 - accuracy: 0.6459 - val_loss: 3.7644 - val_accuracy: 0.1786\n",
      "Epoch 323/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.4122 - accuracy: 0.6592 - val_loss: 3.7448 - val_accuracy: 0.1833\n",
      "Epoch 324/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.4525 - accuracy: 0.6306 - val_loss: 3.7505 - val_accuracy: 0.1762\n",
      "Epoch 325/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.4576 - accuracy: 0.6143 - val_loss: 3.7465 - val_accuracy: 0.1905\n",
      "Epoch 326/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.4173 - accuracy: 0.6327 - val_loss: 3.7427 - val_accuracy: 0.1833\n",
      "Epoch 327/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.4006 - accuracy: 0.6224 - val_loss: 3.7402 - val_accuracy: 0.1881\n",
      "Epoch 328/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.4201 - accuracy: 0.6398 - val_loss: 3.7456 - val_accuracy: 0.1881\n",
      "Epoch 329/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.3799 - accuracy: 0.6480 - val_loss: 3.7405 - val_accuracy: 0.1857\n",
      "Epoch 330/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.3940 - accuracy: 0.6429 - val_loss: 3.7431 - val_accuracy: 0.1857\n",
      "Epoch 331/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.3745 - accuracy: 0.6622 - val_loss: 3.7420 - val_accuracy: 0.1857\n",
      "Epoch 332/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.3287 - accuracy: 0.6786 - val_loss: 3.7512 - val_accuracy: 0.1857\n",
      "Epoch 333/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.4049 - accuracy: 0.6500 - val_loss: 3.7491 - val_accuracy: 0.1929\n",
      "Epoch 334/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.4047 - accuracy: 0.6214 - val_loss: 3.7542 - val_accuracy: 0.1762\n",
      "Epoch 335/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.3936 - accuracy: 0.6541 - val_loss: 3.7473 - val_accuracy: 0.1833\n",
      "Epoch 336/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.3847 - accuracy: 0.6469 - val_loss: 3.7431 - val_accuracy: 0.1929\n",
      "Epoch 337/1000\n",
      "980/980 [==============================] - 9s 9ms/step - loss: 2.4009 - accuracy: 0.6367 - val_loss: 3.7490 - val_accuracy: 0.1833\n",
      "Epoch 338/1000\n",
      "980/980 [==============================] - 8s 9ms/step - loss: 2.3720 - accuracy: 0.6459 - val_loss: 3.7480 - val_accuracy: 0.1881\n",
      "Epoch 339/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.3638 - accuracy: 0.6582 - val_loss: 3.7435 - val_accuracy: 0.1833\n",
      "Epoch 340/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.3461 - accuracy: 0.6724 - val_loss: 3.7493 - val_accuracy: 0.1857\n",
      "Epoch 341/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.3854 - accuracy: 0.6357 - val_loss: 3.7495 - val_accuracy: 0.1857\n",
      "Epoch 342/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.3577 - accuracy: 0.6602 - val_loss: 3.7497 - val_accuracy: 0.1810\n",
      "Epoch 343/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.3393 - accuracy: 0.6796 - val_loss: 3.7424 - val_accuracy: 0.1952\n",
      "Epoch 344/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.3813 - accuracy: 0.6561 - val_loss: 3.7503 - val_accuracy: 0.1786\n",
      "Epoch 345/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.3340 - accuracy: 0.6776 - val_loss: 3.7450 - val_accuracy: 0.1833\n",
      "Epoch 346/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.3435 - accuracy: 0.6571 - val_loss: 3.7475 - val_accuracy: 0.1810\n",
      "Epoch 347/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.3197 - accuracy: 0.6837 - val_loss: 3.7418 - val_accuracy: 0.1786\n",
      "Epoch 348/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.3471 - accuracy: 0.6510 - val_loss: 3.7409 - val_accuracy: 0.1810\n",
      "Epoch 349/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.3876 - accuracy: 0.6398 - val_loss: 3.7384 - val_accuracy: 0.1786\n",
      "Epoch 350/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.3565 - accuracy: 0.6500 - val_loss: 3.7367 - val_accuracy: 0.1738\n",
      "Epoch 351/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.3499 - accuracy: 0.6643 - val_loss: 3.7376 - val_accuracy: 0.1929\n",
      "Epoch 352/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.2857 - accuracy: 0.6878 - val_loss: 3.7330 - val_accuracy: 0.1786\n",
      "Epoch 353/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.3521 - accuracy: 0.6500 - val_loss: 3.7359 - val_accuracy: 0.1714\n",
      "Epoch 354/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.3266 - accuracy: 0.6796 - val_loss: 3.7359 - val_accuracy: 0.1762\n",
      "Epoch 355/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.3196 - accuracy: 0.6694 - val_loss: 3.7422 - val_accuracy: 0.1833\n",
      "Epoch 356/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.3249 - accuracy: 0.6786 - val_loss: 3.7417 - val_accuracy: 0.1857\n",
      "Epoch 357/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.3332 - accuracy: 0.6510 - val_loss: 3.7330 - val_accuracy: 0.1786\n",
      "Epoch 358/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.3040 - accuracy: 0.6776 - val_loss: 3.7391 - val_accuracy: 0.1786\n",
      "Epoch 359/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.3081 - accuracy: 0.6592 - val_loss: 3.7325 - val_accuracy: 0.1905\n",
      "Epoch 360/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.3036 - accuracy: 0.6867 - val_loss: 3.7327 - val_accuracy: 0.1857\n",
      "Epoch 361/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.3304 - accuracy: 0.6622 - val_loss: 3.7336 - val_accuracy: 0.1833\n",
      "Epoch 362/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.3195 - accuracy: 0.6765 - val_loss: 3.7364 - val_accuracy: 0.1786\n",
      "Epoch 363/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.2962 - accuracy: 0.6867 - val_loss: 3.7317 - val_accuracy: 0.1810\n",
      "Epoch 364/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.2908 - accuracy: 0.6969 - val_loss: 3.7284 - val_accuracy: 0.1833\n",
      "Epoch 365/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.2781 - accuracy: 0.6857 - val_loss: 3.7251 - val_accuracy: 0.1881\n",
      "Epoch 366/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.2896 - accuracy: 0.6622 - val_loss: 3.7267 - val_accuracy: 0.1881\n",
      "Epoch 367/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.3201 - accuracy: 0.6724 - val_loss: 3.7310 - val_accuracy: 0.1786\n",
      "Epoch 368/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "980/980 [==============================] - 8s 8ms/step - loss: 2.2654 - accuracy: 0.6959 - val_loss: 3.7245 - val_accuracy: 0.1810\n",
      "Epoch 369/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.2486 - accuracy: 0.6898 - val_loss: 3.7314 - val_accuracy: 0.1738\n",
      "Epoch 370/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.2466 - accuracy: 0.7041 - val_loss: 3.7359 - val_accuracy: 0.1762\n",
      "Epoch 371/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.2848 - accuracy: 0.6796 - val_loss: 3.7332 - val_accuracy: 0.1786\n",
      "Epoch 372/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.2879 - accuracy: 0.6837 - val_loss: 3.7374 - val_accuracy: 0.1810\n",
      "Epoch 373/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.2761 - accuracy: 0.6980 - val_loss: 3.7362 - val_accuracy: 0.1786\n",
      "Epoch 374/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.2802 - accuracy: 0.6806 - val_loss: 3.7354 - val_accuracy: 0.1786\n",
      "Epoch 375/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.2373 - accuracy: 0.7031 - val_loss: 3.7397 - val_accuracy: 0.1786\n",
      "Epoch 376/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.2270 - accuracy: 0.6969 - val_loss: 3.7312 - val_accuracy: 0.1786\n",
      "Epoch 377/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.2355 - accuracy: 0.7153 - val_loss: 3.7291 - val_accuracy: 0.1714\n",
      "Epoch 378/1000\n",
      "980/980 [==============================] - 7s 8ms/step - loss: 2.2404 - accuracy: 0.7071 - val_loss: 3.7340 - val_accuracy: 0.1857\n",
      "Epoch 379/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.2656 - accuracy: 0.7020 - val_loss: 3.7273 - val_accuracy: 0.1881\n",
      "Epoch 380/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.2627 - accuracy: 0.6908 - val_loss: 3.7295 - val_accuracy: 0.1762\n",
      "Epoch 381/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.2867 - accuracy: 0.6735 - val_loss: 3.7217 - val_accuracy: 0.1833\n",
      "Epoch 382/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.2051 - accuracy: 0.7235 - val_loss: 3.7292 - val_accuracy: 0.1762\n",
      "Epoch 383/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.2026 - accuracy: 0.6929 - val_loss: 3.7292 - val_accuracy: 0.1810\n",
      "Epoch 384/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.2645 - accuracy: 0.6857 - val_loss: 3.7255 - val_accuracy: 0.1643\n",
      "Epoch 385/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.2226 - accuracy: 0.7102 - val_loss: 3.7242 - val_accuracy: 0.1810\n",
      "Epoch 386/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.2131 - accuracy: 0.7010 - val_loss: 3.7261 - val_accuracy: 0.1738\n",
      "Epoch 387/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.1876 - accuracy: 0.7214 - val_loss: 3.7239 - val_accuracy: 0.1786\n",
      "Epoch 388/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.2325 - accuracy: 0.7092 - val_loss: 3.7193 - val_accuracy: 0.1762\n",
      "Epoch 389/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.2490 - accuracy: 0.6816 - val_loss: 3.7242 - val_accuracy: 0.1786\n",
      "Epoch 390/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.2207 - accuracy: 0.7031 - val_loss: 3.7231 - val_accuracy: 0.1786\n",
      "Epoch 391/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.2253 - accuracy: 0.6857 - val_loss: 3.7270 - val_accuracy: 0.1786\n",
      "Epoch 392/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.1729 - accuracy: 0.7367 - val_loss: 3.7221 - val_accuracy: 0.1786\n",
      "Epoch 393/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.1913 - accuracy: 0.7235 - val_loss: 3.7191 - val_accuracy: 0.1786\n",
      "Epoch 394/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.2127 - accuracy: 0.7041 - val_loss: 3.7197 - val_accuracy: 0.1857\n",
      "Epoch 395/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.1726 - accuracy: 0.7286 - val_loss: 3.7215 - val_accuracy: 0.1762\n",
      "Epoch 396/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.2050 - accuracy: 0.6969 - val_loss: 3.7252 - val_accuracy: 0.1833\n",
      "Epoch 397/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.1843 - accuracy: 0.7265 - val_loss: 3.7187 - val_accuracy: 0.1857\n",
      "Epoch 398/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.1996 - accuracy: 0.6990 - val_loss: 3.7197 - val_accuracy: 0.1786\n",
      "Epoch 399/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.1777 - accuracy: 0.7296 - val_loss: 3.7200 - val_accuracy: 0.1714\n",
      "Epoch 400/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.1346 - accuracy: 0.7316 - val_loss: 3.7202 - val_accuracy: 0.1833\n",
      "Epoch 401/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.1825 - accuracy: 0.7071 - val_loss: 3.7118 - val_accuracy: 0.1929\n",
      "Epoch 402/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.1872 - accuracy: 0.7102 - val_loss: 3.7158 - val_accuracy: 0.1857\n",
      "Epoch 403/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.1774 - accuracy: 0.7255 - val_loss: 3.7216 - val_accuracy: 0.1690\n",
      "Epoch 404/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.1631 - accuracy: 0.7173 - val_loss: 3.7140 - val_accuracy: 0.1810\n",
      "Epoch 405/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.1419 - accuracy: 0.7122 - val_loss: 3.7232 - val_accuracy: 0.1786\n",
      "Epoch 406/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.1751 - accuracy: 0.7204 - val_loss: 3.7218 - val_accuracy: 0.1810\n",
      "Epoch 407/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.1724 - accuracy: 0.7388 - val_loss: 3.7162 - val_accuracy: 0.1762\n",
      "Epoch 408/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.1608 - accuracy: 0.7296 - val_loss: 3.7114 - val_accuracy: 0.1833\n",
      "Epoch 409/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.1348 - accuracy: 0.7347 - val_loss: 3.7145 - val_accuracy: 0.1714\n",
      "Epoch 410/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.1805 - accuracy: 0.7071 - val_loss: 3.7169 - val_accuracy: 0.1738\n",
      "Epoch 411/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.1408 - accuracy: 0.7327 - val_loss: 3.7184 - val_accuracy: 0.1786\n",
      "Epoch 412/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.1645 - accuracy: 0.7286 - val_loss: 3.7148 - val_accuracy: 0.1786\n",
      "Epoch 413/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.1455 - accuracy: 0.7429 - val_loss: 3.7179 - val_accuracy: 0.1762\n",
      "Epoch 414/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.1802 - accuracy: 0.7061 - val_loss: 3.7131 - val_accuracy: 0.1762\n",
      "Epoch 415/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.1438 - accuracy: 0.7276 - val_loss: 3.7105 - val_accuracy: 0.1762\n",
      "Epoch 416/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.1349 - accuracy: 0.7296 - val_loss: 3.7064 - val_accuracy: 0.1762\n",
      "Epoch 417/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.1404 - accuracy: 0.7214 - val_loss: 3.7149 - val_accuracy: 0.1786\n",
      "Epoch 418/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.1350 - accuracy: 0.7327 - val_loss: 3.7112 - val_accuracy: 0.1714\n",
      "Epoch 419/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.1208 - accuracy: 0.7357 - val_loss: 3.7151 - val_accuracy: 0.1714\n",
      "Epoch 420/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.0976 - accuracy: 0.7612 - val_loss: 3.7106 - val_accuracy: 0.1833\n",
      "Epoch 421/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.1240 - accuracy: 0.7582 - val_loss: 3.7065 - val_accuracy: 0.1786\n",
      "Epoch 422/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.1107 - accuracy: 0.7204 - val_loss: 3.7130 - val_accuracy: 0.1762\n",
      "Epoch 423/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.0979 - accuracy: 0.7541 - val_loss: 3.7086 - val_accuracy: 0.1738\n",
      "Epoch 424/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.1087 - accuracy: 0.7388 - val_loss: 3.7080 - val_accuracy: 0.1714\n",
      "Epoch 425/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.0834 - accuracy: 0.7520 - val_loss: 3.7162 - val_accuracy: 0.1738\n",
      "Epoch 426/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.1240 - accuracy: 0.7296 - val_loss: 3.7118 - val_accuracy: 0.1857\n",
      "Epoch 427/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.0819 - accuracy: 0.7418 - val_loss: 3.7094 - val_accuracy: 0.1810\n",
      "Epoch 428/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.0843 - accuracy: 0.7449 - val_loss: 3.7102 - val_accuracy: 0.1738\n",
      "Epoch 429/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.1076 - accuracy: 0.7408 - val_loss: 3.7116 - val_accuracy: 0.1786\n",
      "Epoch 430/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.1219 - accuracy: 0.7286 - val_loss: 3.7120 - val_accuracy: 0.1810\n",
      "Epoch 431/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.1018 - accuracy: 0.7520 - val_loss: 3.7034 - val_accuracy: 0.1810\n",
      "Epoch 432/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.0670 - accuracy: 0.7582 - val_loss: 3.6964 - val_accuracy: 0.1857\n",
      "Epoch 433/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.0736 - accuracy: 0.7643 - val_loss: 3.7042 - val_accuracy: 0.1786\n",
      "Epoch 434/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.0545 - accuracy: 0.7490 - val_loss: 3.7031 - val_accuracy: 0.1738\n",
      "Epoch 435/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.0921 - accuracy: 0.7286 - val_loss: 3.7157 - val_accuracy: 0.1881\n",
      "Epoch 436/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.0610 - accuracy: 0.7582 - val_loss: 3.7000 - val_accuracy: 0.1643\n",
      "Epoch 437/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.1097 - accuracy: 0.7418 - val_loss: 3.6952 - val_accuracy: 0.1905\n",
      "Epoch 438/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.0623 - accuracy: 0.7551 - val_loss: 3.6994 - val_accuracy: 0.1762\n",
      "Epoch 439/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.0608 - accuracy: 0.7367 - val_loss: 3.6923 - val_accuracy: 0.1738\n",
      "Epoch 440/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.0682 - accuracy: 0.7388 - val_loss: 3.7027 - val_accuracy: 0.1714\n",
      "Epoch 441/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.0791 - accuracy: 0.7459 - val_loss: 3.6994 - val_accuracy: 0.1786\n",
      "Epoch 442/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.0723 - accuracy: 0.7378 - val_loss: 3.7049 - val_accuracy: 0.1810\n",
      "Epoch 443/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.0682 - accuracy: 0.7510 - val_loss: 3.6935 - val_accuracy: 0.1714\n",
      "Epoch 444/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.0471 - accuracy: 0.7643 - val_loss: 3.7015 - val_accuracy: 0.1857\n",
      "Epoch 445/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.0799 - accuracy: 0.7316 - val_loss: 3.7059 - val_accuracy: 0.1786\n",
      "Epoch 446/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.0375 - accuracy: 0.7622 - val_loss: 3.6891 - val_accuracy: 0.1762\n",
      "Epoch 447/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.0877 - accuracy: 0.7306 - val_loss: 3.7069 - val_accuracy: 0.1738\n",
      "Epoch 448/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.0533 - accuracy: 0.7541 - val_loss: 3.6997 - val_accuracy: 0.1762\n",
      "Epoch 449/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.0526 - accuracy: 0.7439 - val_loss: 3.6967 - val_accuracy: 0.1714\n",
      "Epoch 450/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.0540 - accuracy: 0.7490 - val_loss: 3.6952 - val_accuracy: 0.1643\n",
      "Epoch 451/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.0611 - accuracy: 0.7490 - val_loss: 3.6992 - val_accuracy: 0.1667\n",
      "Epoch 452/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.0354 - accuracy: 0.7520 - val_loss: 3.6937 - val_accuracy: 0.1762\n",
      "Epoch 453/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.0339 - accuracy: 0.7673 - val_loss: 3.6824 - val_accuracy: 0.1762\n",
      "Epoch 454/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.0302 - accuracy: 0.7776 - val_loss: 3.6834 - val_accuracy: 0.1810\n",
      "Epoch 455/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.0392 - accuracy: 0.7541 - val_loss: 3.6999 - val_accuracy: 0.1690\n",
      "Epoch 456/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.0439 - accuracy: 0.7663 - val_loss: 3.7032 - val_accuracy: 0.1833\n",
      "Epoch 457/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.0095 - accuracy: 0.7806 - val_loss: 3.6934 - val_accuracy: 0.1738\n",
      "Epoch 458/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.0416 - accuracy: 0.7582 - val_loss: 3.6990 - val_accuracy: 0.1786\n",
      "Epoch 459/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.0707 - accuracy: 0.7510 - val_loss: 3.6991 - val_accuracy: 0.1810\n",
      "Epoch 460/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.0211 - accuracy: 0.7459 - val_loss: 3.7019 - val_accuracy: 0.1690\n",
      "Epoch 461/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.0357 - accuracy: 0.7633 - val_loss: 3.6896 - val_accuracy: 0.1786\n",
      "Epoch 462/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.9736 - accuracy: 0.7857 - val_loss: 3.6992 - val_accuracy: 0.1714\n",
      "Epoch 463/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.0279 - accuracy: 0.7500 - val_loss: 3.6889 - val_accuracy: 0.1786\n",
      "Epoch 464/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.9793 - accuracy: 0.7827 - val_loss: 3.6956 - val_accuracy: 0.1643\n",
      "Epoch 465/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.9655 - accuracy: 0.7827 - val_loss: 3.6870 - val_accuracy: 0.1786\n",
      "Epoch 466/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.0240 - accuracy: 0.7541 - val_loss: 3.6844 - val_accuracy: 0.1714\n",
      "Epoch 467/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.9873 - accuracy: 0.7643 - val_loss: 3.6882 - val_accuracy: 0.1786\n",
      "Epoch 468/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.0059 - accuracy: 0.7673 - val_loss: 3.6871 - val_accuracy: 0.1690\n",
      "Epoch 469/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.0047 - accuracy: 0.7571 - val_loss: 3.6904 - val_accuracy: 0.1857\n",
      "Epoch 470/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.9785 - accuracy: 0.7571 - val_loss: 3.6925 - val_accuracy: 0.1762\n",
      "Epoch 471/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.9872 - accuracy: 0.7837 - val_loss: 3.6883 - val_accuracy: 0.1786\n",
      "Epoch 472/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.9880 - accuracy: 0.7633 - val_loss: 3.6902 - val_accuracy: 0.1738\n",
      "Epoch 473/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.9895 - accuracy: 0.7745 - val_loss: 3.6962 - val_accuracy: 0.1786\n",
      "Epoch 474/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.9927 - accuracy: 0.7592 - val_loss: 3.6909 - val_accuracy: 0.1738\n",
      "Epoch 475/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.9441 - accuracy: 0.7898 - val_loss: 3.6862 - val_accuracy: 0.1738\n",
      "Epoch 476/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 2.0000 - accuracy: 0.7847 - val_loss: 3.6853 - val_accuracy: 0.1786\n",
      "Epoch 477/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.9645 - accuracy: 0.7827 - val_loss: 3.6901 - val_accuracy: 0.1786\n",
      "Epoch 478/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.9677 - accuracy: 0.7633 - val_loss: 3.6892 - val_accuracy: 0.1810\n",
      "Epoch 479/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.9895 - accuracy: 0.7520 - val_loss: 3.6968 - val_accuracy: 0.1786\n",
      "Epoch 480/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "980/980 [==============================] - 8s 8ms/step - loss: 1.9629 - accuracy: 0.7827 - val_loss: 3.6909 - val_accuracy: 0.1833\n",
      "Epoch 481/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.9976 - accuracy: 0.7571 - val_loss: 3.6910 - val_accuracy: 0.1690\n",
      "Epoch 482/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.9775 - accuracy: 0.7704 - val_loss: 3.6806 - val_accuracy: 0.1810\n",
      "Epoch 483/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.9875 - accuracy: 0.7776 - val_loss: 3.6906 - val_accuracy: 0.1690\n",
      "Epoch 484/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.9509 - accuracy: 0.7724 - val_loss: 3.6887 - val_accuracy: 0.1738\n",
      "Epoch 485/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.9664 - accuracy: 0.8041 - val_loss: 3.6922 - val_accuracy: 0.1738\n",
      "Epoch 486/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.9324 - accuracy: 0.8020 - val_loss: 3.6843 - val_accuracy: 0.1786\n",
      "Epoch 487/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.9378 - accuracy: 0.7939 - val_loss: 3.6894 - val_accuracy: 0.1810\n",
      "Epoch 488/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.9908 - accuracy: 0.7571 - val_loss: 3.6829 - val_accuracy: 0.1714\n",
      "Epoch 489/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.9383 - accuracy: 0.7959 - val_loss: 3.6841 - val_accuracy: 0.1786\n",
      "Epoch 490/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.9584 - accuracy: 0.7745 - val_loss: 3.6893 - val_accuracy: 0.1857\n",
      "Epoch 491/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.9577 - accuracy: 0.7663 - val_loss: 3.6800 - val_accuracy: 0.1905\n",
      "Epoch 492/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.9340 - accuracy: 0.7929 - val_loss: 3.6893 - val_accuracy: 0.1762\n",
      "Epoch 493/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.9397 - accuracy: 0.7888 - val_loss: 3.6857 - val_accuracy: 0.1857\n",
      "Epoch 494/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.9178 - accuracy: 0.7949 - val_loss: 3.6922 - val_accuracy: 0.1667\n",
      "Epoch 495/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.9880 - accuracy: 0.7714 - val_loss: 3.6869 - val_accuracy: 0.1738\n",
      "Epoch 496/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.9241 - accuracy: 0.7959 - val_loss: 3.6793 - val_accuracy: 0.1762\n",
      "Epoch 497/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.9184 - accuracy: 0.7949 - val_loss: 3.6859 - val_accuracy: 0.1810\n",
      "Epoch 498/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.9023 - accuracy: 0.8041 - val_loss: 3.6707 - val_accuracy: 0.1810\n",
      "Epoch 499/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.9350 - accuracy: 0.7867 - val_loss: 3.6755 - val_accuracy: 0.1810\n",
      "Epoch 500/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.9469 - accuracy: 0.7878 - val_loss: 3.6782 - val_accuracy: 0.1762\n",
      "Epoch 501/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.9095 - accuracy: 0.8051 - val_loss: 3.6808 - val_accuracy: 0.1762\n",
      "Epoch 502/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.9359 - accuracy: 0.7827 - val_loss: 3.6897 - val_accuracy: 0.1738\n",
      "Epoch 503/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.9331 - accuracy: 0.7755 - val_loss: 3.6854 - val_accuracy: 0.1786\n",
      "Epoch 504/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.9271 - accuracy: 0.7806 - val_loss: 3.6860 - val_accuracy: 0.1786\n",
      "Epoch 505/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.9488 - accuracy: 0.7735 - val_loss: 3.6724 - val_accuracy: 0.1786\n",
      "Epoch 506/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.8887 - accuracy: 0.8071 - val_loss: 3.6847 - val_accuracy: 0.1810\n",
      "Epoch 507/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.8985 - accuracy: 0.7969 - val_loss: 3.6742 - val_accuracy: 0.1762\n",
      "Epoch 508/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.9274 - accuracy: 0.7704 - val_loss: 3.6819 - val_accuracy: 0.1810\n",
      "Epoch 509/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.9287 - accuracy: 0.7755 - val_loss: 3.6807 - val_accuracy: 0.1714\n",
      "Epoch 510/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.8922 - accuracy: 0.7867 - val_loss: 3.6785 - val_accuracy: 0.1762\n",
      "Epoch 511/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.9193 - accuracy: 0.7704 - val_loss: 3.6736 - val_accuracy: 0.1810\n",
      "Epoch 512/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.8784 - accuracy: 0.8051 - val_loss: 3.6740 - val_accuracy: 0.1810\n",
      "Epoch 513/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.8889 - accuracy: 0.8041 - val_loss: 3.6771 - val_accuracy: 0.1762\n",
      "Epoch 514/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.9022 - accuracy: 0.7990 - val_loss: 3.6738 - val_accuracy: 0.1786\n",
      "Epoch 515/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.8694 - accuracy: 0.8010 - val_loss: 3.6760 - val_accuracy: 0.1762\n",
      "Epoch 516/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.9102 - accuracy: 0.7898 - val_loss: 3.6775 - val_accuracy: 0.1714\n",
      "Epoch 517/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.8943 - accuracy: 0.7969 - val_loss: 3.6810 - val_accuracy: 0.1690\n",
      "Epoch 518/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.8957 - accuracy: 0.7908 - val_loss: 3.6765 - val_accuracy: 0.1762\n",
      "Epoch 519/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.8626 - accuracy: 0.8204 - val_loss: 3.6713 - val_accuracy: 0.1762\n",
      "Epoch 520/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.8504 - accuracy: 0.8316 - val_loss: 3.6794 - val_accuracy: 0.1738\n",
      "Epoch 521/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.8807 - accuracy: 0.8000 - val_loss: 3.6715 - val_accuracy: 0.1738\n",
      "Epoch 522/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.8953 - accuracy: 0.7918 - val_loss: 3.6731 - val_accuracy: 0.1786\n",
      "Epoch 523/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.8868 - accuracy: 0.7918 - val_loss: 3.6770 - val_accuracy: 0.1810\n",
      "Epoch 524/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.8590 - accuracy: 0.8173 - val_loss: 3.6716 - val_accuracy: 0.1857\n",
      "Epoch 525/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.8343 - accuracy: 0.8194 - val_loss: 3.6709 - val_accuracy: 0.1786\n",
      "Epoch 526/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.8556 - accuracy: 0.8255 - val_loss: 3.6706 - val_accuracy: 0.1833\n",
      "Epoch 527/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.8371 - accuracy: 0.8173 - val_loss: 3.6808 - val_accuracy: 0.1738\n",
      "Epoch 528/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.8560 - accuracy: 0.8286 - val_loss: 3.6844 - val_accuracy: 0.1833\n",
      "Epoch 529/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.8412 - accuracy: 0.8194 - val_loss: 3.6770 - val_accuracy: 0.1762\n",
      "Epoch 530/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.8658 - accuracy: 0.8020 - val_loss: 3.6772 - val_accuracy: 0.1738\n",
      "Epoch 531/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.8626 - accuracy: 0.8092 - val_loss: 3.6710 - val_accuracy: 0.1833\n",
      "Epoch 532/1000\n",
      "980/980 [==============================] - 9s 9ms/step - loss: 1.8575 - accuracy: 0.7918 - val_loss: 3.6763 - val_accuracy: 0.1714\n",
      "Epoch 533/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.8640 - accuracy: 0.8041 - val_loss: 3.6700 - val_accuracy: 0.1738\n",
      "Epoch 534/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.8333 - accuracy: 0.8102 - val_loss: 3.6729 - val_accuracy: 0.1810\n",
      "Epoch 535/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.8688 - accuracy: 0.7949 - val_loss: 3.6647 - val_accuracy: 0.1762\n",
      "Epoch 536/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.8287 - accuracy: 0.8153 - val_loss: 3.6666 - val_accuracy: 0.1738\n",
      "Epoch 537/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.8432 - accuracy: 0.8143 - val_loss: 3.6713 - val_accuracy: 0.1786\n",
      "Epoch 538/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.8733 - accuracy: 0.8082 - val_loss: 3.6872 - val_accuracy: 0.1833\n",
      "Epoch 539/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.8040 - accuracy: 0.8245 - val_loss: 3.6784 - val_accuracy: 0.1786\n",
      "Epoch 540/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.8369 - accuracy: 0.8173 - val_loss: 3.6719 - val_accuracy: 0.1786\n",
      "Epoch 541/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.8253 - accuracy: 0.8214 - val_loss: 3.6751 - val_accuracy: 0.1810\n",
      "Epoch 542/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.8434 - accuracy: 0.7969 - val_loss: 3.6743 - val_accuracy: 0.1833\n",
      "Epoch 543/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.7958 - accuracy: 0.8255 - val_loss: 3.6677 - val_accuracy: 0.1786\n",
      "Epoch 544/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.8216 - accuracy: 0.8122 - val_loss: 3.6645 - val_accuracy: 0.1857\n",
      "Epoch 545/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.8248 - accuracy: 0.8143 - val_loss: 3.6822 - val_accuracy: 0.1786\n",
      "Epoch 546/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.8058 - accuracy: 0.8133 - val_loss: 3.6771 - val_accuracy: 0.1738\n",
      "Epoch 547/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.8005 - accuracy: 0.8245 - val_loss: 3.6778 - val_accuracy: 0.1786\n",
      "Epoch 548/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.8295 - accuracy: 0.8204 - val_loss: 3.6663 - val_accuracy: 0.1833\n",
      "Epoch 549/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.8203 - accuracy: 0.8235 - val_loss: 3.6680 - val_accuracy: 0.1786\n",
      "Epoch 550/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.8139 - accuracy: 0.8276 - val_loss: 3.6805 - val_accuracy: 0.1762\n",
      "Epoch 551/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.7850 - accuracy: 0.8378 - val_loss: 3.6731 - val_accuracy: 0.1714\n",
      "Epoch 552/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.7884 - accuracy: 0.8286 - val_loss: 3.6571 - val_accuracy: 0.1762\n",
      "Epoch 553/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.7791 - accuracy: 0.8276 - val_loss: 3.6772 - val_accuracy: 0.1714\n",
      "Epoch 554/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.7878 - accuracy: 0.8276 - val_loss: 3.6716 - val_accuracy: 0.1762\n",
      "Epoch 555/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.7917 - accuracy: 0.8204 - val_loss: 3.6774 - val_accuracy: 0.1714\n",
      "Epoch 556/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.7995 - accuracy: 0.8214 - val_loss: 3.6677 - val_accuracy: 0.1810\n",
      "Epoch 557/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.8067 - accuracy: 0.8122 - val_loss: 3.6694 - val_accuracy: 0.1786\n",
      "Epoch 558/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.8004 - accuracy: 0.8224 - val_loss: 3.6675 - val_accuracy: 0.1810\n",
      "Epoch 559/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.7718 - accuracy: 0.8357 - val_loss: 3.6775 - val_accuracy: 0.1857\n",
      "Epoch 560/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.7723 - accuracy: 0.8276 - val_loss: 3.6808 - val_accuracy: 0.1738\n",
      "Epoch 561/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.7720 - accuracy: 0.8316 - val_loss: 3.6711 - val_accuracy: 0.1786\n",
      "Epoch 562/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.7762 - accuracy: 0.8408 - val_loss: 3.6666 - val_accuracy: 0.1786\n",
      "Epoch 563/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.7547 - accuracy: 0.8357 - val_loss: 3.6598 - val_accuracy: 0.1833\n",
      "Epoch 564/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.8014 - accuracy: 0.8276 - val_loss: 3.6742 - val_accuracy: 0.1810\n",
      "Epoch 565/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.7633 - accuracy: 0.8439 - val_loss: 3.6599 - val_accuracy: 0.1738\n",
      "Epoch 566/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.7628 - accuracy: 0.8398 - val_loss: 3.6724 - val_accuracy: 0.1690\n",
      "Epoch 567/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.7604 - accuracy: 0.8439 - val_loss: 3.6625 - val_accuracy: 0.1786\n",
      "Epoch 568/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.7514 - accuracy: 0.8459 - val_loss: 3.6706 - val_accuracy: 0.1786\n",
      "Epoch 569/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.7560 - accuracy: 0.8378 - val_loss: 3.6635 - val_accuracy: 0.1762\n",
      "Epoch 570/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.7635 - accuracy: 0.8286 - val_loss: 3.6588 - val_accuracy: 0.1905\n",
      "Epoch 571/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.7545 - accuracy: 0.8398 - val_loss: 3.6694 - val_accuracy: 0.1762\n",
      "Epoch 572/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.7760 - accuracy: 0.8327 - val_loss: 3.6671 - val_accuracy: 0.1786\n",
      "Epoch 573/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.7377 - accuracy: 0.8327 - val_loss: 3.6695 - val_accuracy: 0.1810\n",
      "Epoch 574/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.7324 - accuracy: 0.8388 - val_loss: 3.6574 - val_accuracy: 0.1833\n",
      "Epoch 575/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.7339 - accuracy: 0.8449 - val_loss: 3.6610 - val_accuracy: 0.1738\n",
      "Epoch 576/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.7558 - accuracy: 0.8255 - val_loss: 3.6811 - val_accuracy: 0.1810\n",
      "Epoch 577/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.7545 - accuracy: 0.8204 - val_loss: 3.6744 - val_accuracy: 0.1738\n",
      "Epoch 578/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.7306 - accuracy: 0.8408 - val_loss: 3.6734 - val_accuracy: 0.1762\n",
      "Epoch 579/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.7156 - accuracy: 0.8561 - val_loss: 3.6692 - val_accuracy: 0.1881\n",
      "Epoch 580/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.7378 - accuracy: 0.8418 - val_loss: 3.6574 - val_accuracy: 0.1762\n",
      "Epoch 581/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.7498 - accuracy: 0.8378 - val_loss: 3.6725 - val_accuracy: 0.1810\n",
      "Epoch 582/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.7488 - accuracy: 0.8480 - val_loss: 3.6671 - val_accuracy: 0.1833\n",
      "Epoch 583/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.7089 - accuracy: 0.8612 - val_loss: 3.6774 - val_accuracy: 0.1762\n",
      "Epoch 584/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.7065 - accuracy: 0.8490 - val_loss: 3.6679 - val_accuracy: 0.1762\n",
      "Epoch 585/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.7309 - accuracy: 0.8327 - val_loss: 3.6634 - val_accuracy: 0.1857\n",
      "Epoch 586/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.7231 - accuracy: 0.8459 - val_loss: 3.6583 - val_accuracy: 0.1929\n",
      "Epoch 587/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.7172 - accuracy: 0.8439 - val_loss: 3.6596 - val_accuracy: 0.1857\n",
      "Epoch 588/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.7071 - accuracy: 0.8357 - val_loss: 3.6592 - val_accuracy: 0.1738\n",
      "Epoch 589/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.7446 - accuracy: 0.8337 - val_loss: 3.6620 - val_accuracy: 0.1810\n",
      "Epoch 590/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.7167 - accuracy: 0.8439 - val_loss: 3.6557 - val_accuracy: 0.1833\n",
      "Epoch 591/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.7144 - accuracy: 0.8429 - val_loss: 3.6561 - val_accuracy: 0.1810\n",
      "Epoch 592/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "980/980 [==============================] - 8s 8ms/step - loss: 1.6992 - accuracy: 0.8449 - val_loss: 3.6560 - val_accuracy: 0.1857\n",
      "Epoch 593/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.7137 - accuracy: 0.8469 - val_loss: 3.6504 - val_accuracy: 0.1810\n",
      "Epoch 594/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.7026 - accuracy: 0.8500 - val_loss: 3.6612 - val_accuracy: 0.1857\n",
      "Epoch 595/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.6960 - accuracy: 0.8388 - val_loss: 3.6511 - val_accuracy: 0.1810\n",
      "Epoch 596/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.7149 - accuracy: 0.8194 - val_loss: 3.6583 - val_accuracy: 0.1810\n",
      "Epoch 597/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.7251 - accuracy: 0.8163 - val_loss: 3.6547 - val_accuracy: 0.1786\n",
      "Epoch 598/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.6804 - accuracy: 0.8551 - val_loss: 3.6529 - val_accuracy: 0.1762\n",
      "Epoch 599/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.7053 - accuracy: 0.8398 - val_loss: 3.6586 - val_accuracy: 0.1833\n",
      "Epoch 600/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.6971 - accuracy: 0.8408 - val_loss: 3.6619 - val_accuracy: 0.1881\n",
      "Epoch 601/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.6994 - accuracy: 0.8531 - val_loss: 3.6622 - val_accuracy: 0.1952\n",
      "Epoch 602/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.7174 - accuracy: 0.8449 - val_loss: 3.6562 - val_accuracy: 0.1881\n",
      "Epoch 603/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.7085 - accuracy: 0.8500 - val_loss: 3.6649 - val_accuracy: 0.1762\n",
      "Epoch 604/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.7060 - accuracy: 0.8388 - val_loss: 3.6536 - val_accuracy: 0.1881\n",
      "Epoch 605/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.6726 - accuracy: 0.8541 - val_loss: 3.6685 - val_accuracy: 0.1857\n",
      "Epoch 606/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.6878 - accuracy: 0.8551 - val_loss: 3.6574 - val_accuracy: 0.1810\n",
      "Epoch 607/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.7021 - accuracy: 0.8439 - val_loss: 3.6614 - val_accuracy: 0.1786\n",
      "Epoch 608/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.7063 - accuracy: 0.8347 - val_loss: 3.6601 - val_accuracy: 0.1786\n",
      "Epoch 609/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.6724 - accuracy: 0.8520 - val_loss: 3.6538 - val_accuracy: 0.1833\n",
      "Epoch 610/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.7042 - accuracy: 0.8418 - val_loss: 3.6556 - val_accuracy: 0.1786\n",
      "Epoch 611/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.6852 - accuracy: 0.8520 - val_loss: 3.6668 - val_accuracy: 0.1810\n",
      "Epoch 612/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.6768 - accuracy: 0.8449 - val_loss: 3.6558 - val_accuracy: 0.1833\n",
      "Epoch 613/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.6648 - accuracy: 0.8673 - val_loss: 3.6607 - val_accuracy: 0.1833\n",
      "Epoch 614/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.6733 - accuracy: 0.8612 - val_loss: 3.6512 - val_accuracy: 0.1714\n",
      "Epoch 615/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.6818 - accuracy: 0.8541 - val_loss: 3.6606 - val_accuracy: 0.1738\n",
      "Epoch 616/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.6902 - accuracy: 0.8429 - val_loss: 3.6623 - val_accuracy: 0.1738\n",
      "Epoch 617/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.6753 - accuracy: 0.8500 - val_loss: 3.6552 - val_accuracy: 0.1857\n",
      "Epoch 618/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.6527 - accuracy: 0.8633 - val_loss: 3.6599 - val_accuracy: 0.1738\n",
      "Epoch 619/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.6949 - accuracy: 0.8357 - val_loss: 3.6603 - val_accuracy: 0.1786\n",
      "Epoch 620/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.6913 - accuracy: 0.8408 - val_loss: 3.6605 - val_accuracy: 0.1762\n",
      "Epoch 621/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.7019 - accuracy: 0.8306 - val_loss: 3.6542 - val_accuracy: 0.1762\n",
      "Epoch 622/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.6740 - accuracy: 0.8480 - val_loss: 3.6578 - val_accuracy: 0.1738\n",
      "Epoch 623/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.6663 - accuracy: 0.8551 - val_loss: 3.6684 - val_accuracy: 0.1810\n",
      "Epoch 624/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.6843 - accuracy: 0.8439 - val_loss: 3.6580 - val_accuracy: 0.1762\n",
      "Epoch 625/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.6660 - accuracy: 0.8561 - val_loss: 3.6580 - val_accuracy: 0.1810\n",
      "Epoch 626/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.6265 - accuracy: 0.8582 - val_loss: 3.6520 - val_accuracy: 0.1810\n",
      "Epoch 627/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.6557 - accuracy: 0.8592 - val_loss: 3.6566 - val_accuracy: 0.1833\n",
      "Epoch 628/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.6360 - accuracy: 0.8755 - val_loss: 3.6501 - val_accuracy: 0.1690\n",
      "Epoch 629/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.6689 - accuracy: 0.8531 - val_loss: 3.6538 - val_accuracy: 0.1905\n",
      "Epoch 630/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.6287 - accuracy: 0.8531 - val_loss: 3.6491 - val_accuracy: 0.1833\n",
      "Epoch 631/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.6228 - accuracy: 0.8724 - val_loss: 3.6638 - val_accuracy: 0.1762\n",
      "Epoch 632/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.6534 - accuracy: 0.8439 - val_loss: 3.6556 - val_accuracy: 0.1810\n",
      "Epoch 633/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.6305 - accuracy: 0.8571 - val_loss: 3.6524 - val_accuracy: 0.1833\n",
      "Epoch 634/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.6211 - accuracy: 0.8561 - val_loss: 3.6433 - val_accuracy: 0.1833\n",
      "Epoch 635/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.6422 - accuracy: 0.8663 - val_loss: 3.6577 - val_accuracy: 0.1738\n",
      "Epoch 636/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.6304 - accuracy: 0.8643 - val_loss: 3.6310 - val_accuracy: 0.1929\n",
      "Epoch 637/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.6449 - accuracy: 0.8327 - val_loss: 3.6508 - val_accuracy: 0.1881\n",
      "Epoch 638/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.6202 - accuracy: 0.8714 - val_loss: 3.6513 - val_accuracy: 0.1786\n",
      "Epoch 639/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.6309 - accuracy: 0.8622 - val_loss: 3.6459 - val_accuracy: 0.1881\n",
      "Epoch 640/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.6515 - accuracy: 0.8480 - val_loss: 3.6591 - val_accuracy: 0.1905\n",
      "Epoch 641/1000\n",
      "980/980 [==============================] - 7s 8ms/step - loss: 1.6441 - accuracy: 0.8551 - val_loss: 3.6503 - val_accuracy: 0.1929\n",
      "Epoch 642/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.6151 - accuracy: 0.8684 - val_loss: 3.6595 - val_accuracy: 0.1762\n",
      "Epoch 643/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.6306 - accuracy: 0.8622 - val_loss: 3.6507 - val_accuracy: 0.1762\n",
      "Epoch 644/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.5803 - accuracy: 0.8806 - val_loss: 3.6413 - val_accuracy: 0.1833\n",
      "Epoch 645/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.6468 - accuracy: 0.8541 - val_loss: 3.6402 - val_accuracy: 0.1881\n",
      "Epoch 646/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.5980 - accuracy: 0.8571 - val_loss: 3.6519 - val_accuracy: 0.1786\n",
      "Epoch 647/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.6025 - accuracy: 0.8837 - val_loss: 3.6483 - val_accuracy: 0.1952\n",
      "Epoch 648/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.5933 - accuracy: 0.8694 - val_loss: 3.6395 - val_accuracy: 0.1881\n",
      "Epoch 649/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.6281 - accuracy: 0.8541 - val_loss: 3.6441 - val_accuracy: 0.1786\n",
      "Epoch 650/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.5748 - accuracy: 0.8918 - val_loss: 3.6447 - val_accuracy: 0.1833\n",
      "Epoch 651/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.6045 - accuracy: 0.8735 - val_loss: 3.6432 - val_accuracy: 0.1786\n",
      "Epoch 652/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.5964 - accuracy: 0.8684 - val_loss: 3.6483 - val_accuracy: 0.1833\n",
      "Epoch 653/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.5943 - accuracy: 0.8837 - val_loss: 3.6493 - val_accuracy: 0.1762\n",
      "Epoch 654/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.6381 - accuracy: 0.8357 - val_loss: 3.6644 - val_accuracy: 0.1738\n",
      "Epoch 655/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.5879 - accuracy: 0.8786 - val_loss: 3.6566 - val_accuracy: 0.1857\n",
      "Epoch 656/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.5975 - accuracy: 0.8663 - val_loss: 3.6594 - val_accuracy: 0.1833\n",
      "Epoch 657/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.5833 - accuracy: 0.8908 - val_loss: 3.6439 - val_accuracy: 0.1762\n",
      "Epoch 658/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.5865 - accuracy: 0.8714 - val_loss: 3.6485 - val_accuracy: 0.1833\n",
      "Epoch 659/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.5838 - accuracy: 0.8816 - val_loss: 3.6493 - val_accuracy: 0.1833\n",
      "Epoch 660/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.5892 - accuracy: 0.8704 - val_loss: 3.6471 - val_accuracy: 0.1833\n",
      "Epoch 661/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.5839 - accuracy: 0.8735 - val_loss: 3.6456 - val_accuracy: 0.1857\n",
      "Epoch 662/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.5798 - accuracy: 0.8684 - val_loss: 3.6473 - val_accuracy: 0.1833\n",
      "Epoch 663/1000\n",
      "980/980 [==============================] - 9s 9ms/step - loss: 1.5935 - accuracy: 0.8622 - val_loss: 3.6435 - val_accuracy: 0.1738\n",
      "Epoch 664/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.5979 - accuracy: 0.8643 - val_loss: 3.6442 - val_accuracy: 0.1762\n",
      "Epoch 665/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.5649 - accuracy: 0.8816 - val_loss: 3.6543 - val_accuracy: 0.1762\n",
      "Epoch 666/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.6086 - accuracy: 0.8663 - val_loss: 3.6642 - val_accuracy: 0.1714\n",
      "Epoch 667/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.6027 - accuracy: 0.8582 - val_loss: 3.6671 - val_accuracy: 0.1810\n",
      "Epoch 668/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.5646 - accuracy: 0.8837 - val_loss: 3.6406 - val_accuracy: 0.1857\n",
      "Epoch 669/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.5559 - accuracy: 0.8735 - val_loss: 3.6403 - val_accuracy: 0.1857\n",
      "Epoch 670/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.5881 - accuracy: 0.8622 - val_loss: 3.6555 - val_accuracy: 0.1929\n",
      "Epoch 671/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.5740 - accuracy: 0.8673 - val_loss: 3.6630 - val_accuracy: 0.1881\n",
      "Epoch 672/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.5626 - accuracy: 0.8796 - val_loss: 3.6540 - val_accuracy: 0.1762\n",
      "Epoch 673/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.5297 - accuracy: 0.8878 - val_loss: 3.6565 - val_accuracy: 0.1833\n",
      "Epoch 674/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.5514 - accuracy: 0.8776 - val_loss: 3.6420 - val_accuracy: 0.1762\n",
      "Epoch 675/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.5511 - accuracy: 0.8786 - val_loss: 3.6545 - val_accuracy: 0.1857\n",
      "Epoch 676/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.5460 - accuracy: 0.8847 - val_loss: 3.6456 - val_accuracy: 0.1810\n",
      "Epoch 677/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.5583 - accuracy: 0.8867 - val_loss: 3.6450 - val_accuracy: 0.1857\n",
      "Epoch 678/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.5482 - accuracy: 0.8694 - val_loss: 3.6536 - val_accuracy: 0.1810\n",
      "Epoch 679/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.5617 - accuracy: 0.8816 - val_loss: 3.6489 - val_accuracy: 0.1762\n",
      "Epoch 680/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.5435 - accuracy: 0.8776 - val_loss: 3.6543 - val_accuracy: 0.1738\n",
      "Epoch 681/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.5391 - accuracy: 0.8816 - val_loss: 3.6469 - val_accuracy: 0.1810\n",
      "Epoch 682/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.5561 - accuracy: 0.8888 - val_loss: 3.6467 - val_accuracy: 0.1810\n",
      "Epoch 683/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.5515 - accuracy: 0.8806 - val_loss: 3.6406 - val_accuracy: 0.1833\n",
      "Epoch 684/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.5414 - accuracy: 0.8857 - val_loss: 3.6388 - val_accuracy: 0.1786\n",
      "Epoch 685/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.5540 - accuracy: 0.8837 - val_loss: 3.6458 - val_accuracy: 0.1786\n",
      "Epoch 686/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.5564 - accuracy: 0.8653 - val_loss: 3.6415 - val_accuracy: 0.1714\n",
      "Epoch 687/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.5476 - accuracy: 0.8827 - val_loss: 3.6493 - val_accuracy: 0.1762\n",
      "Epoch 688/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.5651 - accuracy: 0.8582 - val_loss: 3.6477 - val_accuracy: 0.1833\n",
      "Epoch 689/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.5411 - accuracy: 0.8643 - val_loss: 3.6347 - val_accuracy: 0.1929\n",
      "Epoch 690/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.5259 - accuracy: 0.8765 - val_loss: 3.6352 - val_accuracy: 0.1881\n",
      "Epoch 691/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.5301 - accuracy: 0.8867 - val_loss: 3.6451 - val_accuracy: 0.1881\n",
      "Epoch 692/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.5412 - accuracy: 0.8806 - val_loss: 3.6355 - val_accuracy: 0.1857\n",
      "Epoch 693/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.5569 - accuracy: 0.8643 - val_loss: 3.6490 - val_accuracy: 0.1833\n",
      "Epoch 694/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.5421 - accuracy: 0.8847 - val_loss: 3.6488 - val_accuracy: 0.1857\n",
      "Epoch 695/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.5159 - accuracy: 0.8796 - val_loss: 3.6494 - val_accuracy: 0.1714\n",
      "Epoch 696/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.4963 - accuracy: 0.8898 - val_loss: 3.6650 - val_accuracy: 0.1762\n",
      "Epoch 697/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.5069 - accuracy: 0.8949 - val_loss: 3.6433 - val_accuracy: 0.1786\n",
      "Epoch 698/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.5149 - accuracy: 0.8847 - val_loss: 3.6432 - val_accuracy: 0.1762\n",
      "Epoch 699/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.5313 - accuracy: 0.8939 - val_loss: 3.6462 - val_accuracy: 0.1810\n",
      "Epoch 700/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.5479 - accuracy: 0.8827 - val_loss: 3.6481 - val_accuracy: 0.1929\n",
      "Epoch 701/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.5082 - accuracy: 0.8929 - val_loss: 3.6309 - val_accuracy: 0.1929\n",
      "Epoch 702/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.5072 - accuracy: 0.8827 - val_loss: 3.6398 - val_accuracy: 0.1905\n",
      "Epoch 703/1000\n",
      "980/980 [==============================] - 8s 8ms/step - loss: 1.5230 - accuracy: 0.8847 - val_loss: 3.6351 - val_accuracy: 0.1976\n",
      "Epoch 704/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "980/980 [==============================] - 8s 8ms/step - loss: 1.5220 - accuracy: 0.8918 - val_loss: 3.6419 - val_accuracy: 0.1881\n",
      "Epoch 705/1000\n",
      "416/980 [===========>..................] - ETA: 3s - loss: 1.5545 - accuracy: 0.8870"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-18c76cfa159a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvgg16network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_trainHot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_testHot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0mplot_learning_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0mplotKerasLearningCurve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-18c76cfa159a>\u001b[0m in \u001b[0;36mvgg16network\u001b[0;34m(a, b, c, d, e, f, g)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         history = model.fit(a,b, epochs=epochs, class_weight=e, \n\u001b[0;32m---> 38\u001b[0;31m                             validation_data=(c,d), verbose=1,callbacks = [MetricsCheckpoint('logs')])\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nKeras VGG16 - accuracy:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/tfl/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m~/miniconda2/envs/tfl/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/tfl/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3292\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m~/miniconda2/envs/tfl/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "weight_path = './keras-pretrained-models/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "map_characters=dict_characters\n",
    "\n",
    "with tf.device('/device:GPU:0'):\n",
    "\n",
    "    def vgg16network(a,b,c,d,e,f,g):\n",
    "        num_class = f\n",
    "        epochs = g\n",
    "        base_model = VGG16(weights='imagenet',\n",
    "            include_top=False, input_shape=(img_size, img_size, 3))\n",
    "        # Add a new top layer\n",
    "        x = base_model.output\n",
    "        x = Flatten()(x)\n",
    "#         x = Dropout(0.25)(x)\n",
    "#         x = Dense(512, use_bias=False, activation=None,kernel_regularizer=keras.regularizers.l2(0.00))(x)\n",
    "#         x = BatchNormalization()(x)\n",
    "#         x = Activation('relu')(x)\n",
    "#         x = Dropout(0.4)(x)\n",
    "        x = Dense(64, use_bias=False, activation=None,activity_regularizer=keras.regularizers.l2(0.01),\n",
    "                  kernel_regularizer=keras.regularizers.l2(0.01))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "        predictions = Dense(num_class, activation='softmax')(x)\n",
    "        # This is the model we will train\n",
    "        model = Model(inputs=base_model.input, outputs=predictions)\n",
    "        # First: train only the top layers (which were randomly initialized)\n",
    "        for layer in base_model.layers:\n",
    "            layer.trainable = False\n",
    "        model.compile(loss='categorical_crossentropy', \n",
    "                      optimizer=keras.optimizers.SGD(lr=0.0005), metrics=['accuracy'])\n",
    "        callbacks_list = [keras.callbacks.EarlyStopping(monitor='val_acc', patience=3, verbose=1)]\n",
    "        model.summary()\n",
    "        history = model.fit(a,b, epochs=epochs, class_weight=e, \n",
    "                            validation_data=(c,d), verbose=1,callbacks = [MetricsCheckpoint('logs')])\n",
    "        score = model.evaluate(c,d, verbose=0)\n",
    "        print('\\nKeras VGG16 - accuracy:', score[1], '\\n')\n",
    "        y_pred = model.predict(c)\n",
    "        print('\\n', sklearn.metrics.classification_report(np.where(d > 0)[1], np.argmax(y_pred, axis=1), target_names=list(map_characters.values())), sep='') \n",
    "        Y_pred_classes = np.argmax(y_pred,axis = 1) \n",
    "        Y_true = np.argmax(d,axis = 1) \n",
    "        confusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n",
    "        plot_confusion_matrix(confusion_mtx, classes = list(map_characters.values()))\n",
    "        plt.show()\n",
    "        return model, history\n",
    "model, history = vgg16network(X_train, Y_trainHot, X_test, Y_testHot,class_weight,14,1000)\n",
    "plot_learning_curve(history)\n",
    "plotKerasLearningCurve(history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred = model.predict(X_train)\n",
    "Y_pred_classes = np.argmax(y_pred,axis = 1) \n",
    "Y_true = np.argmax(Y_trainHot,axis = 1) \n",
    "confusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n",
    "plot_confusion_matrix(confusion_mtx, classes = list(map_characters.values()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x648 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot ROC curve\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "fig, c_ax = plt.subplots(1,1, figsize = (9, 9))\n",
    "predictions = model.predict(X_train)\n",
    "for (idx, c_label) in enumerate(map_characters):\n",
    "    fpr, tpr, thresholds = roc_curve(Y_trainHot[:,idx].astype(int), predictions[:,idx])\n",
    "    c_ax.plot(fpr, tpr, label = '%s (AUC:%0.2f)'  % (c_label, auc(fpr, tpr)))\n",
    "c_ax.legend()\n",
    "c_ax.set_xlabel('False Positive Rate')\n",
    "c_ax.set_ylabel('True Positive Rate')\n",
    "fig.savefig('barely_trained_net.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alan/miniconda2/envs/tfl/lib/python3.6/site-packages/keras_applications/resnet50.py:265: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.\n",
      "  warnings.warn('The output shape of `ResNet50(include_top=False)` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 128, 128, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 134, 134, 3)  0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 64, 64, 64)   9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bn_conv1 (BatchNormalization)   (None, 64, 64, 64)   256         conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 64, 64, 64)   0           bn_conv1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 66, 66, 64)   0           activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 32, 32, 64)   0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2a (Conv2D)         (None, 32, 32, 64)   4160        max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2a (BatchNormalizati (None, 32, 32, 64)   256         res2a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 32, 32, 64)   0           bn2a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2b (Conv2D)         (None, 32, 32, 64)   36928       activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2b (BatchNormalizati (None, 32, 32, 64)   256         res2a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 32, 32, 64)   0           bn2a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2c (Conv2D)         (None, 32, 32, 256)  16640       activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch1 (Conv2D)          (None, 32, 32, 256)  16640       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2c (BatchNormalizati (None, 32, 32, 256)  1024        res2a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch1 (BatchNormalizatio (None, 32, 32, 256)  1024        res2a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_17 (Add)                    (None, 32, 32, 256)  0           bn2a_branch2c[0][0]              \n",
      "                                                                 bn2a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 32, 32, 256)  0           add_17[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2a (Conv2D)         (None, 32, 32, 64)   16448       activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2a (BatchNormalizati (None, 32, 32, 64)   256         res2b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 32, 32, 64)   0           bn2b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2b (Conv2D)         (None, 32, 32, 64)   36928       activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2b (BatchNormalizati (None, 32, 32, 64)   256         res2b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 32, 32, 64)   0           bn2b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2c (Conv2D)         (None, 32, 32, 256)  16640       activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2c (BatchNormalizati (None, 32, 32, 256)  1024        res2b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_18 (Add)                    (None, 32, 32, 256)  0           bn2b_branch2c[0][0]              \n",
      "                                                                 activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 32, 32, 256)  0           add_18[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2a (Conv2D)         (None, 32, 32, 64)   16448       activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2a (BatchNormalizati (None, 32, 32, 64)   256         res2c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 32, 32, 64)   0           bn2c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2b (Conv2D)         (None, 32, 32, 64)   36928       activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2b (BatchNormalizati (None, 32, 32, 64)   256         res2c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 32, 32, 64)   0           bn2c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2c (Conv2D)         (None, 32, 32, 256)  16640       activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2c (BatchNormalizati (None, 32, 32, 256)  1024        res2c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_19 (Add)                    (None, 32, 32, 256)  0           bn2c_branch2c[0][0]              \n",
      "                                                                 activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 32, 32, 256)  0           add_19[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2a (Conv2D)         (None, 16, 16, 128)  32896       activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2a (BatchNormalizati (None, 16, 16, 128)  512         res3a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 16, 16, 128)  0           bn3a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2b (Conv2D)         (None, 16, 16, 128)  147584      activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2b (BatchNormalizati (None, 16, 16, 128)  512         res3a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 16, 16, 128)  0           bn3a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2c (Conv2D)         (None, 16, 16, 512)  66048       activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch1 (Conv2D)          (None, 16, 16, 512)  131584      activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2c (BatchNormalizati (None, 16, 16, 512)  2048        res3a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch1 (BatchNormalizatio (None, 16, 16, 512)  2048        res3a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_20 (Add)                    (None, 16, 16, 512)  0           bn3a_branch2c[0][0]              \n",
      "                                                                 bn3a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 16, 16, 512)  0           add_20[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2a (Conv2D)         (None, 16, 16, 128)  65664       activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2a (BatchNormalizati (None, 16, 16, 128)  512         res3b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 16, 16, 128)  0           bn3b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2b (Conv2D)         (None, 16, 16, 128)  147584      activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2b (BatchNormalizati (None, 16, 16, 128)  512         res3b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 16, 16, 128)  0           bn3b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2c (Conv2D)         (None, 16, 16, 512)  66048       activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2c (BatchNormalizati (None, 16, 16, 512)  2048        res3b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_21 (Add)                    (None, 16, 16, 512)  0           bn3b_branch2c[0][0]              \n",
      "                                                                 activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 16, 16, 512)  0           add_21[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2a (Conv2D)         (None, 16, 16, 128)  65664       activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2a (BatchNormalizati (None, 16, 16, 128)  512         res3c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_66 (Activation)      (None, 16, 16, 128)  0           bn3c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2b (Conv2D)         (None, 16, 16, 128)  147584      activation_66[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2b (BatchNormalizati (None, 16, 16, 128)  512         res3c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_67 (Activation)      (None, 16, 16, 128)  0           bn3c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2c (Conv2D)         (None, 16, 16, 512)  66048       activation_67[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2c (BatchNormalizati (None, 16, 16, 512)  2048        res3c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_22 (Add)                    (None, 16, 16, 512)  0           bn3c_branch2c[0][0]              \n",
      "                                                                 activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_68 (Activation)      (None, 16, 16, 512)  0           add_22[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2a (Conv2D)         (None, 16, 16, 128)  65664       activation_68[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2a (BatchNormalizati (None, 16, 16, 128)  512         res3d_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_69 (Activation)      (None, 16, 16, 128)  0           bn3d_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2b (Conv2D)         (None, 16, 16, 128)  147584      activation_69[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2b (BatchNormalizati (None, 16, 16, 128)  512         res3d_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_70 (Activation)      (None, 16, 16, 128)  0           bn3d_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2c (Conv2D)         (None, 16, 16, 512)  66048       activation_70[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2c (BatchNormalizati (None, 16, 16, 512)  2048        res3d_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_23 (Add)                    (None, 16, 16, 512)  0           bn3d_branch2c[0][0]              \n",
      "                                                                 activation_68[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_71 (Activation)      (None, 16, 16, 512)  0           add_23[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2a (Conv2D)         (None, 8, 8, 256)    131328      activation_71[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2a (BatchNormalizati (None, 8, 8, 256)    1024        res4a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_72 (Activation)      (None, 8, 8, 256)    0           bn4a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2b (Conv2D)         (None, 8, 8, 256)    590080      activation_72[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2b (BatchNormalizati (None, 8, 8, 256)    1024        res4a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_73 (Activation)      (None, 8, 8, 256)    0           bn4a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2c (Conv2D)         (None, 8, 8, 1024)   263168      activation_73[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch1 (Conv2D)          (None, 8, 8, 1024)   525312      activation_71[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2c (BatchNormalizati (None, 8, 8, 1024)   4096        res4a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch1 (BatchNormalizatio (None, 8, 8, 1024)   4096        res4a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_24 (Add)                    (None, 8, 8, 1024)   0           bn4a_branch2c[0][0]              \n",
      "                                                                 bn4a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_74 (Activation)      (None, 8, 8, 1024)   0           add_24[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2a (Conv2D)         (None, 8, 8, 256)    262400      activation_74[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2a (BatchNormalizati (None, 8, 8, 256)    1024        res4b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_75 (Activation)      (None, 8, 8, 256)    0           bn4b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2b (Conv2D)         (None, 8, 8, 256)    590080      activation_75[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2b (BatchNormalizati (None, 8, 8, 256)    1024        res4b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_76 (Activation)      (None, 8, 8, 256)    0           bn4b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2c (Conv2D)         (None, 8, 8, 1024)   263168      activation_76[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2c (BatchNormalizati (None, 8, 8, 1024)   4096        res4b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_25 (Add)                    (None, 8, 8, 1024)   0           bn4b_branch2c[0][0]              \n",
      "                                                                 activation_74[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_77 (Activation)      (None, 8, 8, 1024)   0           add_25[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2a (Conv2D)         (None, 8, 8, 256)    262400      activation_77[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2a (BatchNormalizati (None, 8, 8, 256)    1024        res4c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_78 (Activation)      (None, 8, 8, 256)    0           bn4c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2b (Conv2D)         (None, 8, 8, 256)    590080      activation_78[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2b (BatchNormalizati (None, 8, 8, 256)    1024        res4c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_79 (Activation)      (None, 8, 8, 256)    0           bn4c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2c (Conv2D)         (None, 8, 8, 1024)   263168      activation_79[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2c (BatchNormalizati (None, 8, 8, 1024)   4096        res4c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_26 (Add)                    (None, 8, 8, 1024)   0           bn4c_branch2c[0][0]              \n",
      "                                                                 activation_77[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_80 (Activation)      (None, 8, 8, 1024)   0           add_26[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2a (Conv2D)         (None, 8, 8, 256)    262400      activation_80[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2a (BatchNormalizati (None, 8, 8, 256)    1024        res4d_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_81 (Activation)      (None, 8, 8, 256)    0           bn4d_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2b (Conv2D)         (None, 8, 8, 256)    590080      activation_81[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2b (BatchNormalizati (None, 8, 8, 256)    1024        res4d_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_82 (Activation)      (None, 8, 8, 256)    0           bn4d_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2c (Conv2D)         (None, 8, 8, 1024)   263168      activation_82[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2c (BatchNormalizati (None, 8, 8, 1024)   4096        res4d_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_27 (Add)                    (None, 8, 8, 1024)   0           bn4d_branch2c[0][0]              \n",
      "                                                                 activation_80[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_83 (Activation)      (None, 8, 8, 1024)   0           add_27[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2a (Conv2D)         (None, 8, 8, 256)    262400      activation_83[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2a (BatchNormalizati (None, 8, 8, 256)    1024        res4e_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_84 (Activation)      (None, 8, 8, 256)    0           bn4e_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2b (Conv2D)         (None, 8, 8, 256)    590080      activation_84[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2b (BatchNormalizati (None, 8, 8, 256)    1024        res4e_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_85 (Activation)      (None, 8, 8, 256)    0           bn4e_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2c (Conv2D)         (None, 8, 8, 1024)   263168      activation_85[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2c (BatchNormalizati (None, 8, 8, 1024)   4096        res4e_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_28 (Add)                    (None, 8, 8, 1024)   0           bn4e_branch2c[0][0]              \n",
      "                                                                 activation_83[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_86 (Activation)      (None, 8, 8, 1024)   0           add_28[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2a (Conv2D)         (None, 8, 8, 256)    262400      activation_86[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2a (BatchNormalizati (None, 8, 8, 256)    1024        res4f_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_87 (Activation)      (None, 8, 8, 256)    0           bn4f_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2b (Conv2D)         (None, 8, 8, 256)    590080      activation_87[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2b (BatchNormalizati (None, 8, 8, 256)    1024        res4f_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_88 (Activation)      (None, 8, 8, 256)    0           bn4f_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2c (Conv2D)         (None, 8, 8, 1024)   263168      activation_88[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2c (BatchNormalizati (None, 8, 8, 1024)   4096        res4f_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_29 (Add)                    (None, 8, 8, 1024)   0           bn4f_branch2c[0][0]              \n",
      "                                                                 activation_86[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_89 (Activation)      (None, 8, 8, 1024)   0           add_29[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2a (Conv2D)         (None, 4, 4, 512)    524800      activation_89[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2a (BatchNormalizati (None, 4, 4, 512)    2048        res5a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_90 (Activation)      (None, 4, 4, 512)    0           bn5a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2b (Conv2D)         (None, 4, 4, 512)    2359808     activation_90[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2b (BatchNormalizati (None, 4, 4, 512)    2048        res5a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_91 (Activation)      (None, 4, 4, 512)    0           bn5a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2c (Conv2D)         (None, 4, 4, 2048)   1050624     activation_91[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch1 (Conv2D)          (None, 4, 4, 2048)   2099200     activation_89[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2c (BatchNormalizati (None, 4, 4, 2048)   8192        res5a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch1 (BatchNormalizatio (None, 4, 4, 2048)   8192        res5a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_30 (Add)                    (None, 4, 4, 2048)   0           bn5a_branch2c[0][0]              \n",
      "                                                                 bn5a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_92 (Activation)      (None, 4, 4, 2048)   0           add_30[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2a (Conv2D)         (None, 4, 4, 512)    1049088     activation_92[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2a (BatchNormalizati (None, 4, 4, 512)    2048        res5b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_93 (Activation)      (None, 4, 4, 512)    0           bn5b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2b (Conv2D)         (None, 4, 4, 512)    2359808     activation_93[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2b (BatchNormalizati (None, 4, 4, 512)    2048        res5b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_94 (Activation)      (None, 4, 4, 512)    0           bn5b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2c (Conv2D)         (None, 4, 4, 2048)   1050624     activation_94[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2c (BatchNormalizati (None, 4, 4, 2048)   8192        res5b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_31 (Add)                    (None, 4, 4, 2048)   0           bn5b_branch2c[0][0]              \n",
      "                                                                 activation_92[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_95 (Activation)      (None, 4, 4, 2048)   0           add_31[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2a (Conv2D)         (None, 4, 4, 512)    1049088     activation_95[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2a (BatchNormalizati (None, 4, 4, 512)    2048        res5c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_96 (Activation)      (None, 4, 4, 512)    0           bn5c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2b (Conv2D)         (None, 4, 4, 512)    2359808     activation_96[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2b (BatchNormalizati (None, 4, 4, 512)    2048        res5c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_97 (Activation)      (None, 4, 4, 512)    0           bn5c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2c (Conv2D)         (None, 4, 4, 2048)   1050624     activation_97[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2c (BatchNormalizati (None, 4, 4, 2048)   8192        res5c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_32 (Add)                    (None, 4, 4, 2048)   0           bn5c_branch2c[0][0]              \n",
      "                                                                 activation_95[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_98 (Activation)      (None, 4, 4, 2048)   0           add_32[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 4, 4, 2048)   0           activation_98[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_2 (Glo (None, 2048)         0           dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 128)          262272      global_average_pooling2d_2[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 128)          0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 64)           8256        dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 64)           0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 64)           4160        dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 64)           0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 14)           910         dropout_8[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 23,863,310\n",
      "Trainable params: 275,598\n",
      "Non-trainable params: 23,587,712\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9343 samples, validate on 2336 samples\n",
      "Epoch 1/10\n",
      "9343/9343 [==============================] - 109s 12ms/step - loss: 5.8843 - accuracy: 0.0872 - val_loss: 43.2679 - val_accuracy: 0.0925\n",
      "Epoch 2/10\n",
      "9343/9343 [==============================] - 106s 11ms/step - loss: 3.0335 - accuracy: 0.0888 - val_loss: 2.5860 - val_accuracy: 0.0796\n",
      "Epoch 3/10\n",
      "9343/9343 [==============================] - 106s 11ms/step - loss: 2.6742 - accuracy: 0.0846 - val_loss: 2.5846 - val_accuracy: 0.0903\n",
      "Epoch 4/10\n",
      "4928/9343 [==============>...............] - ETA: 40s - loss: 2.5784 - accuracy: 0.0842"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-956451d87b90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mResNet50network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_trainHot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_testHot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0mplot_learning_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0mplotKerasLearningCurve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-956451d87b90>\u001b[0m in \u001b[0;36mResNet50network\u001b[0;34m(a, b, c, d, e, f, g)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         history = model.fit(a,b, epochs=epochs, class_weight=e, \n\u001b[0;32m---> 53\u001b[0;31m                             validation_data=(c,d), verbose=1,callbacks = [MetricsCheckpoint('logs')])\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nKeras ResNet50 - accuracy:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/tfl/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m~/miniconda2/envs/tfl/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/tfl/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3292\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m~/miniconda2/envs/tfl/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.models import Model\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "# weight_path = './keras-pretrained-models/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "map_characters=dict_characters\n",
    "import tensorflow as tf\n",
    "\n",
    "with tf.device('/device:GPU:0'):\n",
    "\n",
    "    def ResNet50network(a,b,c,d,e,f,g):\n",
    "        num_class = f\n",
    "        epochs = g\n",
    "        base_model = ResNet50(weights='imagenet',\n",
    "            include_top=False, input_shape=(img_size, img_size, 3))\n",
    "        # Add a new top layer\n",
    "        x = base_model.output\n",
    "#         x = BatchNormalization()(x)\n",
    "        x = Dropout(0.25)(x)\n",
    "        x = GlobalAveragePooling2D()(x)\n",
    "#         x = Flatten()(x)\n",
    "\n",
    "        x = Dense(128, activation = \"relu\",kernel_regularizer=keras.regularizers.l2(0.0)\n",
    "                 ,bias_regularizer=keras.regularizers.l2(0.00))(x)\n",
    "#         x = BatchNormalization()(x)\n",
    "        x = Dropout(0.25)(x)\n",
    "        x = Dense(64, activation = \"relu\",kernel_regularizer=keras.regularizers.l2(0.00)\n",
    "                 ,bias_regularizer=keras.regularizers.l2(0.00))(x)\n",
    "#         x = Dense(256, activation = \"relu\",kernel_regularizer=keras.regularizers.l2(0.005)\n",
    "#                  ,bias_regularizer=keras.regularizers.l2(0.005))(x)\n",
    "#         x = BatchNormalization()(x)\n",
    "        x = Dropout(0.25)(x)\n",
    "        x = Dense(64, activation = \"relu\",kernel_regularizer=keras.regularizers.l2(0.00)\n",
    "                 ,bias_regularizer=keras.regularizers.l2(0.00))(x)\n",
    "#         x = BatchNormalization()(x)\n",
    "        x = Dropout(0.25)(x)\n",
    "        predictions = Dense(num_class, activation='softmax')(x)\n",
    "        # This is the model we will train\n",
    "        model = Model(inputs=base_model.input, outputs=predictions)\n",
    "        # First: train only the top layers (which were randomly initialized)\n",
    "        for layer in base_model.layers:\n",
    "            layer.trainable = False\n",
    "#         for layer in base_model.layers:\n",
    "#             print(layer, layer.trainable)\n",
    "\n",
    "#         checkpoint = ModelCheckpoint(\"best_model.hdf5\", monitor='loss', verbose=1,\n",
    "#                                      save_weights_only=True, mode='auto', period=1)\n",
    "        \n",
    "        model.compile(loss='categorical_crossentropy', \n",
    "                      optimizer=keras.optimizers.Adam(lr=0.1), metrics=['accuracy'])\n",
    "        callbacks_list = [keras.callbacks.EarlyStopping(monitor='val_acc', patience=3, verbose=1)]\n",
    "        model.summary()\n",
    "        history = model.fit(a,b, epochs=epochs, class_weight=e, \n",
    "                            validation_data=(c,d), verbose=1,callbacks = [MetricsCheckpoint('logs')])\n",
    "        score = model.evaluate(c,d, verbose=0)\n",
    "        print('\\nKeras ResNet50 - accuracy:', score[1], '\\n')\n",
    "        y_pred = model.predict(c)\n",
    "        print('\\n', sklearn.metrics.classification_report(np.where(d > 0)[1], np.argmax(y_pred, axis=1), target_names=list(map_characters.values())), sep='') \n",
    "        Y_pred_classes = np.argmax(y_pred,axis = 1) \n",
    "        Y_true = np.argmax(d,axis = 1) \n",
    "        confusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n",
    "        plot_confusion_matrix(confusion_mtx, classes = list(map_characters.values()))\n",
    "        plt.show()\n",
    "        \n",
    "        return model, history\n",
    "model, history = ResNet50network(X_train, Y_trainHot, X_test, Y_testHot,class_weight,14,10)\n",
    "plot_learning_curve(history)\n",
    "plotKerasLearningCurve(history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/alan/miniconda2/envs/tfl/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 79, 79, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv1 (Conv2D)           (None, 39, 39, 32)   864         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv1_bn (BatchNormaliza (None, 39, 39, 32)   128         block1_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv1_act (Activation)   (None, 39, 39, 32)   0           block1_conv1_bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv2 (Conv2D)           (None, 37, 37, 64)   18432       block1_conv1_act[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv2_bn (BatchNormaliza (None, 37, 37, 64)   256         block1_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv2_act (Activation)   (None, 37, 37, 64)   0           block1_conv2_bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv1 (SeparableConv2 (None, 37, 37, 128)  8768        block1_conv2_act[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv1_bn (BatchNormal (None, 37, 37, 128)  512         block2_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv2_act (Activation (None, 37, 37, 128)  0           block2_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv2 (SeparableConv2 (None, 37, 37, 128)  17536       block2_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv2_bn (BatchNormal (None, 37, 37, 128)  512         block2_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 19, 19, 128)  8192        block1_conv2_act[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block2_pool (MaxPooling2D)      (None, 19, 19, 128)  0           block2_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 19, 19, 128)  512         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 19, 19, 128)  0           block2_pool[0][0]                \n",
      "                                                                 batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv1_act (Activation (None, 19, 19, 128)  0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv1 (SeparableConv2 (None, 19, 19, 256)  33920       block3_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv1_bn (BatchNormal (None, 19, 19, 256)  1024        block3_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv2_act (Activation (None, 19, 19, 256)  0           block3_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv2 (SeparableConv2 (None, 19, 19, 256)  67840       block3_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv2_bn (BatchNormal (None, 19, 19, 256)  1024        block3_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 10, 10, 256)  32768       add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block3_pool (MaxPooling2D)      (None, 10, 10, 256)  0           block3_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 10, 10, 256)  1024        conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 10, 10, 256)  0           block3_pool[0][0]                \n",
      "                                                                 batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv1_act (Activation (None, 10, 10, 256)  0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv1 (SeparableConv2 (None, 10, 10, 728)  188672      block4_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv1_bn (BatchNormal (None, 10, 10, 728)  2912        block4_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv2_act (Activation (None, 10, 10, 728)  0           block4_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv2 (SeparableConv2 (None, 10, 10, 728)  536536      block4_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv2_bn (BatchNormal (None, 10, 10, 728)  2912        block4_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 5, 5, 728)    186368      add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block4_pool (MaxPooling2D)      (None, 5, 5, 728)    0           block4_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 5, 5, 728)    2912        conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 5, 5, 728)    0           block4_pool[0][0]                \n",
      "                                                                 batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv1_act (Activation (None, 5, 5, 728)    0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv1 (SeparableConv2 (None, 5, 5, 728)    536536      block5_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv1_bn (BatchNormal (None, 5, 5, 728)    2912        block5_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv2_act (Activation (None, 5, 5, 728)    0           block5_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv2 (SeparableConv2 (None, 5, 5, 728)    536536      block5_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv2_bn (BatchNormal (None, 5, 5, 728)    2912        block5_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv3_act (Activation (None, 5, 5, 728)    0           block5_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv3 (SeparableConv2 (None, 5, 5, 728)    536536      block5_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv3_bn (BatchNormal (None, 5, 5, 728)    2912        block5_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 5, 5, 728)    0           block5_sepconv3_bn[0][0]         \n",
      "                                                                 add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv1_act (Activation (None, 5, 5, 728)    0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv1 (SeparableConv2 (None, 5, 5, 728)    536536      block6_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv1_bn (BatchNormal (None, 5, 5, 728)    2912        block6_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv2_act (Activation (None, 5, 5, 728)    0           block6_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv2 (SeparableConv2 (None, 5, 5, 728)    536536      block6_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv2_bn (BatchNormal (None, 5, 5, 728)    2912        block6_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv3_act (Activation (None, 5, 5, 728)    0           block6_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv3 (SeparableConv2 (None, 5, 5, 728)    536536      block6_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv3_bn (BatchNormal (None, 5, 5, 728)    2912        block6_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 5, 5, 728)    0           block6_sepconv3_bn[0][0]         \n",
      "                                                                 add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv1_act (Activation (None, 5, 5, 728)    0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv1 (SeparableConv2 (None, 5, 5, 728)    536536      block7_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv1_bn (BatchNormal (None, 5, 5, 728)    2912        block7_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv2_act (Activation (None, 5, 5, 728)    0           block7_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv2 (SeparableConv2 (None, 5, 5, 728)    536536      block7_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv2_bn (BatchNormal (None, 5, 5, 728)    2912        block7_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv3_act (Activation (None, 5, 5, 728)    0           block7_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv3 (SeparableConv2 (None, 5, 5, 728)    536536      block7_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv3_bn (BatchNormal (None, 5, 5, 728)    2912        block7_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 5, 5, 728)    0           block7_sepconv3_bn[0][0]         \n",
      "                                                                 add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv1_act (Activation (None, 5, 5, 728)    0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv1 (SeparableConv2 (None, 5, 5, 728)    536536      block8_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv1_bn (BatchNormal (None, 5, 5, 728)    2912        block8_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv2_act (Activation (None, 5, 5, 728)    0           block8_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv2 (SeparableConv2 (None, 5, 5, 728)    536536      block8_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv2_bn (BatchNormal (None, 5, 5, 728)    2912        block8_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv3_act (Activation (None, 5, 5, 728)    0           block8_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv3 (SeparableConv2 (None, 5, 5, 728)    536536      block8_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv3_bn (BatchNormal (None, 5, 5, 728)    2912        block8_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 5, 5, 728)    0           block8_sepconv3_bn[0][0]         \n",
      "                                                                 add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv1_act (Activation (None, 5, 5, 728)    0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv1 (SeparableConv2 (None, 5, 5, 728)    536536      block9_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv1_bn (BatchNormal (None, 5, 5, 728)    2912        block9_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv2_act (Activation (None, 5, 5, 728)    0           block9_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv2 (SeparableConv2 (None, 5, 5, 728)    536536      block9_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv2_bn (BatchNormal (None, 5, 5, 728)    2912        block9_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv3_act (Activation (None, 5, 5, 728)    0           block9_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv3 (SeparableConv2 (None, 5, 5, 728)    536536      block9_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv3_bn (BatchNormal (None, 5, 5, 728)    2912        block9_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 5, 5, 728)    0           block9_sepconv3_bn[0][0]         \n",
      "                                                                 add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv1_act (Activatio (None, 5, 5, 728)    0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv1 (SeparableConv (None, 5, 5, 728)    536536      block10_sepconv1_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv1_bn (BatchNorma (None, 5, 5, 728)    2912        block10_sepconv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv2_act (Activatio (None, 5, 5, 728)    0           block10_sepconv1_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv2 (SeparableConv (None, 5, 5, 728)    536536      block10_sepconv2_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv2_bn (BatchNorma (None, 5, 5, 728)    2912        block10_sepconv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv3_act (Activatio (None, 5, 5, 728)    0           block10_sepconv2_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv3 (SeparableConv (None, 5, 5, 728)    536536      block10_sepconv3_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv3_bn (BatchNorma (None, 5, 5, 728)    2912        block10_sepconv3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 5, 5, 728)    0           block10_sepconv3_bn[0][0]        \n",
      "                                                                 add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv1_act (Activatio (None, 5, 5, 728)    0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv1 (SeparableConv (None, 5, 5, 728)    536536      block11_sepconv1_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv1_bn (BatchNorma (None, 5, 5, 728)    2912        block11_sepconv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv2_act (Activatio (None, 5, 5, 728)    0           block11_sepconv1_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv2 (SeparableConv (None, 5, 5, 728)    536536      block11_sepconv2_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv2_bn (BatchNorma (None, 5, 5, 728)    2912        block11_sepconv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv3_act (Activatio (None, 5, 5, 728)    0           block11_sepconv2_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv3 (SeparableConv (None, 5, 5, 728)    536536      block11_sepconv3_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv3_bn (BatchNorma (None, 5, 5, 728)    2912        block11_sepconv3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 5, 5, 728)    0           block11_sepconv3_bn[0][0]        \n",
      "                                                                 add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv1_act (Activatio (None, 5, 5, 728)    0           add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv1 (SeparableConv (None, 5, 5, 728)    536536      block12_sepconv1_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv1_bn (BatchNorma (None, 5, 5, 728)    2912        block12_sepconv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv2_act (Activatio (None, 5, 5, 728)    0           block12_sepconv1_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv2 (SeparableConv (None, 5, 5, 728)    536536      block12_sepconv2_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv2_bn (BatchNorma (None, 5, 5, 728)    2912        block12_sepconv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv3_act (Activatio (None, 5, 5, 728)    0           block12_sepconv2_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv3 (SeparableConv (None, 5, 5, 728)    536536      block12_sepconv3_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv3_bn (BatchNorma (None, 5, 5, 728)    2912        block12_sepconv3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 5, 5, 728)    0           block12_sepconv3_bn[0][0]        \n",
      "                                                                 add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv1_act (Activatio (None, 5, 5, 728)    0           add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv1 (SeparableConv (None, 5, 5, 728)    536536      block13_sepconv1_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv1_bn (BatchNorma (None, 5, 5, 728)    2912        block13_sepconv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv2_act (Activatio (None, 5, 5, 728)    0           block13_sepconv1_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv2 (SeparableConv (None, 5, 5, 1024)   752024      block13_sepconv2_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv2_bn (BatchNorma (None, 5, 5, 1024)   4096        block13_sepconv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 3, 3, 1024)   745472      add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block13_pool (MaxPooling2D)     (None, 3, 3, 1024)   0           block13_sepconv2_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 3, 3, 1024)   4096        conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 3, 3, 1024)   0           block13_pool[0][0]               \n",
      "                                                                 batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv1 (SeparableConv (None, 3, 3, 1536)   1582080     add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv1_bn (BatchNorma (None, 3, 3, 1536)   6144        block14_sepconv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv1_act (Activatio (None, 3, 3, 1536)   0           block14_sepconv1_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv2 (SeparableConv (None, 3, 3, 2048)   3159552     block14_sepconv1_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv2_bn (BatchNorma (None, 3, 3, 2048)   8192        block14_sepconv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv2_act (Activatio (None, 3, 3, 2048)   0           block14_sepconv2_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 18432)        0           block14_sepconv2_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 18432)        73728       flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 18432)        0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          2359424     dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 128)          512         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 128)          0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 14)           1806        dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 23,296,950\n",
      "Trainable params: 23,205,302\n",
      "Non-trainable params: 91,648\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/alan/miniconda2/envs/tfl/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 980 samples, validate on 420 samples\n",
      "Epoch 1/20\n",
      "980/980 [==============================] - 39s 39ms/step - loss: 4.0748 - accuracy: 0.0878 - val_loss: 2.9299 - val_accuracy: 0.0643\n",
      "Epoch 2/20\n",
      "980/980 [==============================] - 26s 27ms/step - loss: 2.7983 - accuracy: 0.2704 - val_loss: 2.9772 - val_accuracy: 0.0524\n",
      "Epoch 3/20\n",
      "980/980 [==============================] - 26s 26ms/step - loss: 2.0566 - accuracy: 0.4255 - val_loss: 3.0132 - val_accuracy: 0.0524\n",
      "Epoch 4/20\n",
      "980/980 [==============================] - 26s 26ms/step - loss: 1.7393 - accuracy: 0.5184 - val_loss: 3.0245 - val_accuracy: 0.0524\n",
      "Epoch 5/20\n",
      "980/980 [==============================] - 26s 27ms/step - loss: 1.3154 - accuracy: 0.6653 - val_loss: 3.0391 - val_accuracy: 0.0524\n",
      "Epoch 6/20\n",
      "980/980 [==============================] - 27s 27ms/step - loss: 1.0924 - accuracy: 0.7378 - val_loss: 3.0674 - val_accuracy: 0.0524\n",
      "Epoch 7/20\n",
      "980/980 [==============================] - 26s 27ms/step - loss: 0.7992 - accuracy: 0.8429 - val_loss: 3.0624 - val_accuracy: 0.0524\n",
      "Epoch 8/20\n",
      "980/980 [==============================] - 26s 26ms/step - loss: 0.7291 - accuracy: 0.8786 - val_loss: 3.0868 - val_accuracy: 0.0524\n",
      "Epoch 9/20\n",
      "980/980 [==============================] - 26s 26ms/step - loss: 0.6350 - accuracy: 0.8959 - val_loss: 3.1583 - val_accuracy: 0.0524\n",
      "Epoch 10/20\n",
      "980/980 [==============================] - 26s 26ms/step - loss: 0.5520 - accuracy: 0.9378 - val_loss: 3.1880 - val_accuracy: 0.0524\n",
      "Epoch 11/20\n",
      "980/980 [==============================] - 26s 26ms/step - loss: 0.5018 - accuracy: 0.9439 - val_loss: 3.1924 - val_accuracy: 0.0524\n",
      "Epoch 12/20\n",
      "980/980 [==============================] - 26s 26ms/step - loss: 0.4456 - accuracy: 0.9633 - val_loss: 3.1777 - val_accuracy: 0.0524\n",
      "Epoch 13/20\n",
      "980/980 [==============================] - 26s 26ms/step - loss: 0.4248 - accuracy: 0.9694 - val_loss: 3.2364 - val_accuracy: 0.0524\n",
      "Epoch 14/20\n",
      "980/980 [==============================] - 26s 27ms/step - loss: 0.3862 - accuracy: 0.9827 - val_loss: 3.2317 - val_accuracy: 0.0524\n",
      "Epoch 15/20\n",
      "980/980 [==============================] - 26s 26ms/step - loss: 0.3700 - accuracy: 0.9888 - val_loss: 3.1951 - val_accuracy: 0.0524\n",
      "Epoch 16/20\n",
      "980/980 [==============================] - 26s 27ms/step - loss: 0.3527 - accuracy: 0.9918 - val_loss: 3.1553 - val_accuracy: 0.0524\n",
      "Epoch 17/20\n",
      "980/980 [==============================] - 26s 27ms/step - loss: 0.3395 - accuracy: 0.9918 - val_loss: 3.1461 - val_accuracy: 0.0690\n",
      "Epoch 18/20\n",
      "980/980 [==============================] - 28s 29ms/step - loss: 0.3444 - accuracy: 0.9857 - val_loss: 3.1242 - val_accuracy: 0.0738\n",
      "Epoch 19/20\n",
      "980/980 [==============================] - 26s 27ms/step - loss: 0.3341 - accuracy: 0.9898 - val_loss: 3.1609 - val_accuracy: 0.0857\n",
      "Epoch 20/20\n",
      "980/980 [==============================] - 29s 29ms/step - loss: 0.3127 - accuracy: 0.9959 - val_loss: 3.1726 - val_accuracy: 0.0738\n",
      "\n",
      "Keras Xception #2 - accuracy: 0.07380952686071396 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alan/miniconda2/envs/tfl/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "        No Finding       0.00      0.00      0.00        27\n",
      "     Consolidation       0.00      0.00      0.00        35\n",
      "      Infiltration       0.00      0.00      0.00        39\n",
      "      Pneumothorax       0.00      0.00      0.00        37\n",
      "             Edema       0.00      0.00      0.00        28\n",
      "         Emphysema       0.06      0.50      0.11        26\n",
      "          Fibrosis       0.05      0.05      0.05        22\n",
      "          Effusion       0.00      0.00      0.00        32\n",
      "         Pneumonia       0.00      0.00      0.00        32\n",
      "Pleural Thickening       0.00      0.00      0.00        28\n",
      "      Cardiomegaly       0.11      0.22      0.15        36\n",
      "       Nodule Mass       0.00      0.00      0.00        27\n",
      "            Hernia       0.50      0.04      0.07        28\n",
      "       Atelectasis       0.08      0.35      0.13        23\n",
      "\n",
      "          accuracy                           0.07       420\n",
      "         macro avg       0.06      0.08      0.04       420\n",
      "      weighted avg       0.05      0.07      0.03       420\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Object arrays cannot be loaded when allow_pickle=False",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-9c87bcee45ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxceptionNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_trainHot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_testHot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0mplot_learning_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mplotKerasLearningCurve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-4b74a017f4fa>\u001b[0m in \u001b[0;36mplotKerasLearningCurve\u001b[0;34m(history)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplotKerasLearningCurve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'logs.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mfilt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# try to add 'loss' to see the loss learning curve\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfilt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/tfl/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    451\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m                 return format.read_array(fid, allow_pickle=allow_pickle,\n\u001b[0;32m--> 453\u001b[0;31m                                          pickle_kwargs=pickle_kwargs)\n\u001b[0m\u001b[1;32m    454\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m             \u001b[0;31m# Try a pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/tfl/lib/python3.6/site-packages/numpy/lib/format.py\u001b[0m in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m         \u001b[0;31m# The array contained Python objects. We need to unpickle the data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             raise ValueError(\"Object arrays cannot be loaded when \"\n\u001b[0m\u001b[1;32m    723\u001b[0m                              \"allow_pickle=False\")\n\u001b[1;32m    724\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpickle_kwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Object arrays cannot be loaded when allow_pickle=False"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras.applications.xception import Xception\n",
    "from keras.models import Model\n",
    "# weight_path = './keras-pretrained-models/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "map_characters=dict_characters\n",
    "import tensorflow as tf\n",
    "\n",
    "with tf.device('/device:GPU:0'):\n",
    "\n",
    "    def xceptionNet(a,b,c,d,e,f,g):\n",
    "        num_class = f\n",
    "        epochs = g\n",
    "        base_model = Xception(weights=None,\n",
    "            include_top=False, input_shape=(img_size, img_size, 3))\n",
    "        # Add a new top layer\n",
    "        x = base_model.output\n",
    "        x = Flatten()(x)\n",
    "#         x = BatchNormalization()(x)\n",
    "#         x = Dropout(0.5)(x)\n",
    "#         x = Dense(512, activation = \"relu\",kernel_regularizer=keras.regularizers.l2(0.001)\n",
    "#                  ,bias_regularizer=keras.regularizers.l2(0.001))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "        x = Dense(128, activation = \"relu\",kernel_regularizer=keras.regularizers.l2(0.001)\n",
    "                 ,bias_regularizer=keras.regularizers.l2(0.001))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "        predictions = Dense(num_class, activation='softmax')(x)\n",
    "        # This is the model we will train\n",
    "        model = Model(inputs=base_model.input, outputs=predictions)\n",
    "        # First: train only the top layers (which were randomly initialized)\n",
    "#         for layer in base_model.layers:\n",
    "#             layer.trainable = False\n",
    "        model.compile(loss='categorical_crossentropy', \n",
    "                      optimizer=keras.optimizers.adam(lr=0.0001), metrics=['accuracy'])\n",
    "        callbacks_list = [keras.callbacks.EarlyStopping(monitor='val_acc', patience=3, verbose=1)]\n",
    "        model.summary()\n",
    "        history = model.fit(a,b,epochs=epochs, class_weight=e, \n",
    "                            validation_data=(c,d), verbose=1,callbacks = [MetricsCheckpoint('logs')])\n",
    "        score = model.evaluate(c,d, verbose=0)\n",
    "        print('\\nKeras Xception #2 - accuracy:', score[1], '\\n')\n",
    "        y_pred = model.predict(c)\n",
    "        print('\\n', sklearn.metrics.classification_report(np.where(d > 0)[1], np.argmax(y_pred, axis=1), target_names=list(map_characters.values())), sep='') \n",
    "        Y_pred_classes = np.argmax(y_pred,axis = 1) \n",
    "        Y_true = np.argmax(d,axis = 1)\n",
    "        confusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n",
    "        plot_confusion_matrix(confusion_mtx, classes = list(map_characters.values()))\n",
    "        plt.show()\n",
    "        return model, history\n",
    "model, history = xceptionNet(X_train, Y_trainHot, X_test, Y_testHot,class_weight,14,20)\n",
    "plot_learning_curve(history)\n",
    "plotKerasLearningCurve(history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 128, 128, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv1 (Conv2D)           (None, 63, 63, 32)   864         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv1_bn (BatchNormaliza (None, 63, 63, 32)   128         block1_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv1_act (Activation)   (None, 63, 63, 32)   0           block1_conv1_bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv2 (Conv2D)           (None, 61, 61, 64)   18432       block1_conv1_act[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv2_bn (BatchNormaliza (None, 61, 61, 64)   256         block1_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv2_act (Activation)   (None, 61, 61, 64)   0           block1_conv2_bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv1 (SeparableConv2 (None, 61, 61, 128)  8768        block1_conv2_act[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv1_bn (BatchNormal (None, 61, 61, 128)  512         block2_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv2_act (Activation (None, 61, 61, 128)  0           block2_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv2 (SeparableConv2 (None, 61, 61, 128)  17536       block2_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv2_bn (BatchNormal (None, 61, 61, 128)  512         block2_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 31, 31, 128)  8192        block1_conv2_act[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block2_pool (MaxPooling2D)      (None, 31, 31, 128)  0           block2_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 31, 31, 128)  512         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 31, 31, 128)  0           block2_pool[0][0]                \n",
      "                                                                 batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv1_act (Activation (None, 31, 31, 128)  0           add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv1 (SeparableConv2 (None, 31, 31, 256)  33920       block3_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv1_bn (BatchNormal (None, 31, 31, 256)  1024        block3_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv2_act (Activation (None, 31, 31, 256)  0           block3_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv2 (SeparableConv2 (None, 31, 31, 256)  67840       block3_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv2_bn (BatchNormal (None, 31, 31, 256)  1024        block3_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 16, 16, 256)  32768       add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block3_pool (MaxPooling2D)      (None, 16, 16, 256)  0           block3_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 16, 16, 256)  1024        conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 16, 16, 256)  0           block3_pool[0][0]                \n",
      "                                                                 batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv1_act (Activation (None, 16, 16, 256)  0           add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv1 (SeparableConv2 (None, 16, 16, 728)  188672      block4_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv1_bn (BatchNormal (None, 16, 16, 728)  2912        block4_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv2_act (Activation (None, 16, 16, 728)  0           block4_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv2 (SeparableConv2 (None, 16, 16, 728)  536536      block4_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv2_bn (BatchNormal (None, 16, 16, 728)  2912        block4_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 8, 8, 728)    186368      add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block4_pool (MaxPooling2D)      (None, 8, 8, 728)    0           block4_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 8, 8, 728)    2912        conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 8, 8, 728)    0           block4_pool[0][0]                \n",
      "                                                                 batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv1_act (Activation (None, 8, 8, 728)    0           add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv1 (SeparableConv2 (None, 8, 8, 728)    536536      block5_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv1_bn (BatchNormal (None, 8, 8, 728)    2912        block5_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv2_act (Activation (None, 8, 8, 728)    0           block5_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv2 (SeparableConv2 (None, 8, 8, 728)    536536      block5_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv2_bn (BatchNormal (None, 8, 8, 728)    2912        block5_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv3_act (Activation (None, 8, 8, 728)    0           block5_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv3 (SeparableConv2 (None, 8, 8, 728)    536536      block5_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv3_bn (BatchNormal (None, 8, 8, 728)    2912        block5_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_16 (Add)                    (None, 8, 8, 728)    0           block5_sepconv3_bn[0][0]         \n",
      "                                                                 add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv1_act (Activation (None, 8, 8, 728)    0           add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv1 (SeparableConv2 (None, 8, 8, 728)    536536      block6_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv1_bn (BatchNormal (None, 8, 8, 728)    2912        block6_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv2_act (Activation (None, 8, 8, 728)    0           block6_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv2 (SeparableConv2 (None, 8, 8, 728)    536536      block6_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv2_bn (BatchNormal (None, 8, 8, 728)    2912        block6_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv3_act (Activation (None, 8, 8, 728)    0           block6_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv3 (SeparableConv2 (None, 8, 8, 728)    536536      block6_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv3_bn (BatchNormal (None, 8, 8, 728)    2912        block6_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_17 (Add)                    (None, 8, 8, 728)    0           block6_sepconv3_bn[0][0]         \n",
      "                                                                 add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv1_act (Activation (None, 8, 8, 728)    0           add_17[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv1 (SeparableConv2 (None, 8, 8, 728)    536536      block7_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv1_bn (BatchNormal (None, 8, 8, 728)    2912        block7_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv2_act (Activation (None, 8, 8, 728)    0           block7_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv2 (SeparableConv2 (None, 8, 8, 728)    536536      block7_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv2_bn (BatchNormal (None, 8, 8, 728)    2912        block7_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv3_act (Activation (None, 8, 8, 728)    0           block7_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv3 (SeparableConv2 (None, 8, 8, 728)    536536      block7_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv3_bn (BatchNormal (None, 8, 8, 728)    2912        block7_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_18 (Add)                    (None, 8, 8, 728)    0           block7_sepconv3_bn[0][0]         \n",
      "                                                                 add_17[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv1_act (Activation (None, 8, 8, 728)    0           add_18[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv1 (SeparableConv2 (None, 8, 8, 728)    536536      block8_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv1_bn (BatchNormal (None, 8, 8, 728)    2912        block8_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv2_act (Activation (None, 8, 8, 728)    0           block8_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv2 (SeparableConv2 (None, 8, 8, 728)    536536      block8_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv2_bn (BatchNormal (None, 8, 8, 728)    2912        block8_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv3_act (Activation (None, 8, 8, 728)    0           block8_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv3 (SeparableConv2 (None, 8, 8, 728)    536536      block8_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv3_bn (BatchNormal (None, 8, 8, 728)    2912        block8_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_19 (Add)                    (None, 8, 8, 728)    0           block8_sepconv3_bn[0][0]         \n",
      "                                                                 add_18[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv1_act (Activation (None, 8, 8, 728)    0           add_19[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv1 (SeparableConv2 (None, 8, 8, 728)    536536      block9_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv1_bn (BatchNormal (None, 8, 8, 728)    2912        block9_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv2_act (Activation (None, 8, 8, 728)    0           block9_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv2 (SeparableConv2 (None, 8, 8, 728)    536536      block9_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv2_bn (BatchNormal (None, 8, 8, 728)    2912        block9_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv3_act (Activation (None, 8, 8, 728)    0           block9_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv3 (SeparableConv2 (None, 8, 8, 728)    536536      block9_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv3_bn (BatchNormal (None, 8, 8, 728)    2912        block9_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_20 (Add)                    (None, 8, 8, 728)    0           block9_sepconv3_bn[0][0]         \n",
      "                                                                 add_19[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv1_act (Activatio (None, 8, 8, 728)    0           add_20[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv1 (SeparableConv (None, 8, 8, 728)    536536      block10_sepconv1_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv1_bn (BatchNorma (None, 8, 8, 728)    2912        block10_sepconv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv2_act (Activatio (None, 8, 8, 728)    0           block10_sepconv1_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv2 (SeparableConv (None, 8, 8, 728)    536536      block10_sepconv2_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv2_bn (BatchNorma (None, 8, 8, 728)    2912        block10_sepconv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv3_act (Activatio (None, 8, 8, 728)    0           block10_sepconv2_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv3 (SeparableConv (None, 8, 8, 728)    536536      block10_sepconv3_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv3_bn (BatchNorma (None, 8, 8, 728)    2912        block10_sepconv3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_21 (Add)                    (None, 8, 8, 728)    0           block10_sepconv3_bn[0][0]        \n",
      "                                                                 add_20[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv1_act (Activatio (None, 8, 8, 728)    0           add_21[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv1 (SeparableConv (None, 8, 8, 728)    536536      block11_sepconv1_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv1_bn (BatchNorma (None, 8, 8, 728)    2912        block11_sepconv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv2_act (Activatio (None, 8, 8, 728)    0           block11_sepconv1_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv2 (SeparableConv (None, 8, 8, 728)    536536      block11_sepconv2_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv2_bn (BatchNorma (None, 8, 8, 728)    2912        block11_sepconv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv3_act (Activatio (None, 8, 8, 728)    0           block11_sepconv2_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv3 (SeparableConv (None, 8, 8, 728)    536536      block11_sepconv3_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv3_bn (BatchNorma (None, 8, 8, 728)    2912        block11_sepconv3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_22 (Add)                    (None, 8, 8, 728)    0           block11_sepconv3_bn[0][0]        \n",
      "                                                                 add_21[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv1_act (Activatio (None, 8, 8, 728)    0           add_22[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv1 (SeparableConv (None, 8, 8, 728)    536536      block12_sepconv1_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv1_bn (BatchNorma (None, 8, 8, 728)    2912        block12_sepconv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv2_act (Activatio (None, 8, 8, 728)    0           block12_sepconv1_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv2 (SeparableConv (None, 8, 8, 728)    536536      block12_sepconv2_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv2_bn (BatchNorma (None, 8, 8, 728)    2912        block12_sepconv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv3_act (Activatio (None, 8, 8, 728)    0           block12_sepconv2_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv3 (SeparableConv (None, 8, 8, 728)    536536      block12_sepconv3_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv3_bn (BatchNorma (None, 8, 8, 728)    2912        block12_sepconv3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_23 (Add)                    (None, 8, 8, 728)    0           block12_sepconv3_bn[0][0]        \n",
      "                                                                 add_22[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv1_act (Activatio (None, 8, 8, 728)    0           add_23[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv1 (SeparableConv (None, 8, 8, 728)    536536      block13_sepconv1_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv1_bn (BatchNorma (None, 8, 8, 728)    2912        block13_sepconv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv2_act (Activatio (None, 8, 8, 728)    0           block13_sepconv1_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv2 (SeparableConv (None, 8, 8, 1024)   752024      block13_sepconv2_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv2_bn (BatchNorma (None, 8, 8, 1024)   4096        block13_sepconv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 4, 4, 1024)   745472      add_23[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block13_pool (MaxPooling2D)     (None, 4, 4, 1024)   0           block13_sepconv2_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 4, 4, 1024)   4096        conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_24 (Add)                    (None, 4, 4, 1024)   0           block13_pool[0][0]               \n",
      "                                                                 batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv1 (SeparableConv (None, 4, 4, 1536)   1582080     add_24[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv1_bn (BatchNorma (None, 4, 4, 1536)   6144        block14_sepconv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv1_act (Activatio (None, 4, 4, 1536)   0           block14_sepconv1_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv2 (SeparableConv (None, 4, 4, 2048)   3159552     block14_sepconv1_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv2_bn (BatchNorma (None, 4, 4, 2048)   8192        block14_sepconv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv2_act (Activatio (None, 4, 4, 2048)   0           block14_sepconv2_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 32768)        0           block14_sepconv2_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 32768)        131072      flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 32768)        0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 128)          4194432     dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 128)          512         dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128)          0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 14)           1806        dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 25,189,302\n",
      "Trainable params: 4,262,030\n",
      "Non-trainable params: 20,927,272\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8175 samples, validate on 3504 samples\n",
      "Epoch 1/500\n",
      "8175/8175 [==============================] - 159s 20ms/step - loss: 3.7593 - accuracy: 0.0920 - val_loss: 3.3839 - val_accuracy: 0.0910\n",
      "Epoch 2/500\n",
      "8175/8175 [==============================] - 118s 14ms/step - loss: 3.3988 - accuracy: 0.1207 - val_loss: 3.8404 - val_accuracy: 0.0979\n",
      "Epoch 3/500\n",
      "8175/8175 [==============================] - 118s 14ms/step - loss: 3.2497 - accuracy: 0.1524 - val_loss: 4.0995 - val_accuracy: 0.1042\n",
      "Epoch 4/500\n",
      "8175/8175 [==============================] - 117s 14ms/step - loss: 3.0845 - accuracy: 0.1737 - val_loss: 4.0383 - val_accuracy: 0.0945\n",
      "Epoch 5/500\n",
      "8175/8175 [==============================] - 118s 14ms/step - loss: 3.0101 - accuracy: 0.1906 - val_loss: 4.5401 - val_accuracy: 0.0850\n",
      "Epoch 6/500\n",
      "8175/8175 [==============================] - 118s 14ms/step - loss: 2.9152 - accuracy: 0.2049 - val_loss: 4.3286 - val_accuracy: 0.0959\n",
      "Epoch 7/500\n",
      "8175/8175 [==============================] - 117s 14ms/step - loss: 2.8488 - accuracy: 0.2246 - val_loss: 4.3064 - val_accuracy: 0.1099\n",
      "Epoch 8/500\n",
      "8175/8175 [==============================] - 117s 14ms/step - loss: 2.8018 - accuracy: 0.2335 - val_loss: 4.2489 - val_accuracy: 0.1045\n",
      "Epoch 9/500\n",
      "8175/8175 [==============================] - 118s 14ms/step - loss: 2.7125 - accuracy: 0.2575 - val_loss: 5.3163 - val_accuracy: 0.0893\n",
      "Epoch 10/500\n",
      "8175/8175 [==============================] - 118s 14ms/step - loss: 2.6434 - accuracy: 0.2711 - val_loss: 4.9485 - val_accuracy: 0.0947\n",
      "Epoch 11/500\n",
      "8175/8175 [==============================] - 117s 14ms/step - loss: 2.5688 - accuracy: 0.2914 - val_loss: 4.7804 - val_accuracy: 0.1093\n",
      "Epoch 12/500\n",
      "8175/8175 [==============================] - 118s 14ms/step - loss: 2.5379 - accuracy: 0.2993 - val_loss: 4.7471 - val_accuracy: 0.1010\n",
      "Epoch 13/500\n",
      "8160/8175 [============================>.] - ETA: 0s - loss: 2.4701 - accuracy: 0.3163"
     ]
    }
   ],
   "source": [
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.models import Model\n",
    "# weight_path = './keras-pretrained-models/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "map_characters=dict_characters\n",
    "import tensorflow as tf\n",
    "\n",
    "with tf.device('/device:GPU:0'):\n",
    "\n",
    "    def inceptionNet(a,b,c,d,e,f,g):\n",
    "        num_class = f\n",
    "        epochs = g\n",
    "        base_model = InceptionV3(weights='imagenet',\n",
    "            include_top=False, input_shape=(img_size, img_size, 3))\n",
    "        # Add a new top layer\n",
    "        x = base_model.output\n",
    "        x = Flatten()(x)\n",
    "#         x = BatchNormalization()(x)\n",
    "#         x = Dropout(0.5)(x)\n",
    "#         x = Dense(512, activation = \"relu\",kernel_regularizer=keras.regularizers.l2(0.001)\n",
    "#                  ,bias_regularizer=keras.regularizers.l2(0.001))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "        x = Dense(128, activation = \"relu\",kernel_regularizer=keras.regularizers.l2(0.001)\n",
    "                 ,bias_regularizer=keras.regularizers.l2(0.001))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "        predictions = Dense(num_class, activation='softmax')(x)\n",
    "        # This is the model we will train\n",
    "        model = Model(inputs=base_model.input, outputs=predictions)\n",
    "        # First: train only the top layers (which were randomly initialized)\n",
    "        for layer in base_model.layers:\n",
    "            layer.trainable = False\n",
    "        model.compile(loss='categorical_crossentropy', \n",
    "                      optimizer=keras.optimizers.adam(lr=0.0001), metrics=['accuracy'])\n",
    "        callbacks_list = [keras.callbacks.EarlyStopping(monitor='val_acc', patience=3, verbose=1)]\n",
    "        model.summary()\n",
    "        history = model.fit(a,b,epochs=epochs, class_weight=e, \n",
    "                            validation_data=(c,d), verbose=1,callbacks = [MetricsCheckpoint('logs')])\n",
    "        score = model.evaluate(c,d, verbose=0)\n",
    "        print('\\nKeras Xception #2 - accuracy:', score[1], '\\n')\n",
    "        y_pred = model.predict(c)\n",
    "        print('\\n', sklearn.metrics.classification_report(np.where(d > 0)[1], np.argmax(y_pred, axis=1), target_names=list(map_characters.values())), sep='') \n",
    "        Y_pred_classes = np.argmax(y_pred,axis = 1) \n",
    "        Y_true = np.argmax(d,axis = 1)\n",
    "        confusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n",
    "        plot_confusion_matrix(confusion_mtx, classes = list(map_characters.values()))\n",
    "        plt.show()\n",
    "        return model, history\n",
    "model, history = inceptionNet(X_train, Y_trainHot, X_test, Y_testHot,class_weight,14,500)\n",
    "plot_learning_curve(history)\n",
    "plotKerasLearningCurve(history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "When setting `include_top=True` and loading `imagenet` weights, `input_shape` should be (331, 331, 3).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-7ddcdf7e805e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNasNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_trainHot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_testHot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0mplot_learning_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0mplotKerasLearningCurve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-7ddcdf7e805e>\u001b[0m in \u001b[0;36mNasNetwork\u001b[0;34m(a, b, c, d, e, f, g)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         base_model = NASNetLarge(weights='imagenet',\n\u001b[0;32m---> 13\u001b[0;31m             include_top=False, input_shape=(img_size, img_size, 3))\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;31m# Add a new top layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/tfl/lib/python3.6/site-packages/keras/applications/__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'models'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'utils'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbase_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/tfl/lib/python3.6/site-packages/keras/applications/nasnet.py\u001b[0m in \u001b[0;36mNASNetLarge\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mkeras_modules_injection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mNASNetLarge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnasnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNASNetLarge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/tfl/lib/python3.6/site-packages/keras_applications/nasnet.py\u001b[0m in \u001b[0;36mNASNetLarge\u001b[0;34m(input_shape, include_top, weights, input_tensor, pooling, classes, **kwargs)\u001b[0m\n\u001b[1;32m    364\u001b[0m                   \u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m                   \u001b[0mdefault_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m331\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 366\u001b[0;31m                   **kwargs)\n\u001b[0m\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/tfl/lib/python3.6/site-packages/keras_applications/nasnet.py\u001b[0m in \u001b[0;36mNASNet\u001b[0;34m(input_shape, penultimate_filters, num_blocks, stem_block_filters, skip_reduction, filter_multiplier, include_top, weights, input_tensor, pooling, classes, default_size, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m                                       \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_data_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m                                       \u001b[0mrequire_flatten\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m                                       weights=weights)\n\u001b[0m\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_data_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'channels_last'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/tfl/lib/python3.6/site-packages/keras_applications/imagenet_utils.py\u001b[0m in \u001b[0;36m_obtain_input_shape\u001b[0;34m(input_shape, default_size, min_size, data_format, require_flatten, weights)\u001b[0m\n\u001b[1;32m    290\u001b[0m                                  \u001b[0;34m'and loading `imagenet` weights, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m                                  \u001b[0;34m'`input_shape` should be '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m                                  str(default_shape) + '.')\n\u001b[0m\u001b[1;32m    293\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdefault_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: When setting `include_top=True` and loading `imagenet` weights, `input_shape` should be (331, 331, 3)."
     ]
    }
   ],
   "source": [
    "from keras.applications.nasnet import NASNetLarge\n",
    "from keras.models import Model\n",
    "# weight_path = './keras-pretrained-models/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "map_characters=dict_characters\n",
    "import tensorflow as tf\n",
    "\n",
    "with tf.device('/device:GPU:0'):\n",
    "\n",
    "    def NasNetwork(a,b,c,d,e,f,g):\n",
    "        num_class = f\n",
    "        epochs = g\n",
    "        base_model = NASNetLarge(weights='imagenet',\n",
    "            include_top=False, input_shape=(img_size, img_size, 3))\n",
    "        # Add a new top layer\n",
    "        x = base_model.output\n",
    "        x = Flatten()(x)\n",
    "#         x = BatchNormalization()(x)\n",
    "#         x = Dropout(0.5)(x)\n",
    "#         x = Dense(512, activation = \"relu\",kernel_regularizer=keras.regularizers.l2(0.001)\n",
    "#                  ,bias_regularizer=keras.regularizers.l2(0.001))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "        x = Dense(128, activation = \"relu\",kernel_regularizer=keras.regularizers.l2(0.001)\n",
    "                 ,bias_regularizer=keras.regularizers.l2(0.001))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "        predictions = Dense(num_class, activation='softmax')(x)\n",
    "        # This is the model we will train\n",
    "        model = Model(inputs=base_model.input, outputs=predictions)\n",
    "        # First: train only the top layers (which were randomly initialized)\n",
    "        for layer in base_model.layers:\n",
    "            layer.trainable = False\n",
    "        model.compile(loss='categorical_crossentropy', \n",
    "                      optimizer=keras.optimizers.adam(lr=0.0001), metrics=['accuracy'])\n",
    "        callbacks_list = [keras.callbacks.EarlyStopping(monitor='val_acc', patience=3, verbose=1)]\n",
    "        model.summary()\n",
    "        history = model.fit(a,b,epochs=epochs, class_weight=e, \n",
    "                            validation_data=(c,d), verbose=1,callbacks = [MetricsCheckpoint('logs')])\n",
    "        score = model.evaluate(c,d, verbose=0)\n",
    "        print('\\nKeras Xception #2 - accuracy:', score[1], '\\n')\n",
    "        y_pred = model.predict(c)\n",
    "        print('\\n', sklearn.metrics.classification_report(np.where(d > 0)[1], np.argmax(y_pred, axis=1), target_names=list(map_characters.values())), sep='') \n",
    "        Y_pred_classes = np.argmax(y_pred,axis = 1) \n",
    "        Y_true = np.argmax(d,axis = 1)\n",
    "        confusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n",
    "        plot_confusion_matrix(confusion_mtx, classes = list(map_characters.values()))\n",
    "        plt.show()\n",
    "        return model, history\n",
    "model, history = NasNetwork(X_train, Y_trainHot, X_test, Y_testHot,class_weight,14,10)\n",
    "plot_learning_curve(history)\n",
    "plotKerasLearningCurve(history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "When setting `include_top=True` and loading `imagenet` weights, `input_shape` should be (331, 331, 3).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-7ddcdf7e805e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNasNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_trainHot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_testHot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0mplot_learning_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0mplotKerasLearningCurve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-7ddcdf7e805e>\u001b[0m in \u001b[0;36mNasNetwork\u001b[0;34m(a, b, c, d, e, f, g)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         base_model = NASNetLarge(weights='imagenet',\n\u001b[0;32m---> 13\u001b[0;31m             include_top=False, input_shape=(img_size, img_size, 3))\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;31m# Add a new top layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/tfl/lib/python3.6/site-packages/keras/applications/__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'models'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'utils'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbase_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/tfl/lib/python3.6/site-packages/keras/applications/nasnet.py\u001b[0m in \u001b[0;36mNASNetLarge\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mkeras_modules_injection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mNASNetLarge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnasnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNASNetLarge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/tfl/lib/python3.6/site-packages/keras_applications/nasnet.py\u001b[0m in \u001b[0;36mNASNetLarge\u001b[0;34m(input_shape, include_top, weights, input_tensor, pooling, classes, **kwargs)\u001b[0m\n\u001b[1;32m    364\u001b[0m                   \u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m                   \u001b[0mdefault_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m331\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 366\u001b[0;31m                   **kwargs)\n\u001b[0m\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/tfl/lib/python3.6/site-packages/keras_applications/nasnet.py\u001b[0m in \u001b[0;36mNASNet\u001b[0;34m(input_shape, penultimate_filters, num_blocks, stem_block_filters, skip_reduction, filter_multiplier, include_top, weights, input_tensor, pooling, classes, default_size, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m                                       \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_data_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m                                       \u001b[0mrequire_flatten\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m                                       weights=weights)\n\u001b[0m\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_data_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'channels_last'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/tfl/lib/python3.6/site-packages/keras_applications/imagenet_utils.py\u001b[0m in \u001b[0;36m_obtain_input_shape\u001b[0;34m(input_shape, default_size, min_size, data_format, require_flatten, weights)\u001b[0m\n\u001b[1;32m    290\u001b[0m                                  \u001b[0;34m'and loading `imagenet` weights, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m                                  \u001b[0;34m'`input_shape` should be '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m                                  str(default_shape) + '.')\n\u001b[0m\u001b[1;32m    293\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdefault_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: When setting `include_top=True` and loading `imagenet` weights, `input_shape` should be (331, 331, 3)."
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "# weight_path = './keras-pretrained-models/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "map_characters=dict_characters\n",
    "import tensorflow as tf\n",
    "\n",
    "with tf.device('/device:GPU:0'):\n",
    "\n",
    "    def LeNet(a,b,c,d,e,f,g):\n",
    "        num_class = f\n",
    "        epochs = g\n",
    "        model = keras.Sequential()\n",
    "\n",
    "        model.add(layers.Conv2D(filters=6, kernel_size=(3, 3), activation='relu', input_shape=(img_size,img_size,3)))\n",
    "        model.add(layers.AveragePooling2D())\n",
    "\n",
    "        model.add(layers.Conv2D(filters=16, kernel_size=(3, 3), activation='relu'))\n",
    "        model.add(layers.AveragePooling2D())\n",
    "\n",
    "        model.add(layers.Flatten())\n",
    "\n",
    "        model.add(layers.Dense(units=120, activation='relu'))\n",
    "\n",
    "        model.add(layers.Dense(units=84, activation='relu'))\n",
    "\n",
    "        model.add(layers.Dense(units=14, activation = 'softmax'))\n",
    "\n",
    "        model.compile(loss='categorical_crossentropy', \n",
    "                      optimizer=keras.optimizers.adam(lr=0.0001), metrics=['accuracy'])\n",
    "        callbacks_list = [keras.callbacks.EarlyStopping(monitor='val_acc', patience=3, verbose=1)]\n",
    "        model.summary()\n",
    "        history = model.fit(a,b,epochs=epochs, class_weight=e, \n",
    "                            validation_data=(c,d), verbose=1,callbacks = [MetricsCheckpoint('logs')])\n",
    "        score = model.evaluate(c,d, verbose=0)\n",
    "        print('\\nKeras LeNet - accuracy:', score[1], '\\n')\n",
    "        y_pred = model.predict(c)\n",
    "        print('\\n', sklearn.metrics.classification_report(np.where(d > 0)[1], np.argmax(y_pred, axis=1), target_names=list(map_characters.values())), sep='') \n",
    "        Y_pred_classes = np.argmax(y_pred,axis = 1) \n",
    "        Y_true = np.argmax(d,axis = 1)\n",
    "        confusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n",
    "        plot_confusion_matrix(confusion_mtx, classes = list(map_characters.values()))\n",
    "        plt.show()\n",
    "        return model, history\n",
    "model, history = NasNetwork(X_train, Y_trainHot, X_test, Y_testHot,class_weight,14,10)\n",
    "plot_learning_curve(history)\n",
    "plotKerasLearningCurve(history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "data_dir = 'data/hymenoptera_data'\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n",
    "                                             shuffle=True, num_workers=4)\n",
    "              for x in ['train', 'val']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(inp, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "\n",
    "\n",
    "# Get a batch of training data\n",
    "inputs, classes = next(iter(dataloaders['train']))\n",
    "\n",
    "# Make a grid from batch\n",
    "out = torchvision.utils.make_grid(inputs)\n",
    "\n",
    "imshow(out, title=[class_names[x] for x in classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_model(model, num_images=6):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    images_so_far = 0\n",
    "    fig = plt.figure()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(dataloaders['val']):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            for j in range(inputs.size()[0]):\n",
    "                images_so_far += 1\n",
    "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
    "                ax.axis('off')\n",
    "                ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n",
    "                imshow(inputs.cpu().data[j])\n",
    "\n",
    "                if images_so_far == num_images:\n",
    "                    model.train(mode=was_training)\n",
    "                    return\n",
    "        model.train(mode=was_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = models.resnet18(pretrained=True)\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "# Here the size of each output sample is set to 2.\n",
    "# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n",
    "model_ft.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_model(model_ft)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "23b67fb9-392c-42db-ae71-d041020830b5",
    "_uuid": "1fa87b6471eac782c671b74d03d9bf8fdfb7de9d"
   },
   "source": [
    "The imbalance in our dataset has resulted in a biased model.  I tried to prevent this by modifying the class_weights parameter and using in the model.fit function but apparently that was not enough.  Now I will try to compensate for the imbalanced sample size by oversampling or upsampling the minority classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_cell_guid": "1f2b2635-0fac-4a43-94ca-5d19f4d7c38d",
    "_uuid": "8efc5b3a1460ec44e5a40e976fa0a424935b3fb0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'No Finding', 1: 'Consolidation', 2: 'Infiltration', 3: 'Pneumothorax', 4: 'Edema', 5: 'Emphysema', 6: 'Fibrosis', 7: 'Effusion', 8: 'Pneumonia', 9: 'Pleural Thickening', 10: 'Cardiomegaly', 11: 'Nodule Mass', 12: 'Hernia', 13: 'Atelectasis'}\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lab = df['labels']\n",
    "dist = lab.value_counts()\n",
    "sns.countplot(lab)\n",
    "print(dict_characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e5cb5458-1c17-4a75-be12-fb05bfe72cdd",
    "_uuid": "902522528a8e06af26d0f6f635b7306d042496e3"
   },
   "source": [
    "It is very import to do upsampling AFTER the train_test_split function otherwise you can end up with values in the testing dataset that are related to the values within the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_cell_guid": "939c16e1-c31e-4c3a-8973-cf45987cbf61",
    "_uuid": "9e8fd0135091fb3e2e26e852c41fbe20a2d2574e"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2)\n",
    "# Reduce Sample Size for DeBugging\n",
    "X_train = X_train[0:5000] \n",
    "Y_train = Y_train[0:5000]\n",
    "X_test = X_test[0:2000] \n",
    "Y_test = Y_test[0:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_cell_guid": "c0a627f5-fe92-4a63-b46a-642cf136ab4a",
    "_uuid": "4a925b6f4e899a8cf13fb4ac79c771649a80bf31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train Shape:  (1556, 64, 64, 3)\n",
      "X_test Shape:  (390, 64, 64, 3)\n",
      "X_trainFlat Shape:  (1556, 12288)\n",
      "X_testFlat Shape:  (390, 12288)\n"
     ]
    }
   ],
   "source": [
    "# Make Data 1D for compatability upsampling methods\n",
    "X_trainShape = X_train.shape[1]*X_train.shape[2]*X_train.shape[3]\n",
    "X_testShape = X_test.shape[1]*X_test.shape[2]*X_test.shape[3]\n",
    "X_trainFlat = X_train.reshape(X_train.shape[0], X_trainShape)\n",
    "X_testFlat = X_test.reshape(X_test.shape[0], X_testShape)\n",
    "print(\"X_train Shape: \",X_train.shape)\n",
    "print(\"X_test Shape: \",X_test.shape)\n",
    "print(\"X_trainFlat Shape: \",X_trainFlat.shape)\n",
    "print(\"X_testFlat Shape: \",X_testFlat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_cell_guid": "8b4650f7-1bce-4008-a2ab-191cf063b449",
    "_uuid": "92bfdb43ac864b9de5e8ec4f365902819bc3f48a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:  (1556, 64, 64, 3)\n",
      "X_trainFlat:  (1556, 12288)\n",
      "X_trainRos Shape:  (5516, 12288)\n",
      "X_testRos Shape:  (1526, 12288)\n",
      "Y_trainRosHot Shape:  (5516, 14)\n",
      "Y_testRosHot Shape:  (1526, 14)\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "ros = RandomOverSampler(ratio='auto')\n",
    "X_trainRos, Y_trainRos = ros.fit_sample(X_trainFlat, Y_train)\n",
    "X_testRos, Y_testRos = ros.fit_sample(X_testFlat, Y_test)\n",
    "\n",
    "# Encode labels to hot vectors (ex : 2 -> [0,0,1,0,0,0,0,0,0,0])\n",
    "Y_trainRosHot = to_categorical(Y_trainRos, num_classes = 14)\n",
    "Y_testRosHot = to_categorical(Y_testRos, num_classes = 14)\n",
    "print(\"X_train: \", X_train.shape)\n",
    "print(\"X_trainFlat: \", X_trainFlat.shape)\n",
    "print(\"X_trainRos Shape: \",X_trainRos.shape)\n",
    "print(\"X_testRos Shape: \",X_testRos.shape)\n",
    "print(\"Y_trainRosHot Shape: \",Y_trainRosHot.shape)\n",
    "print(\"Y_testRosHot Shape: \",Y_testRosHot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_cell_guid": "db3c4af4-a0a7-4dfe-871b-7111a42e1e41",
    "_uuid": "5c3ffcdb87753f3b0aa8135c54c75c0643512f40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_trainRos Shape:  (5516, 12288)\n",
      "X_trainRosReshaped Shape:  (5516, 64, 64, 3)\n",
      "X_testRos Shape:  (1526, 12288)\n",
      "X_testRosReshaped Shape:  (1526, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(X_trainRos)):\n",
    "    height, width, channels = 64,64,3\n",
    "    X_trainRosReshaped = X_trainRos.reshape(len(X_trainRos),height,width,channels)\n",
    "print(\"X_trainRos Shape: \",X_trainRos.shape)\n",
    "print(\"X_trainRosReshaped Shape: \",X_trainRosReshaped.shape)\n",
    "\n",
    "for i in range(len(X_testRos)):\n",
    "    height, width, channels = 64,64,3\n",
    "    X_testRosReshaped = X_testRos.reshape(len(X_testRos),height,width,channels)\n",
    "print(\"X_testRos Shape: \",X_testRos.shape)\n",
    "print(\"X_testRosReshaped Shape: \",X_testRosReshaped.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_cell_guid": "a767f02c-2bfa-4a75-9035-7ef3cc5cfc78",
    "_uuid": "659a245fccbe24ff457b2c12f4b4be8d85d35416"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'No Finding', 1: 'Consolidation', 2: 'Infiltration', 3: 'Pneumothorax', 4: 'Edema', 5: 'Emphysema', 6: 'Fibrosis', 7: 'Effusion', 8: 'Pneumonia', 9: 'Pleural Thickening', 10: 'Cardiomegaly', 11: 'Nodule Mass', 12: 'Hernia', 13: 'Atelectasis'}\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dfRos = pd.DataFrame()\n",
    "dfRos[\"labels\"]=Y_trainRos\n",
    "labRos = dfRos['labels']\n",
    "distRos = lab.value_counts()\n",
    "sns.countplot(labRos)\n",
    "print(dict_characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5aba6477-ec4d-43dd-bc54-88942f8004eb",
    "_uuid": "3bb56b79191780f37fcd0935b10de950e0ac5b5c"
   },
   "source": [
    "Now we have a much more even distriution of sample sizes for each of our 7 ailments (plus an 8th category for other/typos).  This should help make our model less biased in favor of the majority class (0=No Finding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_cell_guid": "2a201885-aa4a-4cbd-a522-4fbf149a29fa",
    "_uuid": "f811702e7777693eeb37a2bf7b1f14fd799a6c3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old Class Weights:  [ 0.30021598  1.93055556  0.27634195  1.21929825  3.3902439   3.30952381\n",
      "  3.65789474  0.68472906  9.92857143  2.13846154  2.78        0.96527778\n",
      " 27.8         0.72395833]\n",
      "New Class Weights:  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import class_weight\n",
    "class_weight = class_weight.compute_class_weight('balanced', np.unique(y), y)\n",
    "print(\"Old Class Weights: \",class_weight)\n",
    "from sklearn.utils import class_weight\n",
    "class_weight = class_weight.compute_class_weight('balanced', np.unique(Y_trainRos), Y_trainRos)\n",
    "print(\"New Class Weights: \",class_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "54069df5941947f1514c7a7ac9bfaebe66802172"
   },
   "source": [
    "Our model can predict pathologies in x-ray images with an accuracy rate that is much better than random chance but there is still a lot of room for improvement.  Please see the [following blogpost](https://lukeoakdenrayner.wordpress.com/2017/12/18/the-chestxray14-dataset-problems/) for more detail about specific challenges associated with this dataset . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ResNet50() got an unexpected keyword argument 'weights'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-032fc07b828c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m \u001b[0mResNet50\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_trainHot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_testHot\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m14\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-41-032fc07b828c>\u001b[0m in \u001b[0;36mResNet50\u001b[1;34m(a, b, c, d, e, f, g)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     base_model = ResNet50(#weights='imagenet',\n\u001b[1;32m---> 10\u001b[1;33m         weights = weight_path, input_shape=(im_size, im_size, 3))\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[1;31m# Add a new top layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: ResNet50() got an unexpected keyword argument 'weights'"
     ]
    }
   ],
   "source": [
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.models import Model\n",
    "weight_path = './keras-pretrained-models/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "im_size = 64\n",
    "map_characters=dict_characters  \n",
    "def ResNet50(a,b,c,d,e,f,g):\n",
    "    num_class = f\n",
    "    epochs = g\n",
    "    base_model = ResNet50(#weights='imagenet',\n",
    "        weights = weight_path, include_top=False, input_shape=(im_size, im_size, 3))\n",
    "    # Add a new top layer\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dropout(0.7)(x)\n",
    "    x = Flatten()(x)\n",
    "    predictions = Dense(num_class, activation='softmax')(x)\n",
    "    # This is the model we will train\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    # First: train only the top layers (which were randomly initialized)\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                  optimizer=keras.optimizers.RMSprop(lr=0.0001), \n",
    "                  metrics=['accuracy'])\n",
    "    callbacks_list = [keras.callbacks.EarlyStopping(monitor='val_acc', patience=3, verbose=1)]\n",
    "    model.summary()\n",
    "    model.fit(a,b, epochs=epochs, class_weight=e, validation_data=(c,d), verbose=1,callbacks = [MetricsCheckpoint('logs')])\n",
    "    score = model.evaluate(c,d, verbose=0)\n",
    "    print('\\nKeras CNN #2 - accuracy:', score[1], '\\n')\n",
    "    y_pred = model.predict(c)\n",
    "    print('\\n', sklearn.metrics.classification_report(np.where(d > 0)[1], np.argmax(y_pred, axis=1), target_names=list(map_characters.values())), sep='') \n",
    "    Y_pred_classes = np.argmax(y_pred,axis = 1) \n",
    "    Y_true = np.argmax(d,axis = 1) \n",
    "    confusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n",
    "    plotKerasLearningCurve()\n",
    "    plt.show()\n",
    "    plot_confusion_matrix(confusion_mtx, classes = list(map_characters.values()))\n",
    "    plt.show()\n",
    "    return model\n",
    "ResNet50(X_train, Y_trainHot, X_test, Y_testHot,class_weight,14,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
